{"title":"Лабораторна робота № 2","markdown":{"yaml":{"title":"Лабораторна робота № 2"},"headingText":"Теоретичні відомості","containsRefs":false,"markdown":"\n\n\n\n\n**Тема.** Використання рекурентного аналізу для моделювання і прогнозування\nнелінійних динамічних властивостей складних систем\n\n**Мета.** Навчитися інструментарію нелінійної динаміки, який відноситься до\nрекурентних властивостей нестаціонарних динамічних рядів\n\n\n\n\nДослідження складних систем, як природних, так і штучних, показали, що в їх основі лежать нелінійні процеси, ретельне вивчення яких необхідне для розуміння і моделювання складних систем. У останні десятиліття набір традиційних (лінійних) методик дослідження був істотно розширений нелінійними методами, одержаними з теорії нелінійної динаміки і хаосу; багато досліджень були присвячені оцінці нелінійних характеристик і властивостей процесів, що протікають в природі (скейлінг, фрактальна розмірність). Проте більшість методів нелінійного аналізу вимагає або достатньо довгих, або стаціонарних рядів даних, які досить важко одержати в природний спосіб. Більш того, було показано, що дані методи дають задовільні результати для моделей реальних систем, що ідеалізуються. Ці чинники вимагали розробки нових методик нелінійного аналізу даних.\n\nСтан природних або штучних систем, як правило, змінюється в часі. Вивчення цих, часто складних процесів --- важлива задача в багатьох дисциплінах, дозволяє зрозуміти і описати їх суть, наприклад, для прогнозування стану на деякий час у майбутнє. Метою таких досліджень є знаходження математичних моделей, які б достатньо відповідали реальним процесам і могли б бути використані для розв'язання поставлених задач.\n\nРозглянемо ідею і коротко опишемо теорію рекурентного аналізу, наведемо деякі приклади, розглянемо його можливі області застосування при аналізі і прогнозування складних фінансово-економічних систем.\n\n### Фазовий простір та його реконструкція\n\nСтан системи описується її змінними стану\n\n$$\nx^1(t),x^2(t),...,x^d(t),\n$$\n\nде верхній індекс --- номер змінної. Набір із $d$ змінних стану в момент часу $t$ складає вектор стану $\\vec x(t)$ в $d$-вимірному фазовому просторі. Даний вектор переміщується в часі та в напрямі, що визначається його вектором швидкості:\n\n$$\n\\dot{\\vec x}(t)=\\partial_t\\vec x(t)=\\vec F(t).\n$$\n\nПослідовність векторів $\\vec x(t)$ утворює траєкторію у фазовому просторі, причому поле швидкості $\\vec F$ дотичне до цієї траєкторії. Еволюція траєкторії описує динаміку системи і її атрактор. Знаючи $\\vec F$, можна одержати інформацію про стан системи в момент $t$ шляхом інтегрування виразу. Оскільки форма траєкторії дозволяє судити про характер процесу (періодичні або хаотичні процеси мають характерні фазові портрети), то для визначення стану системи не обов'язково проводити інтегрування, достатньо побудувати графічне відображення траєкторії.\n\nПри дослідженні складних систем часто відсутня інформація щодо всіх змінних стану, або не всі з них можливо виміряти. Як правило, маємо єдине спостереження, проведене через дискретний часовий інтервал $\\Delta t$. Таким чином, вимірювання записуються у вигляді ряду $u_i(t)$ i, де $t=i\\cdot \\Delta t$. Інтервал $\\Delta t$ може бути постійним, проте це не завжди можливо і створює проблеми для застосування стандартних методів аналізу даних, що вимагають рівномірної шкали спостережень.\n\nВзаємодії і їх кількість у складних системах такі, що навіть за однією змінною стану можна судити про динаміку всієї системи в цілому (даний факт був встановлений групою американських учених при вивченні турбулентності). Таким чином, еквівалентна фазова траєкторія, що зберігає структури оригінальної фазової траєкторії, може бути відновлена з одного спостереження або часового ряду [@PhysRevLett.45.712] за теоремою Такенса (Takens) методом **часових затримок** [@10.1007/BFb0091924]:\n\n$$\n\\widehat{\\vec x}(t)=(u_i,u_{i+\\tau},...,u_{i+(m-1)\\tau}).\n$$\n\nТут $m$ --- розмірність вкладення, $\\tau$ --- часова затримка (реальна часова затримка визначається як $\\tau \\cdot \\Delta t$). Топологічні структури відновленої траєкторії зберігаються, якщо $m \\geq 2 \\cdot d+1$, де $d$ --- розмірність атрактора [@10.1007/BFb0091924]. На практиці у більшості випадків атрактор може бути відновлений і при $m \\leq 2d$. Затримка, як правило, вибирається апріорно.\n\nІснує кілька підходів до вибору мінімально достатньої розмірності $m$, крім аналітичного. Високу ефективність показали методи, засновані на концепції **фальшивих найближчих точок** (false nearest neighbours, FNN). Суть її заключається у тому, що при зменшенні розмірності вкладення відбувається збільшення кількості фальшивих точок, що потрапляють в околицю будь-якої точки фазового простору. Звідси витікає простий метод --- визначення кількості FNN як функції розмірності. Існують і інші методи, засновані на цій концепції, наприклад, визначення відносин відстаней між одними і тими ж сусідніми точками при різних $m$. Розмірність атрактора також може бути визначена за допомогою крос-кореляційних сум.\n\n::: {#fig-recurrence layout-ncol=2}\n\n![(a)](Images\\lab_2\\2_1.jpg){width=10cm}\n\n![(b)](Images\\lab_2\\2_2.jpg){width=10cm}\n\nВідрізок траєкторії у фазовому просторі системи Рьослера $i$ (a) та відповідний рекурентний графік (b). Вектор фазового простору в точці $j$, який потрапляє в околицю (сіре коло в (a)) заданого вектора фазового простору вектора в точці $i$ вважається точкою рекурентності (чорна точка на траєкторії в (a)). Вона позначається чорною точкою на рекурентній діаграмі у позиції $(i, j)$. Вектор фазового простору за межами околу (порожнє коло в (a)) позначається білою точкою рекурентній діаграмі\n\n:::\n\n### Рекурентний аналіз\n\nПроцесам у природі властива яскраво виражена рекурентна поведінка, така, як періодичність або іррегулярна циклічність. Більш того, рекурентність (повторюваність) станів у значенні проходження подальшої траєкторії достатньо близько до попередньої є фундаментальною властивістю дисипативних динамічних систем. Ця властивість була відмічена ще в 80-х роках XIX століття французьким математиком Пуанкаре (Poincare) і згодом сформульовано у вигляді \"теореми рекурентності\", опублікованої в 1890 р.:\n\n:::{.callout-note}\n## Примітка\n\n**Якщо система зводить свою динаміку до обмеженої підмножини фазового простору, то вона майже напевно, тобто з вірогідністю, практично рівною 1, скільки завгодно близько повертається до якого-небудь спочатку заданого режиму**\n\n:::\n\nСуть цієї фундаментальної властивості у тому, що, навіть мале збурення в складній динамічній системі може привести систему до експоненціального відхилення від її стану, через деякий час система прагне повернутися до стану близького до попереднього, і проходить при цьому подібні етапи еволюції.\n\nПереконатися в цьому можна за допомогою графічного зображення траєкторії системи у фазовому просторі. Проте можливості такого аналізу сильно обмежені. Як правило, розмірність фазового простору складної динамічної системи більша трьох, що\nробить практично незручним його розгляд напряму; єдина можливість --- проекції в дво- і тривимірні простори, що часто не дає вірного уявлення про фазовий портрет.\n\nУ 1987 р. Екман (Eckmann) і співавтори запропонували спосіб відображення $m$-вимірної фазової траєкторії станів системи $\\vec x(t)$ завдовжки $N$ на двовимірну квадратну двійкову матрицю розміром $N \\times N$ [@Eckmann_1987], в якій 1 (чорна точка) відповідає повторенню стану при деякому часі $i$ в деякий інший час $j$, а обидві координатні осі є осями часу. Таке представлення було назване **рекурентною картою** або **діаграмою** (recurrence plot, RP), оскільки воно фіксує інформацію про рекурентну поведінку системи.\n\nМатематично вищесказане описується як\n\n$$\nR_{i,j}^{m,\\varepsilon_i}=\\Theta(\\varepsilon_i-\\| \\vec x_i - \\vec x_j \\|), \\quad \\vec x \\in \\Re^m, \\quad i, j=1,...,N,\n$$\n\nде $N$ --- кількість даних станів, $x_i, \\varepsilon_i$ --- розмір околиці точки $\\vec x$ у момент $i$, $\\| \\cdot \\|$ --- норма і $\\Theta(\\cdot)$ --- функція Хевісайда.\n\nНепрактично і, як правило, неможливо знайти повну рекурентність у значенні $\\vec x_i \\equiv \\vec x_j$ (стан динамічної, а особливо --- хаотичної системи не повторюється повністю еквівалентно початковому стану, а підходить до нього скільки завгодно близько). Таким чином, рекурентність визначається як достатня близькість стану $\\vec x_j$ до стану $\\vec x_i$. Іншими словами, рекурентними є стани $\\vec x_j$, які потрапляють в $m$-вимірну околицю з радіусом $\\varepsilon_i$ і центром в $\\vec x_i$. Ці точки $\\vec x_j$ називаються **рекурентними точками** (recurrence points) [@DBLP:conf/icteri/SolovievB18;@10.1007/978-3-030-13929-2_14].\n\nОскільки $R_{i,i}=1$, $i=1,...,N$ за визначенням, то рекурентна діаграма завжди містить чорну діагональну лінію --- **лінію ідентичності** (line of identity, LOI) під кутом $\\pi/4$ до осей координат. Довільно узята рекурентна точка не несе якої-небудь корисної інформації про стани в часи $i$ і $j$. Тільки вся сукупність рекурентних точок дозволяє відновити властивості системи.\n\nЗовнішній вигляд рекурентної діаграми дозволяє судити про характер процесів, які протікають в системі, наявності і впливі шуму, станів повторення і завмирання (ламінарності), здійсненні в ході еволюції системи різких змін стану (екстремальних подій).\n\n::: {#fig-recurrence-types layout-nrow=5}\n\n![(a)](Images\\lab_2\\2_3.jpg){width=15cm}\n\n![(b)](Images\\lab_2\\2_4.jpg){width=15cm}\n\n![(c)](Images\\lab_2\\2_5.jpg){width=15cm}\n\n![(d)](Images\\lab_2\\2_6.jpg){width=15cm}\n\n![(e)](Images\\lab_2\\2_7.jpg){width=15cm}\n\n\nДинамічні ряди, що характеризують однорідність (a), дрейф (b), осциляції (c),\nконтрастну топологію (d), ламінарність (e) та побудовані для них рекурентні діаграми\n\n:::\n\n### Аналіз діаграм\n\nОчевидно, що процеси різної поведінки даватимуть рекурентні діаграми з різним рисунком. Таким чином, візуальна оцінка діаграм може дати уявлення про еволюцію досліджуваної траєкторії. Виділяють два основних класи структури зображення: **топологія** (*typology*), що представляється крупномасштабними структурами, і **текстура** (*texture*), що формується дрібномасштабними структурами.\n\nТопологія дає загальне уявлення про характер процесу. Виділяють чотири основні класи:\n\n- **однорідні** рекурентні діаграми типові для стаціонарних і автономних систем, в яких час релаксації малий у порівнянні з довжиною ряду;\n- **періодичні** структури, що повторюються (діагональні лінії, патерни у шаховому порядку) відповідають різним осцилюючим системам з періодичністю в динаміці;\n- **дрейф** відповідає системам з параметрами, що поволі змінюються, і це робить білими лівий верхній і правий нижній кути рекурентної діаграми;\n- **різкі зміни** в динаміці системи, рівно як і екстремальні ситуації, обумовлюють появу білих областей або смуг.\n\nРекурентні діаграми спрощують виявлення екстремальних і рідкісних подій.\n\n::: {#fig-recurrence-diagrams layout-ncol=4}\n\n![(a)](Images\\lab_2\\type_of_rec_a.png){width=10cm}\n\n![(b)](Images\\lab_2\\type_of_rec_b.png){width=10cm}\n\n![(c)](Images\\lab_2\\type_of_rec_c.png){width=10cm}\n\n![(d)](Images\\lab_2\\type_of_rec_d.png){width=10cm}\n\nХарактернi топологiї рекурентних дiаграм: (а) --- однорiдна (нормально розподiлений шум); (b) --- перiодична (генератор Ван дер Поля); (c) --- дрейф (вiдображення Iкеди з накладеною послiдовнiстю, що лiнiйно росте); (d) --- контрастнi областi або смуги (узагальнений броунiвський рух) [@shockley2015recurrence]\n\n:::\n\nДетальний розгляд рекурентних діаграм дозволяє виявити дрібномасштабні структури --- текстуру, яка складається з простих точок, діагональних, горизонтальних і вертикальних ліній. Комбінації вертикальних і горизонтальних ліній формують прямокутні кластери точок:\n\n- ***самотні***, окремо розташовані рекурентні точки з'являються в тому разі, коли відповідні стани рідкісні, або нестійкі в часі, або викликані сильною флуктуацією. При цьому вони не є ознаками випадковості або шуму;\n- ***діагональні лінії*** $R_{i+k, j+k}=1$ (при $k = 1...l$ де $l$ --- довжина діагональної лінії) з'являються у разі, коли сегмент траєкторії у фазовому просторі пролягає паралельно іншому сегменту, тобто траєкторія повторює саму себе, повертаючись в одну і ту ж область фазового простору у різний час. Довжина таких ліній визначається часом, протягом якого сегменти траєкторії залишаються паралельними; напрям (кут нахилу) ліній характеризує внутрішній час підпроцесів, відповідних даним сегментам траєкторії. Проходження ліній паралельно лінії ідентичності (під кутом $\\pi/4$ до осей координат) свідчить про однаковий напрям сегментів траєкторії, перпендикулярно --- про протилежний (\"відображені\" сегменти), що може також бути ознакою реконструкції фазового простору з невідповідною розмірністю вкладення. Нерегулярна поява діагональних ліній є ознакою хаотичного процесу;\n- ***вертикальні (горизонтальні) лінії*** $R_{i, j+k}=1$ (при $k = 1...\\upsilon$, де $\\upsilon$ --- довжина вертикальної або горизонтальної лінії) виділяють проміжки часу, в котрі стан системи не змінюється або змінюється не суттєво (система як би \"заморожена\" на цей час), що є ознакою \"ламінарних\" станів.\n\n::: {#fig-recurrence-concept}\n\n![](Images\\lab_2\\recurrence_lines.png){width=10cm, height=10cm}\n\nОсновнi концепцiї рекурентного аналiзу. Вiдображена дiаграма рекурентностi базується на часовому ряді, що було реконструйовано до 11 реконструйованих векторiв, вiд $\\vec{X}(0)$ до $\\vec{X}(10)$. Видiлено дiагональну лiнiю довжиною $d = 3$, вертикальну лiнiю довжиною $v = 3$ i бiлу вертикальну лiнiю довжиною $w = 5$ [@Rawald2018Scalable]\n\n:::\n\n## Хід роботи\n\nСпочатку побудуємо дво- та тривимірні фазові портрети як для модельних значень, так і для реальних. Використовуватимемо бібліотеки `neurokit2` для побудови атракторів та рекурентного аналізу.\n\n### Процедура реконструкції фазового простору\n\nДля побудови фазового портрету скористаємось методами `complexity_attractor()` та `complexity_embedding()` бібліотеки `neuralkit2`. Синтаксис `complexity_attractor()` виглядає наступним чином:\n\n**`complexity_attractor(embedded='lorenz', alpha='time', color='last_dim', shadows=True, linewidth=1, **kwargs)`**\n\n**Параметри:**\n\n- **embedded** (*Union[str, np.ndarray]*) --- результат функції `complexity_embedding()`. Також може бути рядком, наприклад, `\"lorenz\"` (атрактор Лоренца) або `\"rossler\"` (атрактор Рьосслера);\n- **alpha** (*Union[str, float]*) --- прозорість ліній;\n- **color** (*str*) --- колір графіку. Якщо `\"last_dim\"`, буде використано останній вимір (максимум 4-й) вбудованих даних, коли розмірність більша за 2. Корисно для візуалізації глибини (для 3-вимірного вбудовування), або четвертого виміру, але працюватиме це повільно;\n- **shadows** (*bool*) --- якщо значення `True`, 2D-проекції буде додано до бокових сторін 3D-атрактора;\n- **linewidth** (*float*) --- задає товщину лінії;\n- **kwargs** --- до палітри кольорів (наприклад, `name=\"plasma\"`) або до симулятора системи Лоренца передаються додаткові аргументи ключових слів, такі як `duration` (за замовчуванням = 100), `sampling_rate` (за замовчуванням = 10), `sigma` (за замовчуванням = 10), `beta` (за замовчуванням = 8/3), `rho` (за замовчуванням = 28).\n\nЯк вже зазначалося, побудова фазового простору, на основі якого і проводитиметься рекурентний аналіз, вимагає реконструкції. Виконати реконструкції фазового простору із одновимірного часового ряду можна із використанням методу часових затримок.\n\nМетод часових затримок є однією з ключових концепцій науки про складність. Він базується на ідеї, що динамічна система може бути описана вектором чисел, який називається її \"станом\", і має на меті забезпечити повний опис системи в даний момент часу. Множина всіх можливих станів називається \"простором станів\".\n\nТеорема Такенса [@10.1007/BFb0091924] припускає, що послідовність вимірювань динамічної системи містить у собі всю інформацію, необхідну для повної реконструкції простору станів. Метод часових затримок намагається визначити стан $s$ системи в певний момент часу $t$, шукаючи в минулій історії спостережень схожі стани, і, вивчаючи еволюцію схожих станів, виводити інформацію про майбутнє системи.\n\nЯк візуалізувати динаміку системи? Послідовність значень стану в часі називається траєкторією. Залежно від системи, різні траєкторії можуть еволюціонувати до спільної підмножини простору станів, яка називається атрактором. Наявність та поведінка атракторів дає інтуїтивне уявлення про досліджувану динамічну систему.\n\nОдже, згідно Такенсу, ідея полягає в тому, щоб на основі одиничних вимірювань системи, отримати $m$-розмірні реконструйовані часові вкладення\n\n$$\n\\vec{x}_i = \\left( x_i, x_{i+\\tau}, ... , x_{i+(m-1)\\tau} \\right),\n$$ {#eq-2-1}\n\nа $i$ проходить в діапазоні $1,..., N-(m-1)\\tau$; значення $\\tau$ представляє часову затримку, а $m$ --- це розмірність вкладень (кількість змінних, що включає кожна траєкторія).\n\nКод для реконструкції фазового простору може виглядати наступним чином:\n\nДля реконструкції фазового простору використовуватимемо метод `complexity_embedding()`. Його синтаксис:\n\n**`complexity_embedding(signal, delay=1, dimension=3, show=False, **kwargs)`**\n\n**Параметри:**\n\n- **signal** (*Union[list, np.array, pd.Series]*) --- сигнал (тобто часовий ряд) у вигляді вектора значень. Також може бути рядком, наприклад, `\"lorenz\"` (атрактор Лоренца), `\"rossler\"` (атрактор Росслера) або `\"clifford\"` (атрактор Кліффорда) для отримання попередньо визначеного атрактора;\n- **delay** (*int*) --- часова затримка (часто позначається $\\tau$ іноді називають запізненням). Ще розглянемо метод `complexity_delay()` для оцінки оптимального значення цього параметра;\n- **dimension** (*int*) --- розмірність вкладень ($m$, іноді позначається як $d$). Далі звернемось до методу `complexity_dimension()`, щоб оцінити оптимальне значення для цього параметра;\n- **show** (*bool*) --- побудувати графік реконструйованого атрактора;\n- **kwargs** --- інші аргументи, що передаються до `complexity_attractor()`.\n\n**Повертає:**\n\n- *array* --- реконструйований атрактор розміру `length - (dimension - 1) * delay`.\n\nДалі імпортуємо необхідні для подальшої роботи модулі:\n\nІ виконаємо налаштування рисунків для виводу:\n\nТепер розглянемо можливість використання методу часових затримок і отриманих у подальшому атракторів у якості індикатора складності. Як і в попередній роботі, для прикладу завантажимо часовий ряд Біткоїна за період з 1 вересня 2015 по 1 березня 2020, використовуючи `yfinance`:\n\n::: {.callout-warning}\n## Увага\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance, а із власного файлу. Зрозуміло, що й аналіз результатів, і висновки залежать від того з яким рядом ми працюємо\n\n:::\n\n---\n\n---\n\nВиводимо досліджуваний ряд:\n\nСпочатку оберемо вид ряду: 1 - вихідний ряд; 2 - детермінований (різниця між теперішнім та попереднім значенням); 3 - прибутковості звичайні; 4 - стандартизовані прибутковості; 5 - абсолютні значення (волатильності); 6 - стандартизований ряд.\n\nДля подальших розрахунків накращим варіантом буде вибір стандартизованого вихідного ряду або прибутковостей, оскільки значення вихідного часового ряду відрізняються на декілька порядків, і можуть сильно перевищувати встановлений параметр $\\varepsilon$. У цьому випадку для вихідних значень, що сильно різняться між собою, увесь часовий діапазон буде розглядатися як нерекурентний.\n\nСпочатку визначимо функції для виконання перетворення ряду:\n\nі тепер виконаємо перетворення, використовуючи дану функцію:\n\nОскільки ми не матимемо змоги візуалізувати багатовимірний фазовий простір ($m>3$), ми послуговуватимемось значеннями $m=2$ та $m=3$. Значення $\\tau$ будемо варіювати як із власних переконань, так і з опорою на функціонал бібліотеки `neuralkit2`.\n\nСкористаємось методом `complexity_simulate()` для генерації різних тестових сигналів:\n\n::: {.callout-important}\n## Автоматизований підбір параметрів\n\nУ зазначених вище прикладах ми обирали параметри $m$ і $\\tau$ згідно нашими власними міркуваннями. Але на практиці бажано було б, щоб зазначені параметри обирались автоматично, спираючись на конкретну статистичну процедуру. Бібліотека `neurokit2` представляє функціонал для автоматичного підбору параметрів розмірності та часової затримки. У завданнях самостійної роботи буде надано короткий опис алгоритмів для автоматичного підбору розмірності атрактору та часової затримки. В основній частині лабораторної роботи для побудови рекурентних діаграм будуть використанні самостійно підібрані значення $m$ і $\\tau$\n\n:::\n\n\n\n### Побудова рекурентної матриці\n\nЯк вже зазначалося, рекурентний аналіз на основі реконструйованих траєкторій фазового простору визначає кількість і тривалість рекурентних станів динамічної системи.\n\nМи маємо змогу побудувати рекурентну матрицю, використовуючи метод `recurrence_matrix()`.\n\nЙого синтаксис виглядає наступним чином:\n\n**`recurrence_matrix(signal, delay=1, dimension=3, tolerance='default', show=False)`**\n\n**Параметри:**\n\n- **signal** (*Union[list, np.ndarray, pd.Series]*) --- сигнал у вигляді вектора значень;\n- **delay** (*int*) --- затримка в часі;\n- **dimension** (*int*) --- розмірність вкладень, $m$;\n- **tolerance** (*float*) --- радіус $\\varepsilon$ багатовимірного околу, в межах якого шукаються рекурентні траєкторії, а дві точки даних вважаються схожими. Якщо `\"sd\"` (за замовчуванням), буде встановлено значення $0.2 \\cdot SD_{signal}$. Емпіричним правилом є встановлення $\\varepsilon$ таким чином, щоб відсоток точок, класифікованих як рекурентні, становив приблизно 2-5%;\n- **show** (*bool*) --- візуалізувати рекурентну матрицю.\n\n**Повертає:**\n\n- *np.ndarray* --- рекурентну матрицю;\n- *np.ndarray* --- матрицю відстаней.\n\nПобудуємо рекурентну матрицю для фрагменту вихідних значень Біткоїна, його прибутковостей та стандартизованого фоагменту його вихідного ряду. Розмірність $m=4$, часова затримка $\\tau=1$. Спробуємо різні варіанти $\\varepsilon$:\n\n- для вихідного ряду $\\varepsilon=100$;\n- для прибутковостей та стандартизованого ряду $\\varepsilon=0.8$.\n\n::: {.callout-note}\n## Примітка для побудови матриці рекурентності\n\nЯк уже зазначалось на початку, рекурентна діаграма --- це двовимірна квадратна двійкова матриця розміром $N^2$. Одним із її основних недоліків є те, що вона погано масштабується на великих даних. Наприклад, якщо ви плануєте досліджувати часовий ряд довжиною 1000000 значень, тоді рекурентна матриця буде складатись із 1000000000000 білих і чорних точок, що може бути викликом для вашого процесора та оперативної пам'яті. Тому в подальшому ми пропонуємо будувати рекурентну матрицю не для всього ряду, а тільки для його підмножини. Для цього ми визначимо змінні початкового (`idx_beg`) та кінцевого (`inx_end`) відліку в межах якого буде здійснюватись побудова рекурентної матриці\n\n:::\n\nЯк можна бачити з рисунку [-@fig-btc-rec-init] всі траєкторії для простору вихідних значень за абсолютною шкалою залишаються доволі віддаленими один від одного. Для розпізнавання рекурентних закономірностей нам потребується поступово нарощувати $\\varepsilon$. З цього рисунку видно, що $\\varepsilon=100$ буде замало.  \n\nТепер спробуємо трохи нормалізувати значення вихідного фрагменту Біткоїна, аби реконструйовані траєкторії не знаходились занадто віддаленими один від одного. Для цього розрахуємо стандартизовані прибутковості:\n\nТепер можемо бачити, що Біткоїн став характризуватися чорними смугами, що відображають динаміку певних детермінованих процесів. У той же час білі смуги характеризують періоди абсолютно аномальної (непередбачуваної) поведінки на даному ринку. Видно, що прибутковості залишаються доволі некорельованими, про що і свідчить переважне домінування саме білих областей.\n\nСпробуємо тепер подивитись на стандартизований вихідний ряд:\n\nНа початку свого існування Біткоїн характеризувався доволі високим ступенем передбачуваності, низькою волатильністю коливань. Надалі почали домінувати білі області, а зараз Біткоїну властива динаміка схожа з броунівським рухом.\n\n## Висновок\n\nУ даній лабораторній роботі було продемонстровано можливість дослідження складних нелінійних систем із використанням рекурентних діаграм. На прикладі Біткоїна було показано, що складним фінансовим систем властива топологія узагальненого броунівського руху, де рекурентність може варіюватись із плином часу. Тобто, той самий криптовалютний ринок характеризується мінливістю ступеня передбачуваності. Змога знаходження станів передбачуваності з точки зору рекурентних діаграм надає перспективи для їх використання разом із методами прогнозування та побудови індикаторів-передвісників.\n\n## Завдання для самостійної роботи\n\n1. Отримати індекс часового ряду у викладача\n2. Провести дослідження його рекурентних властивостей згідно інструкції\n3. Порівняти фазові портрети і рекурентні діаграми для стандартизованого вихідного ряду та прибутковостей. Що спільного між ними і чим вони відрізняються?\n4. Провести побудову фазових портретів і рекурентних діаграм для вашого часового ряду із використанням процедур автоматичного підбору параметрів розмірності вкладень і часової затримки. Порівняти результати з тими, що були отримані без використання даних методів і зробити висновки\n\n::: {.callout-important}\n## Для виконання 4-го завдання самостійної роботи\n\nДалі надається опис алгоритмів для автоматизованого підбору розмірності реконструйованого фазового простору та часової затримки\n\n:::\n\n\n\n### Автоматизований підбір параметра часової затримки, $\\tau$\n\nЧасова затримка $\\tau$ (також відома як *L*) є одним з двох критичних параметрів, що беруть участь у процедурі реконструкції фазового простору. Значення $L$ відповідає затримці у відліках між вихідним сигналом і його затриманою версією (версіями). Іншими словами, скільки відліків ми розглядаємо між певним станом сигналу та його найближчим минулим станом.\n\nЯкщо $\\tau$ менше оптимального теоретичного значення, послідовні координати стану системи корельовані і атрактор недостатньо розгорнутий. І навпаки, коли $\\tau$ більше, ніж повинно бути, послідовні координати майже незалежні, що призводить до некорельованої та неструктурованої хмари точок.\n\nВибір параметрів *затримки* та *розмірності* представляє нетривіальну задачу. Один з підходів полягає у їх (напів)незалежному виборі (оскільки вибір розмірності часто вимагає затримки) за допомогою функцій `complexity_delay()` та `complexity_dimension()`. Однак, існують методи спільного оцінювання, які намагаються знайти оптимальну затримку та розмірність одночасно.\n\nЗауважте також, що деякі автори (наприклад, Розенштейн, 1994) пропонують спочатку визначити оптимальну розмірність вбудовування, а потім розглядати оптимальне значення затримки як оптимальну затримку між першою та останньою координатами затримки (іншими словами, фактична затримка має дорівнювати оптимальній затримці, поділеній на оптимальну розмірність вбудовування мінус 1).\n\nДекілька авторів запропонували різні методи для вибору затримки:\n\n- **Фрейзер і Свінні (1986)** [@PhysRevA.33.1134] пропонують використовувати перший локальний мінімум взаємної інформації між затриманим і незатриманим часовими рядами, ефективно визначаючи значення $\\tau$, для якого вони діляться найменшою інформацією (і де атрактор є найменш надлишковим). На відміну від автокореляції, взаємна інформація враховує також нелінійні кореляції;\n- **Тейлер (1990)** [@PhysRevA.41.3038] запропонував вибирати таке значення $\\tau$, при якому автокореляція між сигналом та його зміщенною версією при $\\tau$ вперше перетинає значення $1/e$. Методи, що базуються на автокореляції, мають перевагу за часом обчислень, коли вони знаходяться за допомогою алгоритму швидкого перетворення Фур'є (fast Fourier transform, FFT);\n- **Касдаглі (1991)** [@CASDAGLI199152] пропонує замість цього брати перший нульовий перетин автокореляції;\n- **Розенштейн (1993, 1994)** [@ROSENSTEIN1993117;@ROSENSTEIN199482] вважає, що слід апроксимувати точку, де функція автокореляцій падає до $\\left( 1-1/e \\right)$ від свого максимального значення. Або ж наближатися до точки, близької до 40% нахилу середнього зміщення від діагоналі;\n- **Кім (1999)** [@KIM199948] оцінює $\\tau$ за допомогою кореляційного інтегралу, який називається C-C методом, і який, як виявилося, узгоджується з результатами, отриманими за допомогою методу взаємної інформації. Цей метод використовує статистику в реконструйованому фазовому просторі, а не аналізує часову еволюцію ряду. Однак час обчислень є значно довшим через необхідність порівнювати кожну унікальну пару парних векторів у реконструйованому сигналі на кожну затримку;\n- **Лайл (2021)** [@10.3389/fcvm.2021.709457] описує \"Реконструкцію симетричного проекційного атрактора\" (Symmetric Projection Attractor Reconstruction, SPAR), де $1/3$ від домінуючої частоти (тобто довжини середнього \"циклу\") може бути підходящим значенням для приблизно періодичних даних, і робить атрактор чутливим до морфологічних змін. Див. також [доповідь Астона](https://youtu.be/GGrOJtcTcHA?t=730). Цей метод також є найшвидшим, але може не підходити для аперіодичних сигналів. Аргумент `algorithm` (за замовчуванням `\"fft\"`) передається до аргументу `method` методу [`signal_psd()`](https://neuropsychology.github.io/NeuroKit/functions/signal.html#signal-psd).\n\nМожна також відмітити метод для об'єднаного підбору параметрів затримки та розмірності.\n\n- **Гаутама (2003)** [@lirias1573905] зазначає, що на практиці часто використовують фіксовану часову затримку і відповідно регулюють розмірність вбудовування. Оскільки це може призвести до великих значень $m$ (а отже, до вкладених даних великого розміру) і, відповідно, до повільної обробки, використовується метод оптимізації для спільного визначення $m$ і $\\tau$ на основі показника **entropy ratio**.\n\nРозглянемо оптимальні значення розмірності та затримки для часового сигналу Біткоїна:\n\n@fig-btc-delay-fraser1986 показує, що перший локальний мінімум взаємної інформації для стандартизованих вихідних значень Біткоїна знаходиться на 273 лагу. Для візуального огляду реконструйованого атрактора це значення, можливо, є найбільш адекватним. Але використовуючи настільки велику часову затримку, ми втрачаємо доволі багато проміжних значень, що також можуть містити досить важливу приховану інформацію для кількісних розрахунків.\n\n@fig-btc-delay-theiler демонструє, що автокореляція між стандартизованих вихідним сигналом Біткоїна та його зміщенною версією при $\\tau=195$ вперше перетинає значення $1/\\exp$. Бачимо, що дане значення затримки є трохи меншим за те, що було отримано до цього, але суті це не змінює. Також бачимо, що між реконструйованими атракторами для $\\tau=195$ та $\\tau=273$ немає кардинальної візуальної різниці.\n\nЯк можна бачити по прикладу вище, не всі методи надають адекватну оцінку розмірності нашого сигналу. Спробуємо привести вихідні значення Біткоїна до прибутковостей та повторити процедуру Касдаглі ще раз.  \n\nЦього разу нам вдалося досягти оптимального результату, але приклад вище демонструє, що кожна процедура має свої виключення. @fig-btc-delay-casdagli1991 показує, що значення прибутковостей Біткоїна характеризуються певними кореляціями лише на перших 4-ох лагах. Подальші часові зміщення роблять значення прибутковостей незалежними один від одного.\n\n@fig-btc-delay-rosenstein1993 демонструє, що при $\\tau=101$ функція автокореляцій перетинає значення $\\left( 1-1/e \\right)$. При цьому видно, що навіть для такого лагу зберігається значна частка кореляцій між стандартизованими вихідними значеннями Біткоїна.\n\nРисунок вище показує, що при $\\tau=120$ зміщення реконструйованих траєкторій від їх оригінального положення на лінії ідентичності зберігає найбільшу кількість інформації стосовно атрактора стандартизованих значень Біткоїна.\n\nЗгідно представленого вище результату найбільш значущі частоти, отримані за допомогою перетворення Фур'є, зберігаються при $\\tau=109$.\n\nТепер подивимось як це виглядатиме для об'єднаного підбору параметрів:\n\nОскільки представлена вище процедура є доволі громіздкою в плані обчислювальних потужностей, ми обрали діапазон $\\tau$ в межах від 1 до 10. Видно, що при $\\tau$ близької до 3 оптимальне значення розмірності атрактора дорівнює 10. Можливо, при значеннях $\\tau$ близьких до 100 або 200, ми могли б отримати зовсім інше значення розмірності, але це потребує додаткових експериментів.\n\n### Автоматизований підбір параметра розмірності вкладень, $m$\n\nЗа дану процедуру відповідає метод `complexity dimension()`. Її синтаксис виглядає наступним чином:\n\n**`complexity_dimension(signal, delay=1, dimension_max=20, method='afnn', show=False, **kwargs)`**\n\nХоча зазвичай використовують $m=2$ або $m=3$, але різні автори пропонують наступні процедури підбору:\n\n- **кореляційна розмірність (Correlation Dimension, CD)**: Одним з перших методів оцінки оптимального $m$ був розрахунок кореляційної розмірності для вкладень різного розміру і пошук насичення (тобто плато) в її значенні при збільшенні розміру векторів [@GRASSBERGER1983189;@PhysRevLett.50.346;@GRASSBERGER1983227]. Одне з обмежень полягає в тому, що насичення буде також мати місце, коли даних недостатньо для адекватного заповнення простору високої розмірності (зауважте, що в загальному випадку не рекомендується мати настільки великі вкладення, оскільки це значно скорочує довжину сигналу);\n- **найближчі хибні сусіди (False Nearest Neighbour, FNN)**: Метод, запропонований Кеннелом та ін. [@PhysRevA.45.3403;@krakovska2015use;@RHODES1997S1149], базується на припущенні, що дві точки, які є близькими одна до одної в достатній розмірності вбудовування, повинні залишатися близькими при збільшенні розмірності. Алгоритм перевіряє сусідів при збільшенні розмірності вкладень, поки не знайде лише незначну кількість хибних сусідів при переході від розмірності $m$ до $m+1$. Це відповідає найнижчій розмірності вкладення, яка, як передбачається, дає розгорнуту реконструкцію просторово-часового стану. Цей метод може не спрацювати в зашумлених сигналах через марну спробу розгорнути шум (а в чисто випадкових сигналах кількість хибних сусідів суттєво не зменшується зі збільшенням $m$). На рисунку нижче (@fig-fnn) показано, як проекції на простори більшої розмірності можна використовувати для виявлення хибних найближчих сусідів. Наприклад, червона та жовта точки є сусідами в одновимірному просторі, але не в двовимірному;\n\n::: {#fig-fnn}\n\n![](Images\\lab_2\\fnn.jpg){width=10cm, height=10cm}\n\nОсновна ідея методу FNN. Найближчі сусіди зеленої точки з'являються у випадку 1-, 2- та 3-вимірних фазових просторів [@STAVRINIDES2022112224]\n\n:::\n\n- **середні хибні сусіди (Average False Neighbors, AFN)**: Ця модифікація методу FNN розроблена Сао (1997) [@CAO199743] і усуває один з його основних недоліків --- необхідність евристичного вибору порогових значень $r$. Метод використовує максимальну евклідову відстань для представлення найближчих сусідів і усереднює всі відношення відстані в $m+1$ розмірності до розмірності $m$ та визначає *E1* та *E2* як параметри. Оптимальна розмірність досягається тоді, коли *E1* перестає змінюватися (виходить на плато). Це відбувається при розмірності *d0*, якщо сигнал надходить від атрактора. Тоді *d0*+1* є оптимальною мінімальною розмірністю вкладення. *E2* є корисною величиною для того, щоб відрізнити детерміновані сигнали від стохастичних. Константа *E2*, що близька до 1 для будь-якої розмірності вкладень $d$, вказує на випадковість даних, оскільки майбутні значення не залежать від минулих.\n\n**Параметри:**\n\n- **signal** (*Union[list, np.array, pd.Series]*) --- сигнал (тобто часовий ряд) у вигляді вектора значень;\n- **delay** (*int*) --- часова затримка у відліках. Для вибору оптимального значення цього параметра ми ще скористаємось методом `complexity_delay()`;\n- **dimension_max** (*int*) --- максимальний розмір вкладення для тестування;\n- **method** (*str*) --- може бути `\"afn\"` (середні хибні сусіди), `\"fnn\"` (найближчий хибний сусід) або `\"cd\"` (кореляційна розмірність);\n- **show** (*bool*) --- візуалізувати результат;\n- **kwargs** --- інші аргументи, такі як $R=10.0$ або $A=2.0$ (відносне та абсолютне граничне значення, тільки для методу `\"fnn\"`).\n\n**Повертає:**\n\n- **dimension** (*int*) --- оптимальна розмірність вкладень;\n- **parameters** (*dict*) --- словник python, що містить додаткову інформацію про параметри, які використовуються для обчислення оптимальної розмірності.\n\nСпробуємо отримати оптимальне значення розмірності згідно зазначених процедур. В якості часової затримки можна взяти $\\tau=100$. Приблизно таке значення спостерігалося для кожної процедури.\n\n@fig-btc-dim-cd демонструє той факт, що оптимальна розмірність вкладень, за якої досягається найбільш інформативна репрезентація фазового простору, дорівнює 7.  \n\nЗ рисунку [-@fig-btc-dim-fnn] видно, що мінімальна розмірність вкладення дорівнює 3. Саме при переході від 3-ох вимірного фазового простору до 4-ох вимірного ми бачимо, що кількість хибних сусідів стає мінімальною і далі не зростає.\n\nАлгоритм середніх хибних сусідів показує, що тут розмірність вкладень $m=5$ є оптимальною. При подальшому зростанні розмірності, атрактор стає більш стохастичним, що вказує на втрату всіх кореляцій.  \n\nЗгідно з представленими вище алгоритмами автоматичного підбору, розмірність вкладень можна обирати в діапазоні значень від 3 до 7. Тепер на основі отриманих результатів приступимо до побудови рекурентної діаграми.\n","srcMarkdownNoYaml":"\n\n\n\n\n**Тема.** Використання рекурентного аналізу для моделювання і прогнозування\nнелінійних динамічних властивостей складних систем\n\n**Мета.** Навчитися інструментарію нелінійної динаміки, який відноситься до\nрекурентних властивостей нестаціонарних динамічних рядів\n\n\n\n## Теоретичні відомості\n\nДослідження складних систем, як природних, так і штучних, показали, що в їх основі лежать нелінійні процеси, ретельне вивчення яких необхідне для розуміння і моделювання складних систем. У останні десятиліття набір традиційних (лінійних) методик дослідження був істотно розширений нелінійними методами, одержаними з теорії нелінійної динаміки і хаосу; багато досліджень були присвячені оцінці нелінійних характеристик і властивостей процесів, що протікають в природі (скейлінг, фрактальна розмірність). Проте більшість методів нелінійного аналізу вимагає або достатньо довгих, або стаціонарних рядів даних, які досить важко одержати в природний спосіб. Більш того, було показано, що дані методи дають задовільні результати для моделей реальних систем, що ідеалізуються. Ці чинники вимагали розробки нових методик нелінійного аналізу даних.\n\nСтан природних або штучних систем, як правило, змінюється в часі. Вивчення цих, часто складних процесів --- важлива задача в багатьох дисциплінах, дозволяє зрозуміти і описати їх суть, наприклад, для прогнозування стану на деякий час у майбутнє. Метою таких досліджень є знаходження математичних моделей, які б достатньо відповідали реальним процесам і могли б бути використані для розв'язання поставлених задач.\n\nРозглянемо ідею і коротко опишемо теорію рекурентного аналізу, наведемо деякі приклади, розглянемо його можливі області застосування при аналізі і прогнозування складних фінансово-економічних систем.\n\n### Фазовий простір та його реконструкція\n\nСтан системи описується її змінними стану\n\n$$\nx^1(t),x^2(t),...,x^d(t),\n$$\n\nде верхній індекс --- номер змінної. Набір із $d$ змінних стану в момент часу $t$ складає вектор стану $\\vec x(t)$ в $d$-вимірному фазовому просторі. Даний вектор переміщується в часі та в напрямі, що визначається його вектором швидкості:\n\n$$\n\\dot{\\vec x}(t)=\\partial_t\\vec x(t)=\\vec F(t).\n$$\n\nПослідовність векторів $\\vec x(t)$ утворює траєкторію у фазовому просторі, причому поле швидкості $\\vec F$ дотичне до цієї траєкторії. Еволюція траєкторії описує динаміку системи і її атрактор. Знаючи $\\vec F$, можна одержати інформацію про стан системи в момент $t$ шляхом інтегрування виразу. Оскільки форма траєкторії дозволяє судити про характер процесу (періодичні або хаотичні процеси мають характерні фазові портрети), то для визначення стану системи не обов'язково проводити інтегрування, достатньо побудувати графічне відображення траєкторії.\n\nПри дослідженні складних систем часто відсутня інформація щодо всіх змінних стану, або не всі з них можливо виміряти. Як правило, маємо єдине спостереження, проведене через дискретний часовий інтервал $\\Delta t$. Таким чином, вимірювання записуються у вигляді ряду $u_i(t)$ i, де $t=i\\cdot \\Delta t$. Інтервал $\\Delta t$ може бути постійним, проте це не завжди можливо і створює проблеми для застосування стандартних методів аналізу даних, що вимагають рівномірної шкали спостережень.\n\nВзаємодії і їх кількість у складних системах такі, що навіть за однією змінною стану можна судити про динаміку всієї системи в цілому (даний факт був встановлений групою американських учених при вивченні турбулентності). Таким чином, еквівалентна фазова траєкторія, що зберігає структури оригінальної фазової траєкторії, може бути відновлена з одного спостереження або часового ряду [@PhysRevLett.45.712] за теоремою Такенса (Takens) методом **часових затримок** [@10.1007/BFb0091924]:\n\n$$\n\\widehat{\\vec x}(t)=(u_i,u_{i+\\tau},...,u_{i+(m-1)\\tau}).\n$$\n\nТут $m$ --- розмірність вкладення, $\\tau$ --- часова затримка (реальна часова затримка визначається як $\\tau \\cdot \\Delta t$). Топологічні структури відновленої траєкторії зберігаються, якщо $m \\geq 2 \\cdot d+1$, де $d$ --- розмірність атрактора [@10.1007/BFb0091924]. На практиці у більшості випадків атрактор може бути відновлений і при $m \\leq 2d$. Затримка, як правило, вибирається апріорно.\n\nІснує кілька підходів до вибору мінімально достатньої розмірності $m$, крім аналітичного. Високу ефективність показали методи, засновані на концепції **фальшивих найближчих точок** (false nearest neighbours, FNN). Суть її заключається у тому, що при зменшенні розмірності вкладення відбувається збільшення кількості фальшивих точок, що потрапляють в околицю будь-якої точки фазового простору. Звідси витікає простий метод --- визначення кількості FNN як функції розмірності. Існують і інші методи, засновані на цій концепції, наприклад, визначення відносин відстаней між одними і тими ж сусідніми точками при різних $m$. Розмірність атрактора також може бути визначена за допомогою крос-кореляційних сум.\n\n::: {#fig-recurrence layout-ncol=2}\n\n![(a)](Images\\lab_2\\2_1.jpg){width=10cm}\n\n![(b)](Images\\lab_2\\2_2.jpg){width=10cm}\n\nВідрізок траєкторії у фазовому просторі системи Рьослера $i$ (a) та відповідний рекурентний графік (b). Вектор фазового простору в точці $j$, який потрапляє в околицю (сіре коло в (a)) заданого вектора фазового простору вектора в точці $i$ вважається точкою рекурентності (чорна точка на траєкторії в (a)). Вона позначається чорною точкою на рекурентній діаграмі у позиції $(i, j)$. Вектор фазового простору за межами околу (порожнє коло в (a)) позначається білою точкою рекурентній діаграмі\n\n:::\n\n### Рекурентний аналіз\n\nПроцесам у природі властива яскраво виражена рекурентна поведінка, така, як періодичність або іррегулярна циклічність. Більш того, рекурентність (повторюваність) станів у значенні проходження подальшої траєкторії достатньо близько до попередньої є фундаментальною властивістю дисипативних динамічних систем. Ця властивість була відмічена ще в 80-х роках XIX століття французьким математиком Пуанкаре (Poincare) і згодом сформульовано у вигляді \"теореми рекурентності\", опублікованої в 1890 р.:\n\n:::{.callout-note}\n## Примітка\n\n**Якщо система зводить свою динаміку до обмеженої підмножини фазового простору, то вона майже напевно, тобто з вірогідністю, практично рівною 1, скільки завгодно близько повертається до якого-небудь спочатку заданого режиму**\n\n:::\n\nСуть цієї фундаментальної властивості у тому, що, навіть мале збурення в складній динамічній системі може привести систему до експоненціального відхилення від її стану, через деякий час система прагне повернутися до стану близького до попереднього, і проходить при цьому подібні етапи еволюції.\n\nПереконатися в цьому можна за допомогою графічного зображення траєкторії системи у фазовому просторі. Проте можливості такого аналізу сильно обмежені. Як правило, розмірність фазового простору складної динамічної системи більша трьох, що\nробить практично незручним його розгляд напряму; єдина можливість --- проекції в дво- і тривимірні простори, що часто не дає вірного уявлення про фазовий портрет.\n\nУ 1987 р. Екман (Eckmann) і співавтори запропонували спосіб відображення $m$-вимірної фазової траєкторії станів системи $\\vec x(t)$ завдовжки $N$ на двовимірну квадратну двійкову матрицю розміром $N \\times N$ [@Eckmann_1987], в якій 1 (чорна точка) відповідає повторенню стану при деякому часі $i$ в деякий інший час $j$, а обидві координатні осі є осями часу. Таке представлення було назване **рекурентною картою** або **діаграмою** (recurrence plot, RP), оскільки воно фіксує інформацію про рекурентну поведінку системи.\n\nМатематично вищесказане описується як\n\n$$\nR_{i,j}^{m,\\varepsilon_i}=\\Theta(\\varepsilon_i-\\| \\vec x_i - \\vec x_j \\|), \\quad \\vec x \\in \\Re^m, \\quad i, j=1,...,N,\n$$\n\nде $N$ --- кількість даних станів, $x_i, \\varepsilon_i$ --- розмір околиці точки $\\vec x$ у момент $i$, $\\| \\cdot \\|$ --- норма і $\\Theta(\\cdot)$ --- функція Хевісайда.\n\nНепрактично і, як правило, неможливо знайти повну рекурентність у значенні $\\vec x_i \\equiv \\vec x_j$ (стан динамічної, а особливо --- хаотичної системи не повторюється повністю еквівалентно початковому стану, а підходить до нього скільки завгодно близько). Таким чином, рекурентність визначається як достатня близькість стану $\\vec x_j$ до стану $\\vec x_i$. Іншими словами, рекурентними є стани $\\vec x_j$, які потрапляють в $m$-вимірну околицю з радіусом $\\varepsilon_i$ і центром в $\\vec x_i$. Ці точки $\\vec x_j$ називаються **рекурентними точками** (recurrence points) [@DBLP:conf/icteri/SolovievB18;@10.1007/978-3-030-13929-2_14].\n\nОскільки $R_{i,i}=1$, $i=1,...,N$ за визначенням, то рекурентна діаграма завжди містить чорну діагональну лінію --- **лінію ідентичності** (line of identity, LOI) під кутом $\\pi/4$ до осей координат. Довільно узята рекурентна точка не несе якої-небудь корисної інформації про стани в часи $i$ і $j$. Тільки вся сукупність рекурентних точок дозволяє відновити властивості системи.\n\nЗовнішній вигляд рекурентної діаграми дозволяє судити про характер процесів, які протікають в системі, наявності і впливі шуму, станів повторення і завмирання (ламінарності), здійсненні в ході еволюції системи різких змін стану (екстремальних подій).\n\n::: {#fig-recurrence-types layout-nrow=5}\n\n![(a)](Images\\lab_2\\2_3.jpg){width=15cm}\n\n![(b)](Images\\lab_2\\2_4.jpg){width=15cm}\n\n![(c)](Images\\lab_2\\2_5.jpg){width=15cm}\n\n![(d)](Images\\lab_2\\2_6.jpg){width=15cm}\n\n![(e)](Images\\lab_2\\2_7.jpg){width=15cm}\n\n\nДинамічні ряди, що характеризують однорідність (a), дрейф (b), осциляції (c),\nконтрастну топологію (d), ламінарність (e) та побудовані для них рекурентні діаграми\n\n:::\n\n### Аналіз діаграм\n\nОчевидно, що процеси різної поведінки даватимуть рекурентні діаграми з різним рисунком. Таким чином, візуальна оцінка діаграм може дати уявлення про еволюцію досліджуваної траєкторії. Виділяють два основних класи структури зображення: **топологія** (*typology*), що представляється крупномасштабними структурами, і **текстура** (*texture*), що формується дрібномасштабними структурами.\n\nТопологія дає загальне уявлення про характер процесу. Виділяють чотири основні класи:\n\n- **однорідні** рекурентні діаграми типові для стаціонарних і автономних систем, в яких час релаксації малий у порівнянні з довжиною ряду;\n- **періодичні** структури, що повторюються (діагональні лінії, патерни у шаховому порядку) відповідають різним осцилюючим системам з періодичністю в динаміці;\n- **дрейф** відповідає системам з параметрами, що поволі змінюються, і це робить білими лівий верхній і правий нижній кути рекурентної діаграми;\n- **різкі зміни** в динаміці системи, рівно як і екстремальні ситуації, обумовлюють появу білих областей або смуг.\n\nРекурентні діаграми спрощують виявлення екстремальних і рідкісних подій.\n\n::: {#fig-recurrence-diagrams layout-ncol=4}\n\n![(a)](Images\\lab_2\\type_of_rec_a.png){width=10cm}\n\n![(b)](Images\\lab_2\\type_of_rec_b.png){width=10cm}\n\n![(c)](Images\\lab_2\\type_of_rec_c.png){width=10cm}\n\n![(d)](Images\\lab_2\\type_of_rec_d.png){width=10cm}\n\nХарактернi топологiї рекурентних дiаграм: (а) --- однорiдна (нормально розподiлений шум); (b) --- перiодична (генератор Ван дер Поля); (c) --- дрейф (вiдображення Iкеди з накладеною послiдовнiстю, що лiнiйно росте); (d) --- контрастнi областi або смуги (узагальнений броунiвський рух) [@shockley2015recurrence]\n\n:::\n\nДетальний розгляд рекурентних діаграм дозволяє виявити дрібномасштабні структури --- текстуру, яка складається з простих точок, діагональних, горизонтальних і вертикальних ліній. Комбінації вертикальних і горизонтальних ліній формують прямокутні кластери точок:\n\n- ***самотні***, окремо розташовані рекурентні точки з'являються в тому разі, коли відповідні стани рідкісні, або нестійкі в часі, або викликані сильною флуктуацією. При цьому вони не є ознаками випадковості або шуму;\n- ***діагональні лінії*** $R_{i+k, j+k}=1$ (при $k = 1...l$ де $l$ --- довжина діагональної лінії) з'являються у разі, коли сегмент траєкторії у фазовому просторі пролягає паралельно іншому сегменту, тобто траєкторія повторює саму себе, повертаючись в одну і ту ж область фазового простору у різний час. Довжина таких ліній визначається часом, протягом якого сегменти траєкторії залишаються паралельними; напрям (кут нахилу) ліній характеризує внутрішній час підпроцесів, відповідних даним сегментам траєкторії. Проходження ліній паралельно лінії ідентичності (під кутом $\\pi/4$ до осей координат) свідчить про однаковий напрям сегментів траєкторії, перпендикулярно --- про протилежний (\"відображені\" сегменти), що може також бути ознакою реконструкції фазового простору з невідповідною розмірністю вкладення. Нерегулярна поява діагональних ліній є ознакою хаотичного процесу;\n- ***вертикальні (горизонтальні) лінії*** $R_{i, j+k}=1$ (при $k = 1...\\upsilon$, де $\\upsilon$ --- довжина вертикальної або горизонтальної лінії) виділяють проміжки часу, в котрі стан системи не змінюється або змінюється не суттєво (система як би \"заморожена\" на цей час), що є ознакою \"ламінарних\" станів.\n\n::: {#fig-recurrence-concept}\n\n![](Images\\lab_2\\recurrence_lines.png){width=10cm, height=10cm}\n\nОсновнi концепцiї рекурентного аналiзу. Вiдображена дiаграма рекурентностi базується на часовому ряді, що було реконструйовано до 11 реконструйованих векторiв, вiд $\\vec{X}(0)$ до $\\vec{X}(10)$. Видiлено дiагональну лiнiю довжиною $d = 3$, вертикальну лiнiю довжиною $v = 3$ i бiлу вертикальну лiнiю довжиною $w = 5$ [@Rawald2018Scalable]\n\n:::\n\n## Хід роботи\n\nСпочатку побудуємо дво- та тривимірні фазові портрети як для модельних значень, так і для реальних. Використовуватимемо бібліотеки `neurokit2` для побудови атракторів та рекурентного аналізу.\n\n### Процедура реконструкції фазового простору\n\nДля побудови фазового портрету скористаємось методами `complexity_attractor()` та `complexity_embedding()` бібліотеки `neuralkit2`. Синтаксис `complexity_attractor()` виглядає наступним чином:\n\n**`complexity_attractor(embedded='lorenz', alpha='time', color='last_dim', shadows=True, linewidth=1, **kwargs)`**\n\n**Параметри:**\n\n- **embedded** (*Union[str, np.ndarray]*) --- результат функції `complexity_embedding()`. Також може бути рядком, наприклад, `\"lorenz\"` (атрактор Лоренца) або `\"rossler\"` (атрактор Рьосслера);\n- **alpha** (*Union[str, float]*) --- прозорість ліній;\n- **color** (*str*) --- колір графіку. Якщо `\"last_dim\"`, буде використано останній вимір (максимум 4-й) вбудованих даних, коли розмірність більша за 2. Корисно для візуалізації глибини (для 3-вимірного вбудовування), або четвертого виміру, але працюватиме це повільно;\n- **shadows** (*bool*) --- якщо значення `True`, 2D-проекції буде додано до бокових сторін 3D-атрактора;\n- **linewidth** (*float*) --- задає товщину лінії;\n- **kwargs** --- до палітри кольорів (наприклад, `name=\"plasma\"`) або до симулятора системи Лоренца передаються додаткові аргументи ключових слів, такі як `duration` (за замовчуванням = 100), `sampling_rate` (за замовчуванням = 10), `sigma` (за замовчуванням = 10), `beta` (за замовчуванням = 8/3), `rho` (за замовчуванням = 28).\n\nЯк вже зазначалося, побудова фазового простору, на основі якого і проводитиметься рекурентний аналіз, вимагає реконструкції. Виконати реконструкції фазового простору із одновимірного часового ряду можна із використанням методу часових затримок.\n\nМетод часових затримок є однією з ключових концепцій науки про складність. Він базується на ідеї, що динамічна система може бути описана вектором чисел, який називається її \"станом\", і має на меті забезпечити повний опис системи в даний момент часу. Множина всіх можливих станів називається \"простором станів\".\n\nТеорема Такенса [@10.1007/BFb0091924] припускає, що послідовність вимірювань динамічної системи містить у собі всю інформацію, необхідну для повної реконструкції простору станів. Метод часових затримок намагається визначити стан $s$ системи в певний момент часу $t$, шукаючи в минулій історії спостережень схожі стани, і, вивчаючи еволюцію схожих станів, виводити інформацію про майбутнє системи.\n\nЯк візуалізувати динаміку системи? Послідовність значень стану в часі називається траєкторією. Залежно від системи, різні траєкторії можуть еволюціонувати до спільної підмножини простору станів, яка називається атрактором. Наявність та поведінка атракторів дає інтуїтивне уявлення про досліджувану динамічну систему.\n\nОдже, згідно Такенсу, ідея полягає в тому, щоб на основі одиничних вимірювань системи, отримати $m$-розмірні реконструйовані часові вкладення\n\n$$\n\\vec{x}_i = \\left( x_i, x_{i+\\tau}, ... , x_{i+(m-1)\\tau} \\right),\n$$ {#eq-2-1}\n\nа $i$ проходить в діапазоні $1,..., N-(m-1)\\tau$; значення $\\tau$ представляє часову затримку, а $m$ --- це розмірність вкладень (кількість змінних, що включає кожна траєкторія).\n\nКод для реконструкції фазового простору може виглядати наступним чином:\n\nДля реконструкції фазового простору використовуватимемо метод `complexity_embedding()`. Його синтаксис:\n\n**`complexity_embedding(signal, delay=1, dimension=3, show=False, **kwargs)`**\n\n**Параметри:**\n\n- **signal** (*Union[list, np.array, pd.Series]*) --- сигнал (тобто часовий ряд) у вигляді вектора значень. Також може бути рядком, наприклад, `\"lorenz\"` (атрактор Лоренца), `\"rossler\"` (атрактор Росслера) або `\"clifford\"` (атрактор Кліффорда) для отримання попередньо визначеного атрактора;\n- **delay** (*int*) --- часова затримка (часто позначається $\\tau$ іноді називають запізненням). Ще розглянемо метод `complexity_delay()` для оцінки оптимального значення цього параметра;\n- **dimension** (*int*) --- розмірність вкладень ($m$, іноді позначається як $d$). Далі звернемось до методу `complexity_dimension()`, щоб оцінити оптимальне значення для цього параметра;\n- **show** (*bool*) --- побудувати графік реконструйованого атрактора;\n- **kwargs** --- інші аргументи, що передаються до `complexity_attractor()`.\n\n**Повертає:**\n\n- *array* --- реконструйований атрактор розміру `length - (dimension - 1) * delay`.\n\nДалі імпортуємо необхідні для подальшої роботи модулі:\n\nІ виконаємо налаштування рисунків для виводу:\n\nТепер розглянемо можливість використання методу часових затримок і отриманих у подальшому атракторів у якості індикатора складності. Як і в попередній роботі, для прикладу завантажимо часовий ряд Біткоїна за період з 1 вересня 2015 по 1 березня 2020, використовуючи `yfinance`:\n\n::: {.callout-warning}\n## Увага\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance, а із власного файлу. Зрозуміло, що й аналіз результатів, і висновки залежать від того з яким рядом ми працюємо\n\n:::\n\n---\n\n---\n\nВиводимо досліджуваний ряд:\n\nСпочатку оберемо вид ряду: 1 - вихідний ряд; 2 - детермінований (різниця між теперішнім та попереднім значенням); 3 - прибутковості звичайні; 4 - стандартизовані прибутковості; 5 - абсолютні значення (волатильності); 6 - стандартизований ряд.\n\nДля подальших розрахунків накращим варіантом буде вибір стандартизованого вихідного ряду або прибутковостей, оскільки значення вихідного часового ряду відрізняються на декілька порядків, і можуть сильно перевищувати встановлений параметр $\\varepsilon$. У цьому випадку для вихідних значень, що сильно різняться між собою, увесь часовий діапазон буде розглядатися як нерекурентний.\n\nСпочатку визначимо функції для виконання перетворення ряду:\n\nі тепер виконаємо перетворення, використовуючи дану функцію:\n\nОскільки ми не матимемо змоги візуалізувати багатовимірний фазовий простір ($m>3$), ми послуговуватимемось значеннями $m=2$ та $m=3$. Значення $\\tau$ будемо варіювати як із власних переконань, так і з опорою на функціонал бібліотеки `neuralkit2`.\n\nСкористаємось методом `complexity_simulate()` для генерації різних тестових сигналів:\n\n::: {.callout-important}\n## Автоматизований підбір параметрів\n\nУ зазначених вище прикладах ми обирали параметри $m$ і $\\tau$ згідно нашими власними міркуваннями. Але на практиці бажано було б, щоб зазначені параметри обирались автоматично, спираючись на конкретну статистичну процедуру. Бібліотека `neurokit2` представляє функціонал для автоматичного підбору параметрів розмірності та часової затримки. У завданнях самостійної роботи буде надано короткий опис алгоритмів для автоматичного підбору розмірності атрактору та часової затримки. В основній частині лабораторної роботи для побудови рекурентних діаграм будуть використанні самостійно підібрані значення $m$ і $\\tau$\n\n:::\n\n\n\n### Побудова рекурентної матриці\n\nЯк вже зазначалося, рекурентний аналіз на основі реконструйованих траєкторій фазового простору визначає кількість і тривалість рекурентних станів динамічної системи.\n\nМи маємо змогу побудувати рекурентну матрицю, використовуючи метод `recurrence_matrix()`.\n\nЙого синтаксис виглядає наступним чином:\n\n**`recurrence_matrix(signal, delay=1, dimension=3, tolerance='default', show=False)`**\n\n**Параметри:**\n\n- **signal** (*Union[list, np.ndarray, pd.Series]*) --- сигнал у вигляді вектора значень;\n- **delay** (*int*) --- затримка в часі;\n- **dimension** (*int*) --- розмірність вкладень, $m$;\n- **tolerance** (*float*) --- радіус $\\varepsilon$ багатовимірного околу, в межах якого шукаються рекурентні траєкторії, а дві точки даних вважаються схожими. Якщо `\"sd\"` (за замовчуванням), буде встановлено значення $0.2 \\cdot SD_{signal}$. Емпіричним правилом є встановлення $\\varepsilon$ таким чином, щоб відсоток точок, класифікованих як рекурентні, становив приблизно 2-5%;\n- **show** (*bool*) --- візуалізувати рекурентну матрицю.\n\n**Повертає:**\n\n- *np.ndarray* --- рекурентну матрицю;\n- *np.ndarray* --- матрицю відстаней.\n\nПобудуємо рекурентну матрицю для фрагменту вихідних значень Біткоїна, його прибутковостей та стандартизованого фоагменту його вихідного ряду. Розмірність $m=4$, часова затримка $\\tau=1$. Спробуємо різні варіанти $\\varepsilon$:\n\n- для вихідного ряду $\\varepsilon=100$;\n- для прибутковостей та стандартизованого ряду $\\varepsilon=0.8$.\n\n::: {.callout-note}\n## Примітка для побудови матриці рекурентності\n\nЯк уже зазначалось на початку, рекурентна діаграма --- це двовимірна квадратна двійкова матриця розміром $N^2$. Одним із її основних недоліків є те, що вона погано масштабується на великих даних. Наприклад, якщо ви плануєте досліджувати часовий ряд довжиною 1000000 значень, тоді рекурентна матриця буде складатись із 1000000000000 білих і чорних точок, що може бути викликом для вашого процесора та оперативної пам'яті. Тому в подальшому ми пропонуємо будувати рекурентну матрицю не для всього ряду, а тільки для його підмножини. Для цього ми визначимо змінні початкового (`idx_beg`) та кінцевого (`inx_end`) відліку в межах якого буде здійснюватись побудова рекурентної матриці\n\n:::\n\nЯк можна бачити з рисунку [-@fig-btc-rec-init] всі траєкторії для простору вихідних значень за абсолютною шкалою залишаються доволі віддаленими один від одного. Для розпізнавання рекурентних закономірностей нам потребується поступово нарощувати $\\varepsilon$. З цього рисунку видно, що $\\varepsilon=100$ буде замало.  \n\nТепер спробуємо трохи нормалізувати значення вихідного фрагменту Біткоїна, аби реконструйовані траєкторії не знаходились занадто віддаленими один від одного. Для цього розрахуємо стандартизовані прибутковості:\n\nТепер можемо бачити, що Біткоїн став характризуватися чорними смугами, що відображають динаміку певних детермінованих процесів. У той же час білі смуги характеризують періоди абсолютно аномальної (непередбачуваної) поведінки на даному ринку. Видно, що прибутковості залишаються доволі некорельованими, про що і свідчить переважне домінування саме білих областей.\n\nСпробуємо тепер подивитись на стандартизований вихідний ряд:\n\nНа початку свого існування Біткоїн характеризувався доволі високим ступенем передбачуваності, низькою волатильністю коливань. Надалі почали домінувати білі області, а зараз Біткоїну властива динаміка схожа з броунівським рухом.\n\n## Висновок\n\nУ даній лабораторній роботі було продемонстровано можливість дослідження складних нелінійних систем із використанням рекурентних діаграм. На прикладі Біткоїна було показано, що складним фінансовим систем властива топологія узагальненого броунівського руху, де рекурентність може варіюватись із плином часу. Тобто, той самий криптовалютний ринок характеризується мінливістю ступеня передбачуваності. Змога знаходження станів передбачуваності з точки зору рекурентних діаграм надає перспективи для їх використання разом із методами прогнозування та побудови індикаторів-передвісників.\n\n## Завдання для самостійної роботи\n\n1. Отримати індекс часового ряду у викладача\n2. Провести дослідження його рекурентних властивостей згідно інструкції\n3. Порівняти фазові портрети і рекурентні діаграми для стандартизованого вихідного ряду та прибутковостей. Що спільного між ними і чим вони відрізняються?\n4. Провести побудову фазових портретів і рекурентних діаграм для вашого часового ряду із використанням процедур автоматичного підбору параметрів розмірності вкладень і часової затримки. Порівняти результати з тими, що були отримані без використання даних методів і зробити висновки\n\n::: {.callout-important}\n## Для виконання 4-го завдання самостійної роботи\n\nДалі надається опис алгоритмів для автоматизованого підбору розмірності реконструйованого фазового простору та часової затримки\n\n:::\n\n\n\n### Автоматизований підбір параметра часової затримки, $\\tau$\n\nЧасова затримка $\\tau$ (також відома як *L*) є одним з двох критичних параметрів, що беруть участь у процедурі реконструкції фазового простору. Значення $L$ відповідає затримці у відліках між вихідним сигналом і його затриманою версією (версіями). Іншими словами, скільки відліків ми розглядаємо між певним станом сигналу та його найближчим минулим станом.\n\nЯкщо $\\tau$ менше оптимального теоретичного значення, послідовні координати стану системи корельовані і атрактор недостатньо розгорнутий. І навпаки, коли $\\tau$ більше, ніж повинно бути, послідовні координати майже незалежні, що призводить до некорельованої та неструктурованої хмари точок.\n\nВибір параметрів *затримки* та *розмірності* представляє нетривіальну задачу. Один з підходів полягає у їх (напів)незалежному виборі (оскільки вибір розмірності часто вимагає затримки) за допомогою функцій `complexity_delay()` та `complexity_dimension()`. Однак, існують методи спільного оцінювання, які намагаються знайти оптимальну затримку та розмірність одночасно.\n\nЗауважте також, що деякі автори (наприклад, Розенштейн, 1994) пропонують спочатку визначити оптимальну розмірність вбудовування, а потім розглядати оптимальне значення затримки як оптимальну затримку між першою та останньою координатами затримки (іншими словами, фактична затримка має дорівнювати оптимальній затримці, поділеній на оптимальну розмірність вбудовування мінус 1).\n\nДекілька авторів запропонували різні методи для вибору затримки:\n\n- **Фрейзер і Свінні (1986)** [@PhysRevA.33.1134] пропонують використовувати перший локальний мінімум взаємної інформації між затриманим і незатриманим часовими рядами, ефективно визначаючи значення $\\tau$, для якого вони діляться найменшою інформацією (і де атрактор є найменш надлишковим). На відміну від автокореляції, взаємна інформація враховує також нелінійні кореляції;\n- **Тейлер (1990)** [@PhysRevA.41.3038] запропонував вибирати таке значення $\\tau$, при якому автокореляція між сигналом та його зміщенною версією при $\\tau$ вперше перетинає значення $1/e$. Методи, що базуються на автокореляції, мають перевагу за часом обчислень, коли вони знаходяться за допомогою алгоритму швидкого перетворення Фур'є (fast Fourier transform, FFT);\n- **Касдаглі (1991)** [@CASDAGLI199152] пропонує замість цього брати перший нульовий перетин автокореляції;\n- **Розенштейн (1993, 1994)** [@ROSENSTEIN1993117;@ROSENSTEIN199482] вважає, що слід апроксимувати точку, де функція автокореляцій падає до $\\left( 1-1/e \\right)$ від свого максимального значення. Або ж наближатися до точки, близької до 40% нахилу середнього зміщення від діагоналі;\n- **Кім (1999)** [@KIM199948] оцінює $\\tau$ за допомогою кореляційного інтегралу, який називається C-C методом, і який, як виявилося, узгоджується з результатами, отриманими за допомогою методу взаємної інформації. Цей метод використовує статистику в реконструйованому фазовому просторі, а не аналізує часову еволюцію ряду. Однак час обчислень є значно довшим через необхідність порівнювати кожну унікальну пару парних векторів у реконструйованому сигналі на кожну затримку;\n- **Лайл (2021)** [@10.3389/fcvm.2021.709457] описує \"Реконструкцію симетричного проекційного атрактора\" (Symmetric Projection Attractor Reconstruction, SPAR), де $1/3$ від домінуючої частоти (тобто довжини середнього \"циклу\") може бути підходящим значенням для приблизно періодичних даних, і робить атрактор чутливим до морфологічних змін. Див. також [доповідь Астона](https://youtu.be/GGrOJtcTcHA?t=730). Цей метод також є найшвидшим, але може не підходити для аперіодичних сигналів. Аргумент `algorithm` (за замовчуванням `\"fft\"`) передається до аргументу `method` методу [`signal_psd()`](https://neuropsychology.github.io/NeuroKit/functions/signal.html#signal-psd).\n\nМожна також відмітити метод для об'єднаного підбору параметрів затримки та розмірності.\n\n- **Гаутама (2003)** [@lirias1573905] зазначає, що на практиці часто використовують фіксовану часову затримку і відповідно регулюють розмірність вбудовування. Оскільки це може призвести до великих значень $m$ (а отже, до вкладених даних великого розміру) і, відповідно, до повільної обробки, використовується метод оптимізації для спільного визначення $m$ і $\\tau$ на основі показника **entropy ratio**.\n\nРозглянемо оптимальні значення розмірності та затримки для часового сигналу Біткоїна:\n\n@fig-btc-delay-fraser1986 показує, що перший локальний мінімум взаємної інформації для стандартизованих вихідних значень Біткоїна знаходиться на 273 лагу. Для візуального огляду реконструйованого атрактора це значення, можливо, є найбільш адекватним. Але використовуючи настільки велику часову затримку, ми втрачаємо доволі багато проміжних значень, що також можуть містити досить важливу приховану інформацію для кількісних розрахунків.\n\n@fig-btc-delay-theiler демонструє, що автокореляція між стандартизованих вихідним сигналом Біткоїна та його зміщенною версією при $\\tau=195$ вперше перетинає значення $1/\\exp$. Бачимо, що дане значення затримки є трохи меншим за те, що було отримано до цього, але суті це не змінює. Також бачимо, що між реконструйованими атракторами для $\\tau=195$ та $\\tau=273$ немає кардинальної візуальної різниці.\n\nЯк можна бачити по прикладу вище, не всі методи надають адекватну оцінку розмірності нашого сигналу. Спробуємо привести вихідні значення Біткоїна до прибутковостей та повторити процедуру Касдаглі ще раз.  \n\nЦього разу нам вдалося досягти оптимального результату, але приклад вище демонструє, що кожна процедура має свої виключення. @fig-btc-delay-casdagli1991 показує, що значення прибутковостей Біткоїна характеризуються певними кореляціями лише на перших 4-ох лагах. Подальші часові зміщення роблять значення прибутковостей незалежними один від одного.\n\n@fig-btc-delay-rosenstein1993 демонструє, що при $\\tau=101$ функція автокореляцій перетинає значення $\\left( 1-1/e \\right)$. При цьому видно, що навіть для такого лагу зберігається значна частка кореляцій між стандартизованими вихідними значеннями Біткоїна.\n\nРисунок вище показує, що при $\\tau=120$ зміщення реконструйованих траєкторій від їх оригінального положення на лінії ідентичності зберігає найбільшу кількість інформації стосовно атрактора стандартизованих значень Біткоїна.\n\nЗгідно представленого вище результату найбільш значущі частоти, отримані за допомогою перетворення Фур'є, зберігаються при $\\tau=109$.\n\nТепер подивимось як це виглядатиме для об'єднаного підбору параметрів:\n\nОскільки представлена вище процедура є доволі громіздкою в плані обчислювальних потужностей, ми обрали діапазон $\\tau$ в межах від 1 до 10. Видно, що при $\\tau$ близької до 3 оптимальне значення розмірності атрактора дорівнює 10. Можливо, при значеннях $\\tau$ близьких до 100 або 200, ми могли б отримати зовсім інше значення розмірності, але це потребує додаткових експериментів.\n\n### Автоматизований підбір параметра розмірності вкладень, $m$\n\nЗа дану процедуру відповідає метод `complexity dimension()`. Її синтаксис виглядає наступним чином:\n\n**`complexity_dimension(signal, delay=1, dimension_max=20, method='afnn', show=False, **kwargs)`**\n\nХоча зазвичай використовують $m=2$ або $m=3$, але різні автори пропонують наступні процедури підбору:\n\n- **кореляційна розмірність (Correlation Dimension, CD)**: Одним з перших методів оцінки оптимального $m$ був розрахунок кореляційної розмірності для вкладень різного розміру і пошук насичення (тобто плато) в її значенні при збільшенні розміру векторів [@GRASSBERGER1983189;@PhysRevLett.50.346;@GRASSBERGER1983227]. Одне з обмежень полягає в тому, що насичення буде також мати місце, коли даних недостатньо для адекватного заповнення простору високої розмірності (зауважте, що в загальному випадку не рекомендується мати настільки великі вкладення, оскільки це значно скорочує довжину сигналу);\n- **найближчі хибні сусіди (False Nearest Neighbour, FNN)**: Метод, запропонований Кеннелом та ін. [@PhysRevA.45.3403;@krakovska2015use;@RHODES1997S1149], базується на припущенні, що дві точки, які є близькими одна до одної в достатній розмірності вбудовування, повинні залишатися близькими при збільшенні розмірності. Алгоритм перевіряє сусідів при збільшенні розмірності вкладень, поки не знайде лише незначну кількість хибних сусідів при переході від розмірності $m$ до $m+1$. Це відповідає найнижчій розмірності вкладення, яка, як передбачається, дає розгорнуту реконструкцію просторово-часового стану. Цей метод може не спрацювати в зашумлених сигналах через марну спробу розгорнути шум (а в чисто випадкових сигналах кількість хибних сусідів суттєво не зменшується зі збільшенням $m$). На рисунку нижче (@fig-fnn) показано, як проекції на простори більшої розмірності можна використовувати для виявлення хибних найближчих сусідів. Наприклад, червона та жовта точки є сусідами в одновимірному просторі, але не в двовимірному;\n\n::: {#fig-fnn}\n\n![](Images\\lab_2\\fnn.jpg){width=10cm, height=10cm}\n\nОсновна ідея методу FNN. Найближчі сусіди зеленої точки з'являються у випадку 1-, 2- та 3-вимірних фазових просторів [@STAVRINIDES2022112224]\n\n:::\n\n- **середні хибні сусіди (Average False Neighbors, AFN)**: Ця модифікація методу FNN розроблена Сао (1997) [@CAO199743] і усуває один з його основних недоліків --- необхідність евристичного вибору порогових значень $r$. Метод використовує максимальну евклідову відстань для представлення найближчих сусідів і усереднює всі відношення відстані в $m+1$ розмірності до розмірності $m$ та визначає *E1* та *E2* як параметри. Оптимальна розмірність досягається тоді, коли *E1* перестає змінюватися (виходить на плато). Це відбувається при розмірності *d0*, якщо сигнал надходить від атрактора. Тоді *d0*+1* є оптимальною мінімальною розмірністю вкладення. *E2* є корисною величиною для того, щоб відрізнити детерміновані сигнали від стохастичних. Константа *E2*, що близька до 1 для будь-якої розмірності вкладень $d$, вказує на випадковість даних, оскільки майбутні значення не залежать від минулих.\n\n**Параметри:**\n\n- **signal** (*Union[list, np.array, pd.Series]*) --- сигнал (тобто часовий ряд) у вигляді вектора значень;\n- **delay** (*int*) --- часова затримка у відліках. Для вибору оптимального значення цього параметра ми ще скористаємось методом `complexity_delay()`;\n- **dimension_max** (*int*) --- максимальний розмір вкладення для тестування;\n- **method** (*str*) --- може бути `\"afn\"` (середні хибні сусіди), `\"fnn\"` (найближчий хибний сусід) або `\"cd\"` (кореляційна розмірність);\n- **show** (*bool*) --- візуалізувати результат;\n- **kwargs** --- інші аргументи, такі як $R=10.0$ або $A=2.0$ (відносне та абсолютне граничне значення, тільки для методу `\"fnn\"`).\n\n**Повертає:**\n\n- **dimension** (*int*) --- оптимальна розмірність вкладень;\n- **parameters** (*dict*) --- словник python, що містить додаткову інформацію про параметри, які використовуються для обчислення оптимальної розмірності.\n\nСпробуємо отримати оптимальне значення розмірності згідно зазначених процедур. В якості часової затримки можна взяти $\\tau=100$. Приблизно таке значення спостерігалося для кожної процедури.\n\n@fig-btc-dim-cd демонструє той факт, що оптимальна розмірність вкладень, за якої досягається найбільш інформативна репрезентація фазового простору, дорівнює 7.  \n\nЗ рисунку [-@fig-btc-dim-fnn] видно, що мінімальна розмірність вкладення дорівнює 3. Саме при переході від 3-ох вимірного фазового простору до 4-ох вимірного ми бачимо, що кількість хибних сусідів стає мінімальною і далі не зростає.\n\nАлгоритм середніх хибних сусідів показує, що тут розмірність вкладень $m=5$ є оптимальною. При подальшому зростанні розмірності, атрактор стає більш стохастичним, що вказує на втрату всіх кореляцій.  \n\nЗгідно з представленими вище алгоритмами автоматичного підбору, розмірність вкладень можна обирати в діапазоні значень від 3 до 7. Тепер на основі отриманих результатів приступимо до побудови рекурентної діаграми.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"number-sections":true,"highlight-style":"arrow","css":["style.css"],"output-file":"lab_2.html"},"language":{"toc-title-document":"Зміст","toc-title-website":"На цій сторінці","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Анотація","section-title-appendices":"Додатки","section-title-footnotes":"Зноски","section-title-references":"Використана література","section-title-reuse":"Повторне використання","section-title-copyright":"Copyright","section-title-citation":"Цитата","appendix-attribution-cite-as":"Будь-ласка, цитуйте цю роботу як:","appendix-attribution-bibtex":"BibTeX:","title-block-author-single":"Автор","title-block-author-plural":"Автори","title-block-affiliation-single":"Приналежність","title-block-affiliation-plural":"Приналежності","title-block-published":"Дата публікації","title-block-modified":"Змінено","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Код","code-tools-menu-caption":"Код","code-tools-show-all-code":"Розгорнути код","code-tools-hide-all-code":"Приховати код","code-tools-view-source":"Переглянути код","code-tools-source-code":"Вихідний код","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Копіювати","copy-button-tooltip-success":"Скопійовано!","repo-action-links-edit":"Редагувати сторінку","repo-action-links-source":"Переглянути код","repo-action-links-issue":"Повідомити про проблему","back-to-top":"Back to top","search-no-results-text":"Пошук не дав результату","search-matching-documents-text":"Результати пошуку","search-copy-link-title":"Скопіюйте посилання для пошуку","search-hide-matches-text":"Приховати додаткові результати","search-more-match-text":"Додатковий результат у цьому документі","search-more-matches-text":"Додаткові результати у цьому документі","search-clear-button-title":"Очистити","search-text-placeholder":"","search-detached-cancel-button-title":"Скасувати","search-submit-button-title":"Надіслати","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Рис.","crossref-tbl-title":"Таблиця","crossref-lst-title":"Список","crossref-thm-title":"Теорема","crossref-lem-title":"Лема","crossref-cor-title":"Наслідок","crossref-prp-title":"Твердження","crossref-cnj-title":"Гіпотеза","crossref-def-title":"Визначення","crossref-exm-title":"Приклад","crossref-exr-title":"Завдання","crossref-ch-prefix":"Глава","crossref-apx-prefix":"Додаток","crossref-sec-prefix":"Розділ","crossref-eq-prefix":"Рівняння","crossref-lof-title":"Список Рисунків","crossref-lot-title":"Список Таблиць","crossref-lol-title":"Список Каталогів","environment-proof-title":"Доведення","environment-remark-title":"Зауваження","environment-solution-title":"Рішення","listing-page-order-by":"Сортувати по","listing-page-order-by-default":"попередньо вибраний","listing-page-order-by-date-asc":"Найновіші","listing-page-order-by-date-desc":"Найстріші","listing-page-order-by-number-desc":"За спаданням","listing-page-order-by-number-asc":"За зростанням","listing-page-field-date":"Дата","listing-page-field-title":"Заголовок","listing-page-field-description":"Опис","listing-page-field-author":"Автор","listing-page-field-filename":"Ім'я файлу","listing-page-field-filemodified":"Змінено","listing-page-field-subtitle":"Підзаголовок","listing-page-field-readingtime":"Час читання","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Категорії","listing-page-minutes-compact":"{0} хвилин","listing-page-category-all":"Все","listing-page-no-matches":"Немає відповідних елементів","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.4.551","bibliography":["references_main.bib"],"csl":"physical-review-b.csl","callout-appearance":"default","grid":{"body-width":"1050px"},"page-layout":"full","theme":{"light":"cosmo","dark":"superhero"},"title":"Лабораторна робота № 2"},"extensions":{"book":{"multiFile":true}}},"docx":{"identifier":{"display-name":"MS Word","target-format":"docx","base-format":"docx"},"execute":{"fig-width":5,"fig-height":4,"fig-format":"png","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":false,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"docx","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":true,"merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"page-width":6.5},"pandoc":{"default-image-extension":"png","to":"docx","toc":true,"number-sections":true,"output-file":"lab_2.docx"},"language":{"toc-title-document":"Зміст","toc-title-website":"На цій сторінці","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Анотація","section-title-appendices":"Додатки","section-title-footnotes":"Зноски","section-title-references":"Використана література","section-title-reuse":"Повторне використання","section-title-copyright":"Copyright","section-title-citation":"Цитата","appendix-attribution-cite-as":"Будь-ласка, цитуйте цю роботу як:","appendix-attribution-bibtex":"BibTeX:","title-block-author-single":"Автор","title-block-author-plural":"Автори","title-block-affiliation-single":"Приналежність","title-block-affiliation-plural":"Приналежності","title-block-published":"Дата публікації","title-block-modified":"Змінено","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Код","code-tools-menu-caption":"Код","code-tools-show-all-code":"Розгорнути код","code-tools-hide-all-code":"Приховати код","code-tools-view-source":"Переглянути код","code-tools-source-code":"Вихідний код","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Копіювати","copy-button-tooltip-success":"Скопійовано!","repo-action-links-edit":"Редагувати сторінку","repo-action-links-source":"Переглянути код","repo-action-links-issue":"Повідомити про проблему","back-to-top":"Back to top","search-no-results-text":"Пошук не дав результату","search-matching-documents-text":"Результати пошуку","search-copy-link-title":"Скопіюйте посилання для пошуку","search-hide-matches-text":"Приховати додаткові результати","search-more-match-text":"Додатковий результат у цьому документі","search-more-matches-text":"Додаткові результати у цьому документі","search-clear-button-title":"Очистити","search-text-placeholder":"","search-detached-cancel-button-title":"Скасувати","search-submit-button-title":"Надіслати","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Рис.","crossref-tbl-title":"Таблиця","crossref-lst-title":"Список","crossref-thm-title":"Теорема","crossref-lem-title":"Лема","crossref-cor-title":"Наслідок","crossref-prp-title":"Твердження","crossref-cnj-title":"Гіпотеза","crossref-def-title":"Визначення","crossref-exm-title":"Приклад","crossref-exr-title":"Завдання","crossref-ch-prefix":"Глава","crossref-apx-prefix":"Додаток","crossref-sec-prefix":"Розділ","crossref-eq-prefix":"Рівняння","crossref-lof-title":"Список Рисунків","crossref-lot-title":"Список Таблиць","crossref-lol-title":"Список Каталогів","environment-proof-title":"Доведення","environment-remark-title":"Зауваження","environment-solution-title":"Рішення","listing-page-order-by":"Сортувати по","listing-page-order-by-default":"попередньо вибраний","listing-page-order-by-date-asc":"Найновіші","listing-page-order-by-date-desc":"Найстріші","listing-page-order-by-number-desc":"За спаданням","listing-page-order-by-number-asc":"За зростанням","listing-page-field-date":"Дата","listing-page-field-title":"Заголовок","listing-page-field-description":"Опис","listing-page-field-author":"Автор","listing-page-field-filename":"Ім'я файлу","listing-page-field-filemodified":"Змінено","listing-page-field-subtitle":"Підзаголовок","listing-page-field-readingtime":"Час читання","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Категорії","listing-page-minutes-compact":"{0} хвилин","listing-page-category-all":"Все","listing-page-no-matches":"Немає відповідних елементів","listing-page-words":"{0} words"},"metadata":{"bibliography":["references_main.bib"],"csl":"physical-review-b.csl","callout-appearance":"default","fontsize":14,"title":"Лабораторна робота № 2"},"extensions":{"book":{"selfContainedOutput":true}}}},"projectFormats":["html","docx"]}