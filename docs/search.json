[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Моделювання складних систем у Python",
    "section": "",
    "text": "Передмова\nНе дивлячись на очікувані прогнози стати 21-му століттю століттям біології (за аналогією з 20-м століттям фізики), з легкої руки геніального британського астрофізика Стівена Гокінга (Stephen William Hawking, 1942-2018) його безумовно можна вважати століттям складності. Дійсно, виявилось, що складні системи різної природи проявляють схожі паттерни складності і можуть бути схарактеризовані універсальними міждисциплінарними кількісними методами і алгоритмами. Свідченням тому є Нобелівська премія з фізики 2021 р. “за новаторський внесок у наше розуміння складних фізичних систем” була присуджена італійському фізику Джорджіо Паризі “за відкриття взаємозв’язку безладу та флуктуацій у фізичних системах від атомного до планетарного масштабів” і японсько-американському кліматологу Шюкуро Манабе та німецькому вченому Клаусу Гассельманну “за фізичне моделювання клімату Землі, кількісну оцінку мінливості та надійне прогнозування глобального потепління”.\nНезважаючи на те, що визначення складних систем історично широко обговорювалося, навряд чи зараз існує широка згода щодо єдиного конкретного визначення: складна система утворена багатьма взаємодіючими елементами, які породжують явища, що виникають [1]. Однак при цьому залишаються невизначеними деякі терміни. Наприклад: скільки елементів достатньо для відображення складних емерджентних явищ? Цікаво, що ця кількість може бути великою, як кількість нейронів і синапсів у мозку людини, але також може бути відносно невеликою: мінімальна клітина також утворена лише кількома сотнями генів і вже жива. З цих прикладів стає зрозуміло, що справжня складність і нові явища вимагають ряду елементів, які можуть бути дуже далекими від великих чисел, які зазвичай розглядаються у фізиці (кількість молекул в молі газу визначається числом Авогадро, тобто приблизно \\(6 \\times 10^{23}\\)). Здається дуже загальноприйнятою ідея, що складність виникає внаслідок конкуренції між випадковістю та порядком і що топологія складної системи за своєю суттю пов’язана з певною часткою випадковості в мережі взаємодій між її елементами (так званий, мережний підхід) або за знаком їх взаємодії (підхід спінового скла).\nТочкою відліку, яку часто цитують у контексті колективних ефектів складних систем, є стаття “More is different”, написана Філіппом Андерсоном (1923-2020) [2], лауреатом Нобелівської премії з фізики 1977 р. Наука про складні системи розглядає способи, якими складові частини породжують колективну поведінку цілої системи. Однак виявилось, що така інтерпретація має обмежену корисність, оскільки охоплює занадто широкий набір обставин.\nІсторично деякий час тому у фізиці з’явився інший більш корисний крок у визначенні складних систем: система є складною, якщо її поведінка суттєво залежить від її деталей [3]. У цьому контексті маються на увазі такі явища, як детермінований хаос, квантова заплутаність, згортання білків, спінові стекла тощо. Колективна складна поведінка може виникати під впливом фрустрації та структурного безладу. Як наслідок, важко досягти стану рівноваги, реакції на зовнішні збурення повільні і дуже часто випадкові. Такі різні явища вивчаються в різних областях фізики: динамічні системи, квантова механіка та статистична фізика. Їх спільною рисою є те, що нескінченно малі зміни початкових умов (хоча й дуже різної природи) призводять до кардинально різних сценаріїв часової еволюції цих систем.\nЄ другий компонент, важливий для визначення складних систем. З одного боку, взаємодії між складовими частинами призводять до колективної поведінки та визначають макростан, але з іншого боку, взаємодії змінюються в ході еволюції системи та піддаються впливу макростану. Іншими словами, макростан і мікростани динамічно оновлюють один одного. Аналіз таких ефектів призвів до створення методів і розвитку концепцій, які були успішно застосовані для опису формально подібних явищ, що відбуваються в хімічних, біологічних, соціальних та інших системах, утворених агентами нефізичного походження.\nНезважаючи на вражаючий прогрес науки про складність, якого було досягнуто за останні 50 років, ми все ще далекі від повного розуміння складності, оскільки ще не визначили необхідні умови для того, щоб система відображала складні емерджентні явища. Наприклад, ми досить далекі від повного розуміння роботи мозку. У результаті цього галузь може виглядати фрагментованою в порівнянні з іншими більш традиційними науковими галузями, наприклад, фізикою. Для вирішення проблеми складності нам дійсно потрібно досліджувати різні аспекти складних систем, і ми повинні прийняти відкриту точку зору, яка здатна описувати та прогнозувати дані складних систем, уникаючи попередньо визначених догм зверху вниз.\nНаука про складність також є ключовою для розуміння та прогнозування еволюції великих пандемій та для інформування політиків та широкої громадськості про ризики поширення епідемії. Дійсно, мережна наукова спільнота вже задовго до Covid усвідомлювала небезпеку глобальних пандемій, які використовують переваги безмасштабних глобальних транспортних систем. На жаль, пандемія застала більшість країн зненацька, оскільки плани на випадок надзвичайних ситуацій насправді не були готові до епідемії такого масштабу, як Covid-19. Для моніторингу розвитку цієї пандемії та будь-якої майбутньої пандемії вчені, ймовірно, поєднають велику кількість даних про соціальну мобільність із моделями прогнозування, які є ключовими для моніторингу пандемії та інформування політиків, незважаючи на багато невизначеностей щодо біологічної еволюції вірусів.\nУ майбутньому прогрес на стику між складністю та біологією стане ключовим для досягнення настільки необхідних успіхів у прецизійній медицині. Ця велика складна проблема вимагатиме справді міждисциплінарного підходу, який поєднує мережну науку, машинне навчання та штучний інтелект з молекулярною біологією та неврологією. Дійсно, в той час як біологія в останні десятиліття широко застосовувала одномолекулярний підхід або значною мірою покладалася на центральну догму молекулярної біології, тепер добре визнано, що більшість захворювань є складними, і для розуміння цих захворювань важливо прийняти складність і неоднорідність взаємодіючих мереж клітини. Нарешті, найближчим часом складність стане ключовою для створення фундаменту для квантового Інтернету, який вимагатиме поєднання прогресу квантової інформації з нашим розумінням класичних складних комунікаційних систем, таких як поточний Інтернет.\nІншим важливим викликом щодо надійності мережі є дослідження мозку, оскільки мозок, безсумнівно, є надійною складною системою, однак дуже важливо розуміти, як на його функцію впливають захворювання. Щоб відповісти на це питання, я вважаю, що нам потрібно прийняти стохастичну природу мозкової активності та отримати подальше розуміння взаємодії між функціями мозку та топологією мозкової мережі. Завдяки фундаментальним досягненням науки про мережі ми вже знаємо, що стійкість мереж до випадкових пошкоджень сильно залежить від статистичних властивостей мережі. Дійсно, безмасштабний розподіл ступенів мереж різко змінює фазову діаграму перколяції, демонструючи критичну поведінку, яка різко відрізняється від перколяції на регулярних решітках або на випадкових графах. Ці результати були ключовими для розуміння взаємодії між основною мережною структурою складних систем та їх динамікою.\nПрогнозування складних систем є складним завданням і, звичайно, обмежене повсюдною нелінійністю їх динаміки. Проте за останні двадцять років було досягнуто важливого прогресу в прогнозуванні складних систем (наприклад, безпрецедентний прогрес у прогнозуванні поширення епідемії). Покращення потужності прогнозування складних систем здебільшого пов’язано з великою кількістю даних, доступних для розробників моделей, і важливим прогресом, якого може досягти наука про складність у поєднанні з наукою про дані та штучним інтелектом (ШІ). Підтвердженням цьому є революційні зрушення у матеріалознавстві [4] чи фінансах [5].\nПідвищення наших можливостей прогнозування складних систем, які врешті-решт поєднають мережну науку, науку про дані та алгоритми штучного інтелекту, є справді ключовим для різноманітних застосувань, зокрема для забезпечення можливих сценаріїв зміни клімату. Однак потужність простих моделей для розуміння складних систем має надзвичайно важливе значення для інтерпретації результатів: прості моделі можуть не охопити всі деталі складних систем, але дозволяють зрозуміти та приборкати складність, що може мати вирішальне значення при розробці кращих алгоритмів ШІ. Теорія спінового скла вчить нас, що модель, яка насправді є досить простою (лише додавання випадкової суміші позитивних і негативних взаємодій до моделі Ізінга в повністю зв’язній мережі), може бути вже дуже складною.\nМетодам моделювання складних систем присвячені наші попередні монографії та навчально-методичні посібники, орієнтовані на систему комп’ютерної математики Матлаб [6-9]. З урахуванням домінування Phyton у прикладних дослідженнях складних систем поява даного посібника на думку авторів є виправданою.\nУ навчальному посібнику висвітлюються основні теоретичні та інструментальні аспекти моделювання складних систем різної природи з використанням інтерактивного веб-додатку Jupyter Notebook. Для дискретних даних у вигляді часових рядів розроблено і адаптовано сучасні міждисциплінарні методи кількісної оцінки складності: (мульти-)фрактальний, рекурентний, інформаційний, ентропійний та ін. види аналізу. Запропоновано відповідні міри складності. Показано, що більшість із них можна використовувати у якості індикаторів чи передвісників критичних або кризових явищ у складних системах.\nПосібником зможуть скористатися як здобувачі другого (магістр) рівня вищої освіти спеціальності 014 Середня освіта (Фізика та астрономія; Інформатика; Математика; Природничі науки), так і інших спеціальностей закладів вищої освіти, наукові працівники й особи, які цікавляться методами моделювання складних систем.\nДана онлайн-книга буде часто оновлюватись і редагуватись. Її вміст є вільним для використання. Метою авторів даної книги було забезпечення:\nЧитачам рекомендується посилатися як на цю книгу, так і на дотичні до неї джерела, коли вони використовують викладені тут методології та ідеї. При цитуванні слід дотримуватися точності деталей, особливо URL-адреси, щоб спрямувати читачів до правильної Інтернет сторінки.\nНижче наведено один із варіантів цитування:",
    "crumbs": [
      "Передмова"
    ]
  },
  {
    "objectID": "index.html#посилання",
    "href": "index.html#посилання",
    "title": "Моделювання складних систем у Python",
    "section": "Посилання",
    "text": "Посилання\n\n\nBianconi G. et al Complex systems in the spotlight: next steps after the 2021 Nobel Prize in Physics (2023) J. Phys. Complex. 4 010201 https://iopscience.iop.org/article/10.1088/2632-072X/ac7f75\nAnderson P.W. More is different. New Series, Vol. 177, No. 4047. (Aug. 4, 1972), pp. 393-396. https://cse-robotics.engr.tamu.edu/dshell/cs689/papers/anderson72more_is_different.pdf\nHolovatch Y., Kenna R., Thurner S. Complex systems: physics beyond physics (2017) Eur. J. Phys. 38 023002. https://doi.org/10.1088/1361-6404/aa5a87\nMerchant A. et al. Scaling deep learning for materials discovery (2023) Nature. Vol 624 (7 December 2023), pp.80-90. https://doi.org/10.1038/s41586-023-06735-9\nChu, Z., Guo, H., Zhou, X., Wang, Y., Yu, F., Chen, H., Xu, W., Lu, X., Cui, Q., Li, L. and Zhou, J., 2023. Data-centric financial large language models. arXiv preprint arXiv:2310.17784. https://doi.org/10.48550/arXiv.2310.17784\nСоловйов В. М. Математична економіка: навч.-метод. посіб. для самостійного вивч. / В. М. Соловйов. – Черкаси: Видавництво ЧНУ ім. Б. Хмельницького, 2008. – 136 с. http://elibrary.kdpu.edu.ua/handle/0564/1049 https://doi.org/10.31812/0564/1049\nДербенцев В. Д. Синергетичні та еконофізичні методи дослідження динамічних та структурних характеристик економічних систем: монографія / В. Д. Дербенцев, О. А. Сердюк, В. М. Соловйов, О. Д. Шарапов. – Черкаси: Брама-Україна, 2010. – 300 с. http://elibrary.kdpu.edu.ua/handle/0564/1045 https://doi.org/10.31812/0564/1045\nГанчук А. А. Методи прогнозування: навчальний посібник / А. А. Ганчук, В. М. Соловйов, Д. М. Чабаненко. – Черкаси: Брама-Україна, 2012. – 140 с. http://elibrary.kdpu.edu.ua/handle/0564/1186 https://doi.org/10.31812/0564/1186\nСоловйов В. М. Моделювання складних систем : навчально-методичний посібник для самостійного вивчення дисципліни / В. М. Соловйов, О. А. Сердюк, Г. Б. Данильчук. – Черкаси: Видавець О. Ю. Вовчок, 2016. – 204 с. http://elibrary.kdpu.edu.ua/handle/0564/1065 https://doi.org/10.31812/0564/1065",
    "crumbs": [
      "Передмова"
    ]
  },
  {
    "objectID": "lab_1.html",
    "href": "lab_1.html",
    "title": "1  Лабораторна робота № 1",
    "section": "",
    "text": "1.1 Теоретичні відомості",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Лабораторна робота № 1</span>"
    ]
  },
  {
    "objectID": "lab_1.html#теоретичні-відомості",
    "href": "lab_1.html#теоретичні-відомості",
    "title": "1  Лабораторна робота № 1",
    "section": "",
    "text": "1.1.1 Аналіз динаміки прибутків, модулів прибутків та волатильностей\nОстаннім часом вчені все більше цікавляться економічними часовими рядами, і відбувається це за кількох причин, зокрема: (1) економічні часові ряди, такі як індекси акцій, курсів валют, залежать від розвитку великої кількості взаємодіючих систем, і є прикладами складних систем, що широко вивчаються у науці; (2) з’явилась велика кількість доступних баз з даними про економічні системи, що містять інформацію з різними часовими шкалами (починаючи з 1 хвилини і закінчуючи 1 роком). Внаслідок цього вже на даний час існує також велика кількість розроблених методів (зокрема, у статистичній фізиці), спрямованих на отримання характеристик цін акцій чи курсів валют, що еволюціонують у часі.\nДослідження, проведені над часовими рядами, показують, що стохастичний процес, який лежить у основі зміни ціни, характеризується кількома ознаками. Розподіл зміни ціни має виділений хвіст порівняно із Гаусовим розподілом. Функція автокореляції зміни ціни спадає експоненційно з певним характерним часом. Однак, виявляється, що амплітуда зміни ціни, виміряна за абсолютними значеннями чи квадратами цін, показує степеневі кореляції з довго часовою персистентністю аж до кількох місяців, або навіть років. Такі довгочасові залежності краще моделюються з використанням “додаткового процесу”, що в економічній літературі часто називається волатильністю. Волатильність змін ціни акції є мірою того, як сильно ринок схильний до флуктуацій, тобто відхилень ціни від попередніх значень.\nПершим кроком при проведенні аналізу є побудова оцінювача волатильності. Ми будемо отримувати волатильність як локальне середнє модуля зміни ціни.\nРозуміння статистичних властивостей волатильності має також важливе практичне застосування. Волатильність є інтересом торговців, оскільки визначає ризик і є ключовим входом практично до всіх моделей цін опціонів (вторинного цінного паперу), включаючи і класичну модель Блека-Шоулза. Без задовільних методів оцінювання волатильності трейдерам було б надзвичайно важко визначати ситуації, в яких опціони попадають в недооцінку чи переоцінку.\n\n\n1.1.2 Визначення волатильності\nТермін волатильність представляє узагальнену міру величини ринкових флуктуацій (відхилень). У літературі існує досить багато визначень волатильності, проте ми будемо використовувати наступне: волатильність є локальним середнім модуля зміни ціни на відповідному часовому інтервалі \\(T\\), що є рухомим параметром нашої оцінки. Для індексу \\(X(t)\\) визначимо зміну ціни \\(G(t)\\) як зміну логарифмів індексів,\n\\[\nG(t) = \\ln{X(t+\\Delta t) - \\ln{X(t)}} \\cong \\left[ X(t+\\Delta t) - X(t) \\right] \\big/ X(t),\n\\tag{1.1}\\]\nде \\(\\Delta t\\) є часовим інтервалом затримки. Величину (1.1) називають прибутковістю (return). Якщо використовувати границі, то малі зміни \\(X(t)\\) приблизно відповідають змінам, визначеним другою рівністю. Ми лише підраховуємо час роботи ринку, викидаємо ночі, вихідні та свята із набору даних, тобто, вважаємо, що ринок працює без перерв.\nМодуль \\(G(t)\\) описує амплітуду флуктуацій. У порівнянні зі значеннями \\(G(t)\\) їх модуль не показує глобальних трендів, але великі значення \\(G(t)\\) відповідають крахам та великим миттєвим змінам на ринках.\nВизначимо волатильність як середнє від \\(G(t)\\) для часових вікон \\(T = n \\cdot \\Delta t\\), тобто\n\\[\nV_{T} = \\frac{1}{n}\\sum_{t^{'}=t}^{t+n-1}\\left| G(t^{'}) \\right|,\n\\tag{1.2}\\]\nде \\(n\\) є цілим числом. Таке визначення може бути ще узагальнене заміною \\(G(t)\\) на \\(\\left| G(t) \\right|^{\\gamma}\\), де \\(\\gamma &gt; 1\\) дає більш виражені великі значення \\(G(t)\\), в той час як \\(0 &lt; \\gamma &lt; 1\\) виділяє малі значення \\(G(t)\\).\nУ цьому визначенні волатильності використовується два параметри: \\(\\Delta t\\) та \\(n\\). Параметр \\(n\\) є шаблонним (чи модельним) часовим інтервалом для даних, а параметр \\(\\Delta t\\) є кроком переміщення часового вікна. Зауважимо, що вказане визначення волатильності має внутрішню помилку, а саме: вибір більшого часового інтервалу \\(T\\) веде до збільшення точності визначення волатильності. Однак, велике значення \\(T\\) також включає погане розбиття часу на інтервали, що веде, у свою чергу, до врахування не всієї прихованої у ряді інформації.\n\n\n1.1.3 Визначення кореляцій\nДля визначення кореляцій часового ряду використовується функція автокореляції. Саме поняття автокореляції означає кореляцію часового ряду самого з собою (між попередніми та наступними значеннями). Автокореляцію іноді називають послідовною кореляцією, що означає кореляцію між членами ряду чисел, розташованих у певному порядку. Також синонімами цього терміну є лагова кореляція та персистентність. Наприклад, часто зустрічається автокореляція геофізичних процесів, що означає перенесення залишкового процесу на наступні часові проміжки.\nПозитивно автокорельований часовий ряд часто називають персистентним, що значить існування тенденції слідування великих значень за великими та малих за малими, інакше позитивно корельований часовий ряд можна назвати інертним.\nВізьмемо \\(N\\) пар спостережень двох змінних \\(x\\) та \\(y\\). Коефіцієнт кореляції між парами \\(x\\) та \\(y\\) визначається як\n\\[\nr = \\left[ \\sum \\left( x_i - \\bar{x} \\right) \\left( y_i - \\bar{y} \\right) \\right] \\Bigg/ \\left[ \\sqrt{\\sum \\left( x_i - \\bar{x} \\right)^{2}} \\sqrt{\\sum \\left( y_i - \\bar{y} \\right)^{2}} \\right],\n\\tag{1.3}\\]\nде сума знаходиться за всіма \\(N\\) спостереженням.\nТаким же чином можна визначати й автокореляцію, або ж кореляцію всередині досліджуваного часового ряду. Для автокореляції першого порядку береться лаг (часова затримка), рівний 1 часовій одиниці. Отже, автокореляція першого порядку використовує перші \\(N−1\\) спостережень \\(x_t, t = 1,..., N−1\\) та наступні \\(N−1\\) спостережень \\(x_t , t = 2,..., N\\).\nКореляція між \\(x_t\\) та \\(x_t + 1\\) визначається як\n\\[\nr_1 = \\left[ \\sum_{t=1}^{N-1} \\left( x_t - \\bar{x} \\right) \\left( x_{t+1} - \\bar{x} \\right) \\right] \\Bigg/ \\left[ \\sum_{t=1}^{N}\\left( x_t - \\bar{x} \\right)^2 \\right],\n\\tag{1.4}\\]\nде \\(x\\) — це середнє для досліджуваного періоду.\nРівняння (1.4) може бути узагальнене для отримання кореляції між спостереженнями, розділеними \\(k\\) часовими інтервалами:\n\\[\nr_k = \\left[ \\sum_{t=1}^{N-k} \\left( x_t - \\bar{x} \\right) \\left( x_{t+k} - \\bar{x} \\right) \\right] \\Bigg/ \\left[ \\sum_{t=1}^{N}\\left( x_t - \\bar{x} \\right)^2 \\right].\n\\tag{1.5}\\]\nЗначення \\(r_k\\) називається коефіцієнтом автокореляції з лагом \\(k\\). Графік функції автокореляції як залежності \\(r_k\\) від \\(k\\) також називають корелограмою.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Лабораторна робота № 1</span>"
    ]
  },
  {
    "objectID": "lab_1.html#хід-роботи",
    "href": "lab_1.html#хід-роботи",
    "title": "1  Лабораторна робота № 1",
    "section": "1.2 Хід роботи",
    "text": "1.2 Хід роботи\nДля подальшої роботи з моделювання складних систем візьмемо з основу бібліотеку yfinance, що дозволяє працювати з даними фінансових ринків засобами мови програмування Python.\n\n\n\n\n\n\nПримітка\n\n\n\nYahoo!, Y!Finance, and Yahoo! finance є зареєстрованими товарними знаками Yahoo, Inc.\nyfinance не є афілійованим, схваленим або перевіреним Yahoo, Inc. Це інструмент з відкритим вихідним кодом, який використовує загальнодоступні API Yahoo, і призначений для дослідницьких та освітніх цілей.\nВи повинні звернутися до умов використання Yahoo! (сюди, сюди і сюди) для отримання детальної інформації про ваші права на використання фактично завантажених даних. Пам’ятайте — фінансовий API Yahoo! призначений лише для особистого використання\n\n\nДля встановлення бібліотеки yfinance можете скористатися наступною командою:\n\n!pip install yfinance --upgrade --no-cache-dir\n\nГітхаб репозиторій містить більше інформації по самій бібліотеці та помилкам, що можуть виникнути та їх потенційним рішенням.\n\n1.2.1 Вступ до модуля Ticker()\nПерш за все імпортуємо бібліотеку yfinance за допомогою наступної команди:\n\nimport yfinance as yf\n\nМодуль Ticker() дозволяє отримувати ринкові та метадані для цінного паперу, використовуючи Python:\n\nmsft = yf.Ticker(\"MSFT\")\nprint(msft)\n\nyfinance.Ticker object &lt;MSFT&gt;\n\n\nМожна вилучити всю інформацію по досліджуваному індексу:\n\n# отримати інформацію по індексу\nmsft.info\n\nМожна вилучити ринкові значення за максимальний період часу:\n\n# отримати ринкові історичні значення індексу\nmsft.history(period=\"max\")\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\nDividends\nStock Splits\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n1986-03-13 00:00:00-05:00\n0.054792\n0.062849\n0.054792\n0.060163\n1031788800\n0.0\n0.0\n\n\n1986-03-14 00:00:00-05:00\n0.060163\n0.063386\n0.060163\n0.062311\n308160000\n0.0\n0.0\n\n\n1986-03-17 00:00:00-05:00\n0.062311\n0.063923\n0.062311\n0.063386\n133171200\n0.0\n0.0\n\n\n1986-03-18 00:00:00-05:00\n0.063386\n0.063923\n0.061237\n0.061774\n67766400\n0.0\n0.0\n\n\n1986-03-19 00:00:00-05:00\n0.061774\n0.062311\n0.060163\n0.060700\n47894400\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2024-03-18 00:00:00-04:00\n414.250000\n420.730011\n413.779999\n417.320007\n20106000\n0.0\n0.0\n\n\n2024-03-19 00:00:00-04:00\n417.829987\n421.670013\n415.549988\n421.410004\n19837900\n0.0\n0.0\n\n\n2024-03-20 00:00:00-04:00\n422.000000\n425.959991\n420.660004\n425.230011\n17860100\n0.0\n0.0\n\n\n2024-03-21 00:00:00-04:00\n429.829987\n430.820007\n427.160004\n429.369995\n21296200\n0.0\n0.0\n\n\n2024-03-22 00:00:00-04:00\n429.700012\n429.859985\n426.070007\n428.739990\n17636500\n0.0\n0.0\n\n\n\n\n9584 rows × 7 columns\n\n\n\n\nОкрім цього, yfinance дозволяє отримати інформацію по дивідентам та сплітам фінансового індексу:\n\n# показувати дії (дивіденди, спліти)\nmsft.actions\n\n\n\n\n\n\n\n\n\nDividends\nStock Splits\n\n\nDate\n\n\n\n\n\n\n1987-09-21 00:00:00-04:00\n0.00\n2.0\n\n\n1990-04-16 00:00:00-04:00\n0.00\n2.0\n\n\n1991-06-27 00:00:00-04:00\n0.00\n1.5\n\n\n1992-06-15 00:00:00-04:00\n0.00\n1.5\n\n\n1994-05-23 00:00:00-04:00\n0.00\n2.0\n\n\n...\n...\n...\n\n\n2023-02-15 00:00:00-05:00\n0.68\n0.0\n\n\n2023-05-17 00:00:00-04:00\n0.68\n0.0\n\n\n2023-08-16 00:00:00-04:00\n0.68\n0.0\n\n\n2023-11-15 00:00:00-05:00\n0.75\n0.0\n\n\n2024-02-14 00:00:00-05:00\n0.75\n0.0\n\n\n\n\n90 rows × 2 columns\n\n\n\n\n\n# продемонструвати дивіденти\nmsft.dividends\n\nDate\n2003-02-19 00:00:00-05:00    0.08\n2003-10-15 00:00:00-04:00    0.16\n2004-08-23 00:00:00-04:00    0.08\n2004-11-15 00:00:00-05:00    3.08\n2005-02-15 00:00:00-05:00    0.08\n                             ... \n2023-02-15 00:00:00-05:00    0.68\n2023-05-17 00:00:00-04:00    0.68\n2023-08-16 00:00:00-04:00    0.68\n2023-11-15 00:00:00-05:00    0.75\n2024-02-14 00:00:00-05:00    0.75\nName: Dividends, Length: 81, dtype: float64\n\n\n\n# продемонструвати спліти\nmsft.splits\n\nDate\n1987-09-21 00:00:00-04:00    2.0\n1990-04-16 00:00:00-04:00    2.0\n1991-06-27 00:00:00-04:00    1.5\n1992-06-15 00:00:00-04:00    1.5\n1994-05-23 00:00:00-04:00    2.0\n1996-12-09 00:00:00-05:00    2.0\n1998-02-23 00:00:00-05:00    2.0\n1999-03-29 00:00:00-05:00    2.0\n2003-02-18 00:00:00-05:00    2.0\nName: Stock Splits, dtype: float64\n\n\nДля методу history() доступні аргументи:\n\nperiod: період даних для завантаження (або використовуйте параметр period, або використовуйте start і end). Допустимі періоди: 1d, 5d, 1mo, 3mo, 6mo, 1y, 2y, 5y, 10y, ytd, max;\ninterval: інтервал даних (внутрішньоденні дані не можуть перевищувати 60 днів) Допустимі інтервали 1m, 2m, 5m, 15m, 30m, 60m, 90m, 1h, 1d, 5d, 1wk, 1mo, 3mo;\nstart: Якщо не використовується період — завантажте рядок дати початку у форматі (YYYY-MM-DD) або datetime;\nend: Якщо не використовується період — завантажте рядок дати закінчення (YYYY-MM-DD) або datetime;\nprepost: Включати в результати попередні та пост ринкові дані (За замовчуванням False);\nauto_adjust: Автоматично налаштовувати всі OHLC (ціни відкриття, закриття, найбільшу та найменшу) (За замовчуванням True);\nactions: Завантажувати події дивідендів та дроблення акцій (За замовчуванням True).\n\n\n\n1.2.2 Одночасне вивантаження декількох ринкових активів\nЯк і до цього, ви також можете завантажувати дані для кількох тикерів одночасно:\n\ndata = yf.download(\"SPY AAPL\", \n                   start=\"2017-01-01\", \n                   end=\"2017-04-30\") # вивантажуємо дані, \n                                     # зберігаємо до змінної data\n\ndata.head() # виводимо перші 5 рядків нашого масиву даних \n\n[*********************100%%**********************]  2 of 2 completed\n\n\n\n\n\n\n\n\n\nPrice\nAdj Close\nClose\nHigh\nLow\nOpen\nVolume\n\n\nTicker\nAAPL\nSPY\nAAPL\nSPY\nAAPL\nSPY\nAAPL\nSPY\nAAPL\nSPY\nAAPL\nSPY\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2017-01-03\n26.989265\n199.200607\n29.037500\n225.240005\n29.082500\n225.830002\n28.690001\n223.880005\n28.950001\n225.039993\n115127600\n91366500\n\n\n2017-01-04\n26.959055\n200.385712\n29.004999\n226.580002\n29.127501\n226.750000\n28.937500\n225.610001\n28.962500\n225.619995\n84472400\n78744400\n\n\n2017-01-05\n27.096151\n200.226501\n29.152500\n226.399994\n29.215000\n226.580002\n28.952499\n225.479996\n28.980000\n226.270004\n88774400\n78379000\n\n\n2017-01-06\n27.398235\n200.942886\n29.477501\n227.210007\n29.540001\n227.750000\n29.117500\n225.899994\n29.195000\n226.529999\n127007600\n71559900\n\n\n2017-01-09\n27.649183\n200.279572\n29.747499\n226.460007\n29.857500\n227.070007\n29.485001\n226.419998\n29.487499\n226.910004\n134247600\n46939700\n\n\n\n\n\n\n\n\nДля отримання конкретно цін закриття індексу SPY вам варто використовувати наступну команду: data['Close']['SPY'] Але, якщо вам необхідно згрупувати дані за їх символом, можна скористатися наступним записом:\n\ndata = yf.download(\"SPY AAPL\", \n                   start=\"2017-01-01\", \n                   end=\"2017-04-30\",\n                   group_by=\"ticker\")\n\n[*********************100%%**********************]  2 of 2 completed\n\n\nТепер для звернення до цін закриття індексу SPY вам треба використовувати наступний запис: data['SPY']['Close'].\nМетод download() приймає додатковий параметр threads для швидшої обробки великої кількості фінансових індексів одночасно.\nДля подальшої роботи нас ще будуть цікавати наступні бібліотеки:\n\nmatplotlib: комплексна бібліотека для створення статичних, анімованих та інтерактивних візуалізацій на Python. Matplotlib робить прості речі простими, а складні — можливими;\npandas: програмна бібліотека написана для мови програмування Python для маніпулювання та аналізу даних. Зокрема, вона пропонує структури даних та операції з числовими таблицями та часовими рядами;\nnumpy: бібліотека, що додає підтримку великих багатовимірних масивів і матриць, а також колекцію високорівневих математичних функцій для роботи з цими масивами;\nneurokit2: зручна бібліотека, що забезпечує легкий доступ до розширених процедур обробки біосигналів. Дослідники та клініцисти без глибоких знань програмування або біомедичної обробки сигналів можуть аналізувати фізіологічні дані за допомогою лише двох рядків коду. Перевага даної бібліотеки полягає в тому, що вона надає функціонал, який можна використовувати не лише для біомедичних сигналів, але й для фінансових, фізичних тощо.\n\nВстановити кожну з даних бібліотек можна в наступний спосіб: !pip install *назва бібліотеки*:\n\n!pip install matplotlib pandas numpy neurokit2\n\nДалі нам треба буде визначити стиль рисунків для виведення та збереження. Встановимо наступну бібліотеку:\n\n# для встановлення останньої версії (із PyPI)\n!pip install SciencePlots\n\nІмпортуємо кожну із зазначених бібліотек:\n\nimport matplotlib.pyplot as plt\nimport pandas as pd \nimport numpy as np\nimport neurokit2 as nk\nimport scienceplots\n\n# магічна команда для вбудування рисунків у середовище jupyter блокнотів\n%matplotlib inline  \n\nВиконаємо налаштування стилю наших подальших рисунків:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nsize = 16\nparams = {\n    'figure.figsize': (8, 6),            # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': size,                   # розмір фонтів рисунку\n    'lines.linewidth': 2,                # товщина ліній\n    'axes.titlesize': 'small',           # розмір титулки над рисунком\n    'axes.labelsize': size,              # розмір підписів по осям\n    'legend.fontsize': size,             # розмір легенди\n    'xtick.labelsize': size,             # розмір розмітки по осі Ох\n    'ytick.labelsize': size,             # розмір розмітки по осі Ох\n    \"font.family\": \"Serif\",              # сімейство стилів підписів \n    \"font.serif\": [\"Times New Roman\"],   # стиль підпису\n    'savefig.dpi': 300,                  # якість збережених зображень\n    'axes.grid': False                   # побудова сітки на самому рисунку\n}\n\nplt.rcParams.update(params)              # оновлення стилю згідно налаштувань\n\nПредставлені налаштування є орієнтовними і можуть змінюватись у ході наступних лабораторних роботах. Ви можете встановлювати власні налаштування. На сайті бібліотеки matplotlib можна ознайомитись з усіма можливими командами.\nРозглянемо можливість використання всіх згаданих показників у якості індикаторів або індикаторів-передвісників кризових явищ. Для прикладу завантажимо часовий ряд Біткоїна за період з 1 вересня 2015 по 1 березня 2020, використовуючи yfinance:\n\nsymbol = 'BTC-USD'                      # Символ індексу\nstart = \"2015-09-01\"                    # Дата початку зчитування даних\nend = \"2020-03-01\"                      # Дата закінчення зчитування даних\n\ndata = yf.download(symbol, start, end)  # вивантажуємо дані\ntime_ser = data['Adj Close'].copy()     # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'                   # підпис по вісі Ох \nylabel = symbol                         # підпис по вісі Оу\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nВажливо\n\n\n\nПредставлені в подальшому методи є універсальними. Іншими словами, ви можете їх використовувати не лише для фінансових часових рядів, але й для біологічних, фізичних та інших систем, що існують в осяжній нами реальності та можуть бути репрезентовані у вигляду часового ряду. Може бути так, що наявні у вас дані, наприклад, представлені у форматі текстового документа (.txt). Нижче представлено приклад зчитування текстового файлу, що представляє залежність між напруженням і деформацією твердого тіла. Представлену далі залежність було отримано в результаті механічних випробувань певного металу. Зрозуміло, що аналіз результатів і висновки у цьому випадку залежать від того, з яким рядом ми працюємо\n\n\n\n\nsymbol = 'sMpa11'                 # Символ індексу\n\npath = \"databases\\sMpa11.txt\"     # шлях по якому здійснюється зчитування файлу\ndata = pd.read_csv(path,          # зчитування даних \n                   names=[symbol])\ntime_ser = data[symbol].copy()    # копіюємо значення кривої \n                                  # \"напруга-видовження\" до окремої змінної\n\nxlabel = r'$\\varepsilon$'    # підпис по вісі Ох \nylabel = symbol              # підпис по вісі Оу\n\nТепер виведемо значення, що були зчитані з текстового документа:\n\nfig, ax = plt.subplots(1, 1)               # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)\nax.set_ylabel(ylabel)\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\n\n\n\nРис. 1.1: Діаграма деформації\n\n\n\n\n\nДля даного ряду, за потребою, можна виконувати подальші розрахунки.\n\n\n\n\n\n\n\nУвага\n\n\n\nЗнову повертаємось до Біткоїна. Для відтворення подальших розрахунків з Біткоїном блок коду в якому зчитувалась та виводилась крива “напруга-видовження” треба проігнорувати\n\n\nВиведемо значення Біткоїна:\n\nfig, ax = plt.subplots(1, 1)               # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)\nax.set_ylabel(ylabel)\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\n\n\n\nРис. 1.2: Динаміка щоденних змін індексу Біткоїна\n\n\n\n\n\nВидно, що ряд нестаціонарний, що викликає певні ускладнення для подальшого аналізу. Тому перейдемо до прибутковостей, які вже є стаціонарними, а нормалізація стандартним відхиленням дозволяє легко порівнювати їх розподіл з розподілом Гауса.\nПрибутковості розраховуватимуться згідно рівняння (1.1). У Python для цього ми використовуватимемо метод pct_change(), що доступний нам завдяки бібліотеці pandas.\nСтандартизовані прибутковості можна визначити як \\(g(t) = \\left[G(t) - \\mu \\right] / \\sigma\\), де \\(\\mu\\) відповідає середньому значенню прибутковостей за досліджуваний часовий інтервал, а \\(\\sigma\\) представляє стандартне відхилення.\n\nret = time_ser.copy()      # копіюємо значення вихідного ряду для збереження \n                           # його від змін\n\nret = ret.pct_change()     # знаходимо прибутковості\nret -= ret.mean()          # вилучаємо середнє \nret /= ret.std()           # ділимо на стандартнє відхилення\n\nret = ret.dropna().values  # видаляємо всі можливі нульові значення \n\nВиводимо отриманий результат:\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index[1:], ret)           # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Додаємо підпис для вісі Ох\nax.set_ylabel(ylabel + ' прибутковості')   # Додаємо підпис для вісі Оу\nax.axhline(y = 3.0, \n           color = 'r', \n           linestyle = '--')    # Додаємо горизонтальну лінію, \n                                # що роз межує 3 сигма події\nax.axhline(y = -3.0, \n           color = 'r', \n           linestyle = '--')    # Додаємо горизонтальну лінію, \n                                # що розмежує -3 сигма події\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'Прибутковості{symbol}.jpg')  # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\n\n\n\nРис. 1.3: Нормалізовані прибутковості досліджуваного часового ряду\n\n\n\n\n\nЗверніть увагу, що флуктуації нормалізованих прибутковостей досить часто перевищують величину \\(\\pm 3\\sigma\\), що, як відомо, надзвичайно рідко спостерігається для незалежних подій. Цей факт можна відобразити шляхом порівняння функції розподілу нормалізованих флуктуацій з розподілом Гауса (Рис. 1.4). Очевидно, що хвости розподілу вихідного ряду містять значні флуктуації, вони досить помітні (часто кажуть “важкі” у порівнянні з самою “головою” розподілу).\nДля побудови нормального розподілу скористаємось бібліотекою scipy. Встановити її можна за аналогією з попередніми бібліотеками.\n\n# Для встановлення останньої версії scipy\n!pip install scipy\n\n\nfrom scipy.stats import norm # імпорт модуля norm для побудови Гаусового розподілу\n\nФункція щільності ймовірності norm для дійсних значень \\(x\\) має наступний вигляд: \\(f(x) = \\exp{(-x^2/2)} \\big/ \\sqrt{2\\pi}\\).\n\nmu, sigma = norm.fit(ret)\n\nx = np.linspace(ret.min(), ret.max(), 10000) # Генеруємо 10000 значень для побудови \n                                             # Гаусового розподілу\np = norm.pdf(x, mu, sigma)                   # Отримання значень функції щільності\n\n\nfig, ax = plt.subplots(1, 1)               # Створюємо порожній графік\nax.plot(x, p, label='Гаусіан')             # Додаємо дані до графіку\nax.hist(ret, bins=50,                      # Побудова гістограми для прибутковостей\n        density=True, \n        alpha=0.6, \n        color='g',\n        label='Прибутковості '+ symbol)\n\nax.legend()                                # Додаємо легенду\nax.set_xlabel(\"g\")                         # Додаємо підпис для вісі Ох\nax.set_ylabel(r\"$f(g)$\")                   # Додаємо підпис для вісі Оу\nax.set_yscale('log')\n\n\nplt.savefig(f'Гаус + прибутковості {symbol}.jpg')   # Зберігаємо графік \nplt.show();                                         # Виводимо графік\n\n\n\n\n\n\n\nРис. 1.4: Порівняння функцій розподілу нормалізованих прибутковостей з нормальним розподілом\n\n\n\n\n\nЯк ми можемо бачити, крива Гауса відхиляється від істинної частоти настання подій, що перевищують \\(\\pm 3\\sigma\\), і ми можемо стверджувати, що прибутковості не є незалежними. Підтвердження цьому факту будемо шукати шляхом вивчення кореляційних властивостей нашого часового ряду.\nДля простоти обчислень скористаємось функцією signal_autocor() бібліотеки neurokit2. Виглядає дана функція наступним чином:\nsignal_autocor(signal, lag=None, demean=True, method='auto', show=False)\nПараметри:\n\nsignal (Union[list, np.array, pd.Series]) — вектор значень;\nlag (int) — часовий лаг. Якщо вказано, буде повернуто одне значення автокореляції сигналу з його власним лагом;\ndemean (bool) — якщо має значення True, від сигналу буде відніматися середнє значення сигналу перед обчисленням автокореляції;\nmethod (str) — використання \"auto\" запускає scipy.signal.correlate швидшого алгоритму. Інші методи зберігаються з причин застарілості, але не рекомендуються. Вони включають \"correlation\" (за допомогою np.correlate()) або \"fft\" (швидке перетворення Фур’є);\nshow (bool) — якщо значення True, побудувати графік автокореляції для всіх значень затримки.\n\nПовертає:\n\nr (float) — крос-кореляція сигналу із самим собою на різних часових лагах. Мінімальний часовий лаг дорівнює 0, максимальний часовий лаг дорівнює довжині сигналу. Або значення кореляції на певному часовому лазі, якщо лаг не дорівнює None;\ninfo (dict) — cловник, що містить додаткову інформацію, наприклад, довірчий інтервал.\n\n\n# розрахунок автокореляції\n\nr_init, _ = nk.signal_autocor(time_ser.values, \n                              method='correlation')  # для вихідних значень ряду                                                                    \nr_ret, _ = nk.signal_autocor(ret, \n                             method='correlation')   # для прибутковостей\nr_vol, _ = nk.signal_autocor(np.abs(ret), \n                             method='correlation')   # для модулів прибутковостей\n\nr_range = np.arange(1, len(r_ret) + 1)               # генерація лагів\n\n\nfig, ax = plt.subplots(1, 1)                # Створюємо порожній графік\n\nax.plot(r_range, r_init[1:], label=symbol)  # Додаємо дані до графіку\nax.plot(r_range, r_ret, label=r'$g(t)$')                          \nax.plot(r_range, r_vol, label=r'$V_{T}$') \n\nax.legend()                                 # Додаємо легенду\nax.set_xlabel(r\"Часовий лаг $k$\")           # Додаємо підпис для вісі Ох\nax.set_ylabel(r\"Автокореляція $r$\")         # Додаємо підпис для вісі Оу\nax.set_ylim(-1.1, 1.1)                      # Встановлюємо обмеження по вісі Oy\n\nplt.savefig(f'Автокореляції {symbol}.jpg')  # Зберігаємо графік \nplt.show();                                 # Виводимо графік\n\n\n\n\n\n\n\nРис. 1.5: Зміна з часом парних автокореляційних функцій для вихідного ряду \\(x\\), нормалізованих прибутковостей \\(g\\) та їх модулів \\(|g|\\)\n\n\n\n\n\nАле, досліджуючи складні системи, варто пам’ятати, що їх складність є варіативною. Тому і внутрішні кореляції системи на різних часових лагах також варіюються з плином часу. Із цього випливає, що подальщі розрахунки варто виконувати не для всього ряду, а для його фрагментів.\nСформулюємо алгоритм ковзного (рухомого) вікна. Виділимо фрагмент часового ряду (вікно), розрахуємо необхідні міри складності, змістимо вікно вздовж часового ряду на заздалегідь визначену величину (крок) і повторимо процедуру до вичерпання часового ряду. Далі, порівнюючи динаміку фактичного часового ряду і відповідних мір складності, ми матимемо змогу судити про характерні зміни в динаміці міри складності зі зміною досліджуваної системи. Якщо та чи інша міра складності поводиться характерним чином для всіх особливих періодів (наприклад, крахів), зменшується або збільшується у передкризовий період, то вона може служити їх індикатором або передвісником.\nРозглянемо як поводитиме себе функція автокореляцій та волатильність в рамках алгоритму ковзного вікна.\nСпочатку визначимо параметри:\n\nret_type = 4           # вид ряду: \n                       # 1 - вихідний, \n                       # 2 - детрендований (різниця між теп. значенням та попереднім)\n                       # 3 - прибутковості звичайні, \n                       # 4 - стандартизовані прибутковості, \n                       # 5 - абсолютні значення (волатильності),\n                       # 6 - стандартизований вихідний ряд \n\nlength = len(time_ser) # довжина всього ряду\n\nwindow = 250           # довжина вікна - період у межах якого розраховуватимуться наші індикатори\ntstep = 1              # крок зміщення вікна\nvolatility = []        # масив значень волатильностей \nautocorr = []          # масив значень автокореляції при змінній lag\n\nДалі розпочнемо розрахунки. Для відслідковування прогресу зміщення ковзного вікна скористаємось бібліотекою tqdm. Її можна встановити аналогічно попереднім бібліотекам:\n\n!pip install tqdm\n\nІмпортуємо модуль для візуалізації прогресу:\n\nfrom tqdm import tqdm\n\nІ тепер приступимо до виконання віконної процедури:\n\nfor i in tqdm(range(0,length-window,tstep)):  # Фрагменти довжиною window  \n                                              # з кроком tstep\n                                              \n    fragm = time_ser.iloc[i:i+window].copy() # відбираємо фрагмент\n\n                                          # подальшому відбираємо потрібний тип ряду                                         \n    if ret_type == 1:                     # вихідні значення \n        pass\n    elif ret_type == 2:                   # різниці\n        fragm = fragm[1:] - fragm[:-1]\n    elif ret_type == 3:                   # прибутковості\n        fragm = fragm.pct_change()\n    elif ret_type == 4:                   # стандартизовані прибутковості\n        fragm = fragm.pct_change()\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n    elif ret_type == 5:                   # абсолютні значення прибутковостей\n        fragm = fragm.pct_change()\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n        fragm = fragm.abs()\n    elif ret_type == 6:                   # стандартизований вихідний ряд\n        fragm -= fragm.mean()\n        fragm /= fragm.std()\n        \n    fragm = fragm.dropna().values         # видаляємо зайві нульові значення, якщо є\n    \n    # розрахунок віконної автокореляції\n    r_window, _ = nk.signal_autocor(fragm, method='correlation') \n\n    # розрахунок волатильності по модулям прибутковостей                                     \n    vol_window = np.mean(np.abs(fragm))\n\n    # збереження результатів до масивів\n    volatility.append(vol_window)\n    autocorr.append(r_window[1])\n\n100%|██████████| 1393/1393 [00:01&lt;00:00, 1140.61it/s]\n\n\nЗбережемо результати в окремих текстових файлах:\n\n# збереження результатів ковзної автокореляції\nnp.savetxt(f\"autocorr_name={symbol}_ \\\n            window={window}_step={tstep}_ \\\n            rettype={ret_type}.txt\", autocorr)\n\n# збереження результатів ковзної волатильності\nnp.savetxt(f\"volatility_name={symbol}_ \\\n            window={window}_step={tstep}_ \\\n            rettype={ret_type}.txt\", volatility)\n\nНарешті, порівняємо динаміку вихідного ряду і розрахованих похідних. Для цього врахуємо, що автокореляцію і волатильність ми знаходили для рухомого вікна. Результати представлено на Рис. 1.6.\n\nfig, ax = plt.subplots(1, 1)\n\nax2 = ax.twinx()\nax3 = ax.twinx()\n\nax2.spines.right.set_position((\"axes\", 1.03))\nax3.spines.right.set_position((\"axes\", 1.10))\n\np1, = ax.plot(time_ser.index[window:length:tstep], \n              time_ser.values[window:length:tstep], \n              \"b-\", label=fr\"{ylabel}\")\n\np2, = ax2.plot(time_ser.index[window:length:tstep], \n               autocorr, \"g-\", label=r\"$A$\")\n\np3, = ax3.plot(time_ser.index[window:length:tstep],\n               volatility, \"m-\", label=r\"$V$\")\n\nax.set_xlabel(xlabel)\nax.set_ylabel(f\"{ylabel}\")\n\nax.yaxis.label.set_color(p1.get_color())\nax2.yaxis.label.set_color(p2.get_color())\nax3.yaxis.label.set_color(p3.get_color())\n\ntkw = dict(size=4, width=1.5)\nax.tick_params(axis='y', colors=p1.get_color(), **tkw)\nax2.tick_params(rotation=90, axis='y', colors=p2.get_color(), **tkw)\nax3.tick_params(rotation=30, axis='y', colors=p3.get_color(), **tkw)\n\nax.tick_params(rotation=45, axis='x', **tkw)\n\nax3.legend(handles=[p1, p2, p3], fontsize=18)\n\nplt.savefig(f\"all_name={symbol}_ret={ret_type}_\\\n            wind={window}_step={tstep}.jpg\")\n\nplt.show();\n\n\n\n\n\n\n\nРис. 1.6: Динаміка індексу Біткоїна, віконних автокореляції та волатильності\n\n\n\n\n\nАналізуючи графік, можна зробити висновок, що у певні моменти спостерігалися стрибки волатильності (як і автокореляції) із поступовим зменшенням її до попереднього рівня, що може бути наслідком збурень у процесі роботи ринку. Аналіз таких збурень, їх частоти та сили, дозволяє виявляти приховані закономірності роботи ринку.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Лабораторна робота № 1</span>"
    ]
  },
  {
    "objectID": "lab_1.html#висновок",
    "href": "lab_1.html#висновок",
    "title": "1  Лабораторна робота № 1",
    "section": "1.3 Висновок",
    "text": "1.3 Висновок\nТаким чином, аналіз флуктуацій прибутковостей та волатильностей шляхом побудови функції автокореляції та розподілу ймовірності дозволяє отримати певні висновки, що можуть допомогти в роботі із аналізованими часовими рядами і ринком, з якого взято зазначені часові ряди. Зокрема, у даному випадку, можна надавати корисні рекомендації фінансовим аналітикам.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Лабораторна робота № 1</span>"
    ]
  },
  {
    "objectID": "lab_1.html#завдання-для-самостійної-роботи",
    "href": "lab_1.html#завдання-для-самостійної-роботи",
    "title": "1  Лабораторна робота № 1",
    "section": "1.4 Завдання для самостійної роботи",
    "text": "1.4 Завдання для самостійної роботи\n\nОтримати індекс часового ряду у викладача\nПровести дослідження згідно інструкції\nДослідити зміни розрахованих величин для вікон 100 і 500, з кроком 1. Порівняти результати\nЗробити висновки",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Лабораторна робота № 1</span>"
    ]
  },
  {
    "objectID": "lab_1.html#контрольні-питання",
    "href": "lab_1.html#контрольні-питання",
    "title": "1  Лабораторна робота № 1",
    "section": "1.5 Контрольні питання",
    "text": "1.5 Контрольні питання\n\nПорівняйте вид залежностей флуктуацій цін і прибутковостей. Чому при розрахунках користуються не цінами, а прибутковостями?\nЯку характеристику ряду визначає волатильність?\nУ чому причина різних залежностей для прибутковостей та їх модулів?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Лабораторна робота № 1</span>"
    ]
  },
  {
    "objectID": "lab_1.html#додаток",
    "href": "lab_1.html#додаток",
    "title": "1  Лабораторна робота № 1",
    "section": "1.6 Додаток",
    "text": "1.6 Додаток\nДля обрання часового індексу часового ряду використаємо дані, що розміщенні на сайті Yahoo! Finance. Оскільки окремі фінансові показники не завжди є доступними, будемо використовувати список компаній, що входять до індексу DJIA. За вказаним посиланням та номером у списку групи оберіть компанію, що входить до індексу та проведіть відповідні розрахунки. Порівняйте отримані результати з такими ж для Біткоїна.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Лабораторна робота № 1</span>"
    ]
  },
  {
    "objectID": "lab_2.html",
    "href": "lab_2.html",
    "title": "2  Лабораторна робота № 2",
    "section": "",
    "text": "2.1 Теоретичні відомості\nДослідження складних систем, як природних, так і штучних, показали, що в їх основі лежать нелінійні процеси, ретельне вивчення яких необхідне для розуміння і моделювання складних систем. У останні десятиліття набір традиційних (лінійних) методик дослідження був істотно розширений нелінійними методами, одержаними з теорії нелінійної динаміки і хаосу; багато досліджень були присвячені оцінці нелінійних характеристик і властивостей процесів, що протікають в природі (скейлінг, фрактальна розмірність). Проте більшість методів нелінійного аналізу вимагає або достатньо довгих, або стаціонарних рядів даних, які досить важко одержати в природний спосіб. Більш того, було показано, що дані методи дають задовільні результати для моделей реальних систем, що ідеалізуються. Ці чинники вимагали розробки нових методик нелінійного аналізу даних.\nСтан природних або штучних систем, як правило, змінюється в часі. Вивчення цих, часто складних процесів — важлива задача в багатьох дисциплінах, дозволяє зрозуміти і описати їх суть, наприклад, для прогнозування стану на деякий час у майбутнє. Метою таких досліджень є знаходження математичних моделей, які б достатньо відповідали реальним процесам і могли б бути використані для розв’язання поставлених задач.\nРозглянемо ідею і коротко опишемо теорію рекурентного аналізу, наведемо деякі приклади, розглянемо його можливі області застосування при аналізі і прогнозування складних фінансово-економічних систем.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Лабораторна робота № 2</span>"
    ]
  },
  {
    "objectID": "lab_2.html#теоретичні-відомості",
    "href": "lab_2.html#теоретичні-відомості",
    "title": "2  Лабораторна робота № 2",
    "section": "",
    "text": "2.1.1 Фазовий простір та його реконструкція\nСтан системи описується її змінними стану\n\\[\nx^1(t),x^2(t),...,x^d(t),\n\\]\nде верхній індекс — номер змінної. Набір із \\(d\\) змінних стану у момент часу \\(t\\) складає вектор стану \\(\\vec x(t)\\) в \\(d\\)-вимірному фазовому просторі. Даний вектор переміщується в часі та в напрямі, що визначається його вектором швидкості:\n\\[\n\\dot{\\vec x}(t)=\\partial_t\\vec x(t)=\\vec F(t).\n\\]\nПослідовність векторів \\(\\vec x(t)\\) утворює траєкторію у фазовому просторі, причому поле швидкості \\(\\vec F\\) дотичне до цієї траєкторії. Еволюція траєкторії описує динаміку системи і її атрактор. Знаючи \\(\\vec F\\), можна одержати інформацію про стан системи в момент \\(t\\) шляхом інтегрування виразу. Оскільки форма траєкторії дозволяє судити про характер процесу (періодичні або хаотичні процеси мають характерні фазові портрети), то для визначення стану системи не обов’язково проводити інтегрування, достатньо побудувати графічне відображення траєкторії.\nПри дослідженні складних систем часто відсутня інформація щодо всіх змінних стану, або не всі з них можливо виміряти. Як правило, маємо єдине спостереження, проведене через дискретний часовий інтервал \\(\\Delta t\\). Таким чином, вимірювання записуються у вигляді ряду \\(u_i(t)\\) i, де \\(t=i\\cdot \\Delta t\\). Інтервал \\(\\Delta t\\) може бути постійним, проте це не завжди можливо і створює проблеми для застосування стандартних методів аналізу даних, що вимагають рівномірної шкали спостережень.\nВзаємодії і їх кількість у складних системах такі, що навіть за однією змінною стану можна судити про динаміку всієї системи в цілому (даний факт був встановлений групою американських учених при вивченні турбулентності). Таким чином, еквівалентна фазова траєкторія, що зберігає структури оригінальної фазової траєкторії, може бути відновлена з одного спостереження або часового ряду  [1] за теоремою Такенса (Takens) методом часових затримок  [2]:\n\\[\n\\widehat{\\vec x}(t)=(u_i,u_{i+\\tau},...,u_{i+(m-1)\\tau}).\n\\]\nТут \\(m\\) — розмірність вкладення, \\(\\tau\\) — часова затримка (реальна часова затримка визначається як \\(\\tau \\cdot \\Delta t\\)). Топологічні структури відновленої траєкторії зберігаються, якщо \\(m \\geq 2 \\cdot d+1\\), де \\(d\\) — розмірність атрактора  [2]. На практиці у більшості випадків атрактор може бути відновлений і при \\(m \\leq 2d\\). Затримка, як правило, вибирається апріорно.\nІснує кілька підходів до вибору мінімально достатньої розмірності \\(m\\), крім аналітичного. Високу ефективність показали методи, засновані на концепції фальшивих найближчих точок (false nearest neighbours, FNN). Суть її заключається у тому, що при зменшенні розмірності вкладення відбувається збільшення кількості фальшивих точок, що потрапляють в околицю будь-якої точки фазового простору. Звідси витікає простий метод — визначення кількості FNN як функції розмірності. Існують і інші методи, засновані на цій концепції, наприклад, визначення відносин відстаней між одними і тими ж сусідніми точками при різних \\(m\\). Розмірність атрактора також може бути визначена за допомогою крос-кореляційних сум.\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nРис. 2.1: Відрізок траєкторії у фазовому просторі системи Рьослера \\(i\\) (a) та відповідний рекурентний графік (b). Вектор фазового простору в точці \\(j\\), який потрапляє в околицю (сіре коло в (a)) заданого вектора фазового простору вектора в точці \\(i\\) вважається точкою рекурентності (чорна точка на траєкторії в (a)). Вона позначається чорною точкою на рекурентній діаграмі у позиції \\((i, j)\\). Вектор фазового простору за межами околу (порожнє коло в (a)) позначається білою точкою рекурентній діаграмі\n\n\n\n\n\n2.1.2 Рекурентний аналіз\nПроцесам у природі властива яскраво виражена рекурентна поведінка, така, як періодичність або іррегулярна циклічність. Більш того, рекурентність (повторюваність) станів у значенні проходження подальшої траєкторії достатньо близько до попередньої є фундаментальною властивістю дисипативних динамічних систем. Ця властивість була відмічена ще в 80-х роках XIX століття французьким математиком Пуанкаре (Poincare) і згодом сформульовано у вигляді “теореми рекурентності”, опублікованої в 1890 р.:\n\n\n\n\n\n\nПримітка\n\n\n\nЯкщо система зводить свою динаміку до обмеженої підмножини фазового простору, то вона майже напевно, тобто з вірогідністю, практично рівною 1, скільки завгодно близько повертається до якого-небудь спочатку заданого режиму\n\n\nСуть цієї фундаментальної властивості у тому, що, навіть мале збурення в складній динамічній системі може привести систему до експоненціального відхилення від її стану, через деякий час система прагне повернутися до стану близького до попереднього, і проходить при цьому подібні етапи еволюції.\nПереконатися в цьому можна за допомогою графічного зображення траєкторії системи у фазовому просторі. Проте можливості такого аналізу сильно обмежені. Як правило, розмірність фазового простору складної динамічної системи більша трьох, що робить практично незручним його розгляд напряму; єдина можливість — проекції в дво- і тривимірні простори, що часто не дає вірного уявлення про фазовий портрет.\nУ 1987 р. Екман (Eckmann) і співавтори запропонували спосіб відображення \\(m\\)-вимірної фазової траєкторії станів системи \\(\\vec x(t)\\) завдовжки \\(N\\) на двовимірну квадратну двійкову матрицю розміром \\(N \\times N\\)  [3], в якій 1 (чорна точка) відповідає повторенню стану при деякому часі \\(i\\) в деякий інший час \\(j\\), а обидві координатні осі є осями часу. Таке представлення було назване рекурентною картою або діаграмою (recurrence plot, RP), оскільки воно фіксує інформацію про рекурентну поведінку системи.\nМатематично вищесказане описується як\n\\[\nR_{i,j}^{m,\\varepsilon_i}=\\Theta(\\varepsilon_i-\\| \\vec x_i - \\vec x_j \\|), \\quad \\vec x \\in \\Re^m, \\quad i, j=1,...,N,\n\\]\nде \\(N\\) — кількість даних станів, \\(x_i, \\varepsilon_i\\) — розмір околиці точки \\(\\vec x\\) у момент \\(i\\), \\(\\| \\cdot \\|\\) — норма і \\(\\Theta(\\cdot)\\) — функція Хевісайда.\nНепрактично і, як правило, неможливо знайти повну рекурентність у значенні \\(\\vec x_i \\equiv \\vec x_j\\) (стан динамічної, а особливо — хаотичної системи не повторюється повністю еквівалентно початковому стану, а підходить до нього скільки завгодно близько). Таким чином, рекурентність визначається як достатня близькість стану \\(\\vec x_j\\) до стану \\(\\vec x_i\\). Іншими словами, рекурентними є стани \\(\\vec x_j\\), які потрапляють в \\(m\\)-вимірну околицю з радіусом \\(\\varepsilon_i\\) і центром в \\(\\vec x_i\\). Ці точки \\(\\vec x_j\\) називаються рекурентними точками (recurrence points)  [4,5].\nОскільки \\(R_{i,i}=1\\), \\(i=1,...,N\\) за визначенням, то рекурентна діаграма завжди містить чорну діагональну лінію — лінію ідентичності (line of identity, LOI) під кутом \\(\\pi/4\\) до осей координат. Довільно узята рекурентна точка не несе якої-небудь корисної інформації про стани в часи \\(i\\) і \\(j\\). Тільки вся сукупність рекурентних точок дозволяє відновити властивості системи.\nЗовнішній вигляд рекурентної діаграми дозволяє судити про характер процесів, які протікають в системі, наявності і впливі шуму, станів повторення і завмирання (ламінарності), здійсненні в ході еволюції системи різких змін стану (екстремальних подій).\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\n\n\n\n(e)\n\n\n\n\n\n\nРис. 2.2: Динамічні ряди, що характеризують однорідність (a), дрейф (b), осциляції (c), контрастну топологію (d), ламінарність (e) та побудовані для них рекурентні діаграми\n\n\n\n\n\n2.1.3 Аналіз діаграм\nОчевидно, що процеси різної поведінки даватимуть рекурентні діаграми з різним рисунком. Таким чином, візуальна оцінка діаграм може дати уявлення про еволюцію досліджуваної траєкторії. Виділяють два основних класи структури зображення: топологія (typology), що представляється крупномасштабними структурами, і текстура (texture), що формується дрібномасштабними структурами.\nТопологія дає загальне уявлення про характер процесу. Виділяють чотири основні класи:\n\nоднорідні рекурентні діаграми типові для стаціонарних і автономних систем, в яких час релаксації малий у порівнянні з довжиною ряду;\nперіодичні структури, що повторюються (діагональні лінії, патерни у шаховому порядку) відповідають різним осцилюючим системам з періодичністю в динаміці;\nдрейф відповідає системам з параметрами, що поволі змінюються, і це робить білими лівий верхній і правий нижній кути рекурентної діаграми;\nрізкі зміни в динаміці системи, рівно як і екстремальні ситуації, обумовлюють появу білих областей або смуг.\n\nРекурентні діаграми спрощують виявлення екстремальних і рідкісних подій.\n\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n(c)\n\n\n\n\n\n\n\n(d)\n\n\n\n\n\n\nРис. 2.3: Характернi топологiї рекурентних дiаграм: (а) — однорiдна (нормально розподiлений шум); (b) — перiодична (генератор Ван дер Поля); (c) — дрейф (вiдображення Iкеди з накладеною послiдовнiстю, що лiнiйно росте); (d) — контрастнi областi або смуги (узагальнений броунiвський рух)  [6]\n\n\n\nДетальний розгляд рекурентних діаграм дозволяє виявити дрібномасштабні структури — текстуру, яка складається з простих точок, діагональних, горизонтальних і вертикальних ліній. Комбінації вертикальних і горизонтальних ліній формують прямокутні кластери точок:\n\nсамотні, окремо розташовані рекурентні точки з’являються в тому разі, коли відповідні стани рідкісні, або нестійкі в часі, або викликані сильною флуктуацією. При цьому вони не є ознаками випадковості або шуму;\nдіагональні лінії \\(R_{i+k, j+k}=1\\) (при \\(k = 1...l\\) де \\(l\\) — довжина діагональної лінії) з’являються у разі, коли сегмент траєкторії у фазовому просторі пролягає паралельно іншому сегменту, тобто траєкторія повторює саму себе, повертаючись в одну і ту ж область фазового простору у різний час. Довжина таких ліній визначається часом, протягом якого сегменти траєкторії залишаються паралельними; напрям (кут нахилу) ліній характеризує внутрішній час підпроцесів, відповідних даним сегментам траєкторії. Проходження ліній паралельно лінії ідентичності (під кутом \\(\\pi/4\\) до осей координат) свідчить про однаковий напрям сегментів траєкторії, перпендикулярно — про протилежний (“відображені” сегменти), що може також бути ознакою реконструкції фазового простору з невідповідною розмірністю вкладення. Нерегулярна поява діагональних ліній є ознакою хаотичного процесу;\nвертикальні (горизонтальні) лінії \\(R_{i, j+k}=1\\) (при \\(k = 1...\\upsilon\\), де \\(\\upsilon\\) — довжина вертикальної або горизонтальної лінії) виділяють проміжки часу, в котрі стан системи не змінюється або змінюється не суттєво (система як би “заморожена” на цей час), що є ознакою “ламінарних” станів.\n\n\n\n\n\n\n\nРис. 2.4: Основнi концепцiї рекурентного аналiзу. Вiдображена дiаграма рекурентностi базується на часовому ряді, що було реконструйовано до 11 реконструйованих векторiв, вiд \\(\\vec{X}(0)\\) до \\(\\vec{X}(10)\\). Видiлено дiагональну лiнiю довжиною \\(d = 3\\), вертикальну лiнiю довжиною \\(v = 3\\) i бiлу вертикальну лiнiю довжиною \\(w = 5\\)  [7]",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Лабораторна робота № 2</span>"
    ]
  },
  {
    "objectID": "lab_2.html#хід-роботи",
    "href": "lab_2.html#хід-роботи",
    "title": "2  Лабораторна робота № 2",
    "section": "2.2 Хід роботи",
    "text": "2.2 Хід роботи\nСпочатку побудуємо дво- та тривимірні фазові портрети як для модельних значень, так і для реальних. Використовуватимемо бібліотеки neurokit2 для побудови атракторів та рекурентного аналізу.\n\n2.2.1 Процедура реконструкції фазового простору\nДля побудови фазового портрету скористаємось методами complexity_attractor() та complexity_embedding() бібліотеки neuralkit2. Синтаксис complexity_attractor() виглядає наступним чином:\ncomplexity_attractor(embedded='lorenz', alpha='time', color='last_dim', shadows=True, linewidth=1, **kwargs)\nПараметри:\n\nembedded (Union[str, np.ndarray]) — результат функції complexity_embedding(). Також може бути рядком, наприклад, \"lorenz\" (атрактор Лоренца) або \"rossler\" (атрактор Рьосслера);\nalpha (Union[str, float]) — прозорість ліній;\ncolor (str) — колір графіку. Якщо \"last_dim\", буде використано останній вимір (максимум 4-й) вбудованих даних, коли розмірність більша за 2. Корисно для візуалізації глибини (для 3-вимірного вбудовування), або четвертого виміру, але працюватиме це повільно;\nshadows (bool) — якщо значення True, 2D-проекції буде додано до бокових сторін 3D-атрактора;\nlinewidth (float) — задає товщину лінії;\nkwargs — до палітри кольорів (наприклад, name=\"plasma\") або до симулятора системи Лоренца передаються додаткові аргументи ключових слів, такі як duration (за замовчуванням = 100), sampling_rate (за замовчуванням = 10), sigma (за замовчуванням = 10), beta (за замовчуванням = 8/3), rho (за замовчуванням = 28).\n\nЯк вже зазначалося, побудова фазового простору, на основі якого і проводитиметься рекурентний аналіз, вимагає реконструкції. Виконати реконструкції фазового простору із одновимірного часового ряду можна із використанням методу часових затримок.\nМетод часових затримок є однією з ключових концепцій науки про складність. Він базується на ідеї, що динамічна система може бути описана вектором чисел, який називається її “станом”, і має на меті забезпечити повний опис системи в даний момент часу. Множина всіх можливих станів називається “простором станів”.\nТеорема Такенса  [2] припускає, що послідовність вимірювань динамічної системи містить у собі всю інформацію, необхідну для повної реконструкції простору станів. Метод часових затримок намагається визначити стан \\(s\\) системи в певний момент часу \\(t\\), шукаючи в минулій історії спостережень схожі стани, і, вивчаючи еволюцію схожих станів, виводити інформацію про майбутнє системи.\nЯк візуалізувати динаміку системи? Послідовність значень стану в часі називається траєкторією. Залежно від системи, різні траєкторії можуть еволюціонувати до спільної підмножини простору станів, яка називається атрактором. Наявність та поведінка атракторів дає інтуїтивне уявлення про досліджувану динамічну систему.\nОдже, згідно Такенсу, ідея полягає в тому, щоб на основі одиничних вимірювань системи, отримати \\(m\\)-розмірні реконструйовані часові вкладення\n\\[\n\\vec{x}_i = \\left( x_i, x_{i+\\tau}, ... , x_{i+(m-1)\\tau} \\right),\n\\tag{2.1}\\]\nа \\(i\\) проходить в діапазоні \\(1,..., N-(m-1)\\tau\\); значення \\(\\tau\\) представляє часову затримку, а \\(m\\) — це розмірність вкладень (кількість змінних, що включає кожна траєкторія).\nКод для реконструкції фазового простору може виглядати наступним чином:\n\ndef complexity_embedding(signal, dimension, delay):\n    N = len(signal)                                        # вимірюємо довжину сигналу\n    Y = np.zeros((dimension, N - (dimension - 1) * delay)) # ініціалізуємо масив нулів,\n                                                           # що будуть представляти траєкторії\n    for i in range(dimension):\n        Y[i] = signal[i * delay : i * delay + Y.shape[1]]  # заповнюємо кожну траєкторію \n\n    embedded = Y.T                                          \n    return embedded                                        # повертаємо результат \n\nДля реконструкції фазового простору використовуватимемо метод complexity_embedding(). Його синтаксис:\ncomplexity_embedding(signal, delay=1, dimension=3, show=False, **kwargs)\nПараметри:\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень. Також може бути рядком, наприклад, \"lorenz\" (атрактор Лоренца), \"rossler\" (атрактор Росслера) або \"clifford\" (атрактор Кліффорда) для отримання попередньо визначеного атрактора;\ndelay (int) — часова затримка (часто позначається \\(\\tau\\) іноді називають запізненням). Ще розглянемо метод complexity_delay() для оцінки оптимального значення цього параметра;\ndimension (int) — розмірність вкладень (\\(m\\), іноді позначається як \\(d\\)). Далі звернемось до методу complexity_dimension(), щоб оцінити оптимальне значення для цього параметра;\nshow (bool) — побудувати графік реконструйованого атрактора;\nkwargs — інші аргументи, що передаються до complexity_attractor().\n\nПовертає:\n\narray — реконструйований атрактор розміру length - (dimension - 1) * delay.\n\nДалі імпортуємо необхідні для подальшої роботи модулі:\n\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport neurokit2 as nk\nimport yfinance as yf\nimport scienceplots\nimport pandas as pd\n\n%matplotlib inline\n\nІ виконаємо налаштування рисунків для виводу:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nsize = 16\nparams = {\n    'figure.figsize': (8, 6),            # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': size,                   # розмір фонтів рисунку\n    'lines.linewidth': 2,                # товщина ліній\n    'axes.titlesize': 'small',           # розмір титулки над рисунком\n    'axes.labelsize': size,              # розмір підписів по осям\n    'legend.fontsize': size,             # розмір легенди\n    'xtick.labelsize': size,             # розмір розмітки по осі Ох\n    'ytick.labelsize': size,             # розмір розмітки по осі Ох\n    \"font.family\": \"Serif\",              # сімейство стилів підписів \n    \"font.serif\": [\"Times New Roman\"],   # стиль підпису\n    'savefig.dpi': 300,                  # якість збережених зображень\n    'axes.grid': False                   # побудова сітки на самому рисунку\n}\n\nplt.rcParams.update(params)              # оновлення стилю згідно налаштувань\n\nТепер розглянемо можливість використання методу часових затримок і отриманих у подальшому атракторів у якості індикатора складності. Як і в попередній роботі, для прикладу завантажимо часовий ряд Біткоїна за період з 1 вересня 2015 по 1 березня 2020, використовуючи yfinance:\n\nsymbol = 'BTC-USD'       # Символ індексу\nstart = \"2015-09-01\"     # Дата початку зчитування даних\nend = \"2020-03-01\"       # Дата закінчення зчитування даних\n\ndata = yf.download(symbol, start, end)  # вивантажуємо дані\ntime_ser = data['Adj Close'].copy()     # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'                 # підпис по вісі Ох \nylabel = symbol                       # підпис по вісі Оу\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nУвага\n\n\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance, а із власного файлу. Зрозуміло, що й аналіз результатів, і висновки залежать від того з яким рядом ми працюємо\n\n\n\n\nsymbol = 'sMpa11'                  # Символ індексу\n\npath = \"databases\\sMpa11.txt\"      # шлях по якому здійснюється зчитування файлу\ndata = pd.read_csv(path,           # зчитування даних \n                   names=[symbol])\ntime_ser = data[symbol].copy()     # копіюємо значення кривої \n                                   # \"напруга-видовження\" до окремої змінної\n\nxlabel = r'$\\varepsilon$'          # підпис по вісі Ох \nylabel = symbol                    # підпис по вісі Оу\n\n\nВиводимо досліджуваний ряд:\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\n\n\n\nРис. 2.5: Динаміка щоденних змін індексу Біткоїна\n\n\n\n\n\nСпочатку оберемо вид ряду: 1 - вихідний ряд; 2 - детермінований (різниця між теперішнім та попереднім значенням); 3 - прибутковості звичайні; 4 - стандартизовані прибутковості; 5 - абсолютні значення (волатильності); 6 - стандартизований ряд.\nДля подальших розрахунків накращим варіантом буде вибір стандартизованого вихідного ряду або прибутковостей, оскільки значення вихідного часового ряду відрізняються на декілька порядків, і можуть сильно перевищувати встановлений параметр \\(\\varepsilon\\). У цьому випадку для вихідних значень, що сильно різняться між собою, увесь часовий діапазон буде розглядатися як нерекурентний.\nСпочатку визначимо функції для виконання перетворення ряду:\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\nі тепер виконаємо перетворення, використовуючи дану функцію:\n\nsignal = time_ser.copy()\nret_type = 6    # вид ряду: 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nfor_rec = transformation(signal, ret_type) \n\nОскільки ми не матимемо змоги візуалізувати багатовимірний фазовий простір (\\(m&gt;3\\)), ми послуговуватимемось значеннями \\(m=2\\) та \\(m=3\\). Значення \\(\\tau\\) будемо варіювати як із власних переконань, так і з опорою на функціонал бібліотеки neuralkit2.\nСкористаємось методом complexity_simulate() для генерації різних тестових сигналів:\n\nsignal_random_walk = nk.complexity_simulate(duration=30, \n                                            sampling_rate=100, \n                                            method=\"randomwalk\") # симуляція випадкового блукання\n\n\nnk.complexity_attractor(embedded=nk.complexity_embedding(signal_random_walk, dimension=2, delay=100), \n                        alpha=1, \n                        color=\"orange\"); \n\n\n\n\n\n\n\nРис. 2.6: Двовимірний фазовий портрет випадкового блукання\n\n\n\n\n\n\nnk.complexity_attractor(nk.complexity_embedding(signal_random_walk, dimension=3, delay=100), \n                        alpha=1, \n                        color=\"orange\");\n\n\n\n\n\n\n\nРис. 2.7: Тривимірний фазовий портрет випадкового блукання\n\n\n\n\n\n\nsignal_ornstein = nk.complexity_simulate(duration=30, \n                                        sampling_rate=100, \n                                        method=\"ornstein\") # симуляція системи Орнштайна\n\n\nnk.complexity_attractor(nk.complexity_embedding(signal_ornstein, dimension=2, delay=100), \n                        alpha=1, \n                        color=\"red\"); \n\n\n\n\n\n\n\nРис. 2.8: Двовимірний фазовий портрет системи Орнштайна\n\n\n\n\n\n\nnk.complexity_attractor(nk.complexity_embedding(signal_ornstein, dimension=3, delay=100), \n                        alpha=1, \n                        color=\"red\"); \n\n\n\n\n\n\n\nРис. 2.9: Двовимірний фазовий портрет системи Орнштайна\n\n\n\n\n\n\nnk.complexity_attractor(color = \"last_dim\", alpha=\"time\", duration=1);\n\n\n\n\n\n\n\nРис. 2.10: Тривимірний фазовий портрет атрактора Лоренца\n\n\n\n\n\n\nnk.complexity_attractor(\"rossler\", color = \"blue\", alpha=1, sampling_rate=5000);\n\n\n\n\n\n\n\nРис. 2.11: Тривимірний фазовий портрет атрактора Рьосслера\n\n\n\n\n\n\nnk.complexity_attractor(nk.complexity_embedding(for_rec, dimension=2, delay=100), \n                        alpha=1, \n                        color=\"maroon\"); \n\n\n\n\n\n\n\nРис. 2.12: Двовимірний фазовий портрет вихідних значень досліджуваного ряду Біткоїна\n\n\n\n\n\n\nnk.complexity_attractor(nk.complexity_embedding(for_rec, dimension=3, delay=100), \n                        alpha=1, \n                        color=\"maroon\"); \n\n\n\n\n\n\n\nРис. 2.13: Тривимірний фазовий портрет вихідних значень досліджуваного ряду Біткоїна\n\n\n\n\n\n\n\n\n\n\n\nАвтоматизований підбір параметрів\n\n\n\nУ зазначених вище прикладах ми обирали параметри \\(m\\) і \\(\\tau\\) згідно нашими власними міркуваннями. Але на практиці бажано було б, щоб зазначені параметри обирались автоматично, спираючись на конкретну статистичну процедуру. Бібліотека neurokit2 представляє функціонал для автоматичного підбору параметрів розмірності та часової затримки. У завданнях самостійної роботи буде надано короткий опис алгоритмів для автоматичного підбору розмірності атрактору та часової затримки. В основній частині лабораторної роботи для побудови рекурентних діаграм будуть використанні самостійно підібрані значення \\(m\\) і \\(\\tau\\)\n\n\n\n\n2.2.2 Побудова рекурентної матриці\nЯк вже зазначалося, рекурентний аналіз на основі реконструйованих траєкторій фазового простору визначає кількість і тривалість рекурентних станів динамічної системи.\nМи маємо змогу побудувати рекурентну матрицю, використовуючи метод recurrence_matrix().\nЙого синтаксис виглядає наступним чином:\nrecurrence_matrix(signal, delay=1, dimension=3, tolerance='default', show=False)\nПараметри:\n\nsignal (Union[list, np.ndarray, pd.Series]) — сигнал у вигляді вектора значень;\ndelay (int) — затримка в часі;\ndimension (int) — розмірність вкладень, \\(m\\);\ntolerance (float) — радіус \\(\\varepsilon\\) багатовимірного околу, в межах якого шукаються рекурентні траєкторії, а дві точки даних вважаються схожими. Якщо \"sd\" (за замовчуванням), буде встановлено значення \\(0.2 \\cdot SD_{signal}\\). Емпіричним правилом є встановлення \\(\\varepsilon\\) таким чином, щоб відсоток точок, класифікованих як рекурентні, становив приблизно 2-5%;\nshow (bool) — візуалізувати рекурентну матрицю.\n\nПовертає:\n\nnp.ndarray — рекурентну матрицю;\nnp.ndarray — матрицю відстаней.\n\nПобудуємо рекурентну матрицю для фрагменту вихідних значень Біткоїна, його прибутковостей та стандартизованого фоагменту його вихідного ряду. Розмірність \\(m=4\\), часова затримка \\(\\tau=1\\). Спробуємо різні варіанти \\(\\varepsilon\\):\n\nдля вихідного ряду \\(\\varepsilon=100\\);\nдля прибутковостей та стандартизованого ряду \\(\\varepsilon=0.8\\).\n\n\n\n\n\n\n\nПримітка для побудови матриці рекурентності\n\n\n\nЯк уже зазначалось на початку, рекурентна діаграма — це двовимірна квадратна двійкова матриця розміром \\(N^2\\). Одним із її основних недоліків є те, що вона погано масштабується на великих даних. Наприклад, якщо ви плануєте досліджувати часовий ряд довжиною 1000000 значень, тоді рекурентна матриця буде складатись із 1000000000000 білих і чорних точок, що може бути викликом для вашого процесора та оперативної пам’яті. Тому в подальшому ми пропонуємо будувати рекурентну матрицю не для всього ряду, а тільки для його підмножини. Для цього ми визначимо змінні початкового (idx_beg) та кінцевого (inx_end) відліку в межах якого буде здійснюватись побудова рекурентної матриці\n\n\n\nidx_beg = 1000 # кінцевий відлік\nidx_end = 1500 # початковий відлік\n\nfragm = signal[idx_beg:idx_end]\n\n# виконуємо приведення вихідного сигналу до прибутковостей\nret_type = 4 \nret = transformation(fragm, ret_type)\n\n# приводимо вихідний ряд до стандартизованого виду\nret_type = 6 \nfor_rec = transformation(fragm, ret_type)\n\n\nrc, _ = nk.recurrence_matrix(fragm, \n                            delay=1, \n                            dimension=4, \n                            tolerance=100,\n                            show=True)\n\n\n\n\n\n\n\nРис. 2.14: Рекурентна матриця для вихідних значень Біткоїна\n\n\n\n\n\nЯк можна бачити з рисунку 2.14 всі траєкторії для простору вихідних значень за абсолютною шкалою залишаються доволі віддаленими один від одного. Для розпізнавання рекурентних закономірностей нам потребується поступово нарощувати \\(\\varepsilon\\). З цього рисунку видно, що \\(\\varepsilon=100\\) буде замало.\nТепер спробуємо трохи нормалізувати значення вихідного фрагменту Біткоїна, аби реконструйовані траєкторії не знаходились занадто віддаленими один від одного. Для цього розрахуємо стандартизовані прибутковості:\n\n# будуємо рекурентну матрицю\nrc, _ = nk.recurrence_matrix(ret, \n                            delay=1, \n                            dimension=4,\n                            tolerance=0.8,\n                            show=True)\n\n\n\n\n\n\n\nРис. 2.15: Рекурентна матриця для стандартизованих прибутковостей Біткоїна\n\n\n\n\n\nТепер можемо бачити, що Біткоїн став характризуватися чорними смугами, що відображають динаміку певних детермінованих процесів. У той же час білі смуги характеризують періоди абсолютно аномальної (непередбачуваної) поведінки на даному ринку. Видно, що прибутковості залишаються доволі некорельованими, про що і свідчить переважне домінування саме білих областей.\nСпробуємо тепер подивитись на стандартизований вихідний ряд:\n\n# будуємо рекурентну матрицю\nrc, _ = nk.recurrence_matrix(for_rec, \n                            delay=1, \n                            dimension=4,\n                            tolerance=0.8,\n                            show=True)\n\n\n\n\n\n\n\nРис. 2.16: Рекурентна матриця для стандартизованого вихідного ряду Біткоїна\n\n\n\n\n\nНа початку свого існування Біткоїн характеризувався доволі високим ступенем передбачуваності, низькою волатильністю коливань. Надалі почали домінувати білі області, а зараз Біткоїну властива динаміка схожа з броунівським рухом.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Лабораторна робота № 2</span>"
    ]
  },
  {
    "objectID": "lab_2.html#завдання-для-самостійної-роботи",
    "href": "lab_2.html#завдання-для-самостійної-роботи",
    "title": "2  Лабораторна робота № 2",
    "section": "2.3 Завдання для самостійної роботи",
    "text": "2.3 Завдання для самостійної роботи\n\nОтримати індекс часового ряду у викладача\nПровести дослідження його рекурентних властивостей згідно інструкції\nПорівняти фазові портрети і рекурентні діаграми для стандартизованого вихідного ряду та прибутковостей. Що спільного між ними і чим вони відрізняються?\nПровести побудову фазових портретів і рекурентних діаграм для вашого часового ряду із використанням процедур автоматичного підбору параметрів розмірності вкладень і часової затримки. Порівняти результати з тими, що були отримані без використання даних методів і зробити висновки\n\n\n\n\n\n\n\nДля виконання 4-го завдання самостійної роботи\n\n\n\nДалі надається опис алгоритмів для автоматизованого підбору розмірності реконструйованого фазового простору та часової затримки\n\n\n\n2.3.1 Автоматизований підбір параметра часової затримки, \\(\\tau\\)\nЧасова затримка \\(\\tau\\) (також відома як L) є одним з двох критичних параметрів, що беруть участь у процедурі реконструкції фазового простору. Значення \\(L\\) відповідає затримці у відліках між вихідним сигналом і його затриманою версією (версіями). Іншими словами, скільки відліків ми розглядаємо між певним станом сигналу та його найближчим минулим станом.\nЯкщо \\(\\tau\\) менше оптимального теоретичного значення, послідовні координати стану системи корельовані і атрактор недостатньо розгорнутий. І навпаки, коли \\(\\tau\\) більше, ніж повинно бути, послідовні координати майже незалежні, що призводить до некорельованої та неструктурованої хмари точок.\nВибір параметрів затримки та розмірності представляє нетривіальну задачу. Один з підходів полягає у їх (напів)незалежному виборі (оскільки вибір розмірності часто вимагає затримки) за допомогою функцій complexity_delay() та complexity_dimension(). Однак, існують методи спільного оцінювання, які намагаються знайти оптимальну затримку та розмірність одночасно.\nЗауважте також, що деякі автори (наприклад, Розенштейн, 1994) пропонують спочатку визначити оптимальну розмірність вбудовування, а потім розглядати оптимальне значення затримки як оптимальну затримку між першою та останньою координатами затримки (іншими словами, фактична затримка має дорівнювати оптимальній затримці, поділеній на оптимальну розмірність вбудовування мінус 1).\nДекілька авторів запропонували різні методи для вибору затримки:\n\nФрейзер і Свінні (1986)  [8] пропонують використовувати перший локальний мінімум взаємної інформації між затриманим і незатриманим часовими рядами, ефективно визначаючи значення \\(\\tau\\), для якого вони діляться найменшою інформацією (і де атрактор є найменш надлишковим). На відміну від автокореляції, взаємна інформація враховує також нелінійні кореляції;\nТейлер (1990)  [9] запропонував вибирати таке значення \\(\\tau\\), при якому автокореляція між сигналом та його зміщенною версією при \\(\\tau\\) вперше перетинає значення \\(1/e\\). Методи, що базуються на автокореляції, мають перевагу за часом обчислень, коли вони знаходяться за допомогою алгоритму швидкого перетворення Фур’є (fast Fourier transform, FFT);\nКасдаглі (1991)  [10] пропонує замість цього брати перший нульовий перетин автокореляції;\nРозенштейн (1993, 1994)  [11,12] вважає, що слід апроксимувати точку, де функція автокореляцій падає до \\(\\left( 1-1/e \\right)\\) від свого максимального значення. Або ж наближатися до точки, близької до 40% нахилу середнього зміщення від діагоналі;\nКім (1999)  [13] оцінює \\(\\tau\\) за допомогою кореляційного інтегралу, який називається C-C методом, і який, як виявилося, узгоджується з результатами, отриманими за допомогою методу взаємної інформації. Цей метод використовує статистику в реконструйованому фазовому просторі, а не аналізує часову еволюцію ряду. Однак час обчислень є значно довшим через необхідність порівнювати кожну унікальну пару парних векторів у реконструйованому сигналі на кожну затримку;\nЛайл (2021)  [14] описує “Реконструкцію симетричного проекційного атрактора” (Symmetric Projection Attractor Reconstruction, SPAR), де \\(1/3\\) від домінуючої частоти (тобто довжини середнього “циклу”) може бути підходящим значенням для приблизно періодичних даних, і робить атрактор чутливим до морфологічних змін. Див. також доповідь Астона. Цей метод також є найшвидшим, але може не підходити для аперіодичних сигналів. Аргумент algorithm (за замовчуванням \"fft\") передається до аргументу method методу signal_psd().\n\nМожна також відмітити метод для об’єднаного підбору параметрів затримки та розмірності.\n\nГаутама (2003)  [15] зазначає, що на практиці часто використовують фіксовану часову затримку і відповідно регулюють розмірність вбудовування. Оскільки це може призвести до великих значень \\(m\\) (а отже, до вкладених даних великого розміру) і, відповідно, до повільної обробки, використовується метод оптимізації для спільного визначення \\(m\\) і \\(\\tau\\) на основі показника entropy ratio.\n\nРозглянемо оптимальні значення розмірності та затримки для часового сигналу Біткоїна:\n\n# виконуємо приведення вихідного сигналу до прибутковостей\nret_type = 4 \nret = transformation(signal, ret_type)\n\n# приводимо вихідний ряд до стандартизованого виду\nret_type = 6 \nfor_rec = transformation(signal, ret_type)\n\n\ndelay, parameters = nk.complexity_delay(for_rec, \n                                        delay_max=300, show=True,\n                                        method=\"fraser1986\")\n\n\n\n\n\n\n\nРис. 2.17: Оптимальне значення часової затримки на основі методу Фрейзера і Свінні для часового ряду Біткоїна\n\n\n\n\n\nРис. 2.17 показує, що перший локальний мінімум взаємної інформації для стандартизованих вихідних значень Біткоїна знаходиться на 273 лагу. Для візуального огляду реконструйованого атрактора це значення, можливо, є найбільш адекватним. Але використовуючи настільки велику часову затримку, ми втрачаємо доволі багато проміжних значень, що також можуть містити досить важливу приховану інформацію для кількісних розрахунків.\n\ndelay, parameters = nk.complexity_delay(for_rec, \n                                        delay_max=300, show=True,\n                                        method=\"theiler1990\")\n\n\n\n\n\n\n\nРис. 2.18: Оптимальне значення часової затримки на основі методу Тейлера для часового ряду Біткоїна\n\n\n\n\n\nРис. 2.18 демонструє, що автокореляція між стандартизованих вихідним сигналом Біткоїна та його зміщенною версією при \\(\\tau=195\\) вперше перетинає значення \\(1/\\exp\\). Бачимо, що дане значення затримки є трохи меншим за те, що було отримано до цього, але суті це не змінює. Також бачимо, що між реконструйованими атракторами для \\(\\tau=195\\) та \\(\\tau=273\\) немає кардинальної візуальної різниці.\n\ndelay, parameters = nk.complexity_delay(for_rec, \n                                        delay_max=500, show=True,\n                                        method=\"casdagli1991\")\n\ndelay\n\nЯк можна бачити по прикладу вище, не всі методи надають адекватну оцінку розмірності нашого сигналу. Спробуємо привести вихідні значення Біткоїна до прибутковостей та повторити процедуру Касдаглі ще раз.\n\nret_type = 4 \nret = transformation(signal, ret_type)\n\n\ndelay, parameters = nk.complexity_delay(ret, \n                                        delay_max=300, show=True,\n                                        method=\"casdagli1991\")\n\n\n\n\n\n\n\nРис. 2.19: Оптимальне значення часової затримки на основі методу Касдаглі для прибутковостей Біткоїна\n\n\n\n\n\nЦього разу нам вдалося досягти оптимального результату, але приклад вище демонструє, що кожна процедура має свої виключення. Рис. 2.19 показує, що значення прибутковостей Біткоїна характеризуються певними кореляціями лише на перших 4-ох лагах. Подальші часові зміщення роблять значення прибутковостей незалежними один від одного.\n\ndelay, parameters = nk.complexity_delay(for_rec, \n                                        delay_max=300, show=True,\n                                        method=\"rosenstein1993\")\n\n\n\n\n\n\n\nРис. 2.20: Оптимальне значення часової затримки на основі методу Розенштайна (1993) для часового ряду Біткоїна\n\n\n\n\n\nРис. 2.20 демонструє, що при \\(\\tau=101\\) функція автокореляцій перетинає значення \\(\\left( 1-1/e \\right)\\). При цьому видно, що навіть для такого лагу зберігається значна частка кореляцій між стандартизованими вихідними значеннями Біткоїна.\n\ndelay, parameters = nk.complexity_delay(for_rec, \n                                        delay_max=300, show=True,\n                                        method=\"rosenstein1994\")\n\n\n\n\n\n\n\nРис. 2.21: Оптимальне значення часової затримки на основі методу Розенштайна (1994) для часового ряду Біткоїна\n\n\n\n\n\nРисунок вище показує, що при \\(\\tau=120\\) зміщення реконструйованих траєкторій від їх оригінального положення на лінії ідентичності зберігає найбільшу кількість інформації стосовно атрактора стандартизованих значень Біткоїна.\n\ndelay, parameters = nk.complexity_delay(for_rec, \n                                        delay_max=300, show=True,\n                                        method=\"lyle2021\")\n\n\n\n\n\n\n\nРис. 2.22: Оптимальне значення часової затримки на основі методу Лайла для часового ряду Біткоїна\n\n\n\n\n\nЗгідно представленого вище результату найбільш значущі частоти, отримані за допомогою перетворення Фур’є, зберігаються при \\(\\tau=109\\).\nТепер подивимось як це виглядатиме для об’єднаного підбору параметрів:\n\ndelay, parameters = nk.complexity_delay(for_rec,\n    delay_max=np.arange(1, 10, 1), # діапазон значень затримки\n    dimension_max=10,              # максимальна розмірність вкладень\n    method=\"gautama2003\",\n    surrogate_n=5,                 # Кількість сурогатних сигналів \n                                   # для генерації\n    surrogate_method=\"random\",     # Спосіб генерації сигналів\n    show=True)\n \n\n\n\n\n\n\n\nРис. 2.23: Оптимальне значення розмірності та затримки на основі методу Гаутами для часового ряду Біткоїна\n\n\n\n\n\n\ndimension = parameters[\"Dimension\"]\ndimension\n\n10\n\n\nОскільки представлена вище процедура є доволі громіздкою в плані обчислювальних потужностей, ми обрали діапазон \\(\\tau\\) в межах від 1 до 10. Видно, що при \\(\\tau\\) близької до 3 оптимальне значення розмірності атрактора дорівнює 10. Можливо, при значеннях \\(\\tau\\) близьких до 100 або 200, ми могли б отримати зовсім інше значення розмірності, але це потребує додаткових експериментів.\n\n\n2.3.2 Автоматизований підбір параметра розмірності вкладень, \\(m\\)\nЗа дану процедуру відповідає метод complexity dimension(). Її синтаксис виглядає наступним чином:\ncomplexity_dimension(signal, delay=1, dimension_max=20, method='afnn', show=False, **kwargs)\nХоча зазвичай використовують \\(m=2\\) або \\(m=3\\), але різні автори пропонують наступні процедури підбору:\n\nкореляційна розмірність (Correlation Dimension, CD): Одним з перших методів оцінки оптимального \\(m\\) був розрахунок кореляційної розмірності для вкладень різного розміру і пошук насичення (тобто плато) в її значенні при збільшенні розміру векторів  [16–18]. Одне з обмежень полягає в тому, що насичення буде також мати місце, коли даних недостатньо для адекватного заповнення простору високої розмірності (зауважте, що в загальному випадку не рекомендується мати настільки великі вкладення, оскільки це значно скорочує довжину сигналу);\nнайближчі хибні сусіди (False Nearest Neighbour, FNN): Метод, запропонований Кеннелом та ін.  [19–21], базується на припущенні, що дві точки, які є близькими одна до одної в достатній розмірності вбудовування, повинні залишатися близькими при збільшенні розмірності. Алгоритм перевіряє сусідів при збільшенні розмірності вкладень, поки не знайде лише незначну кількість хибних сусідів при переході від розмірності \\(m\\) до \\(m+1\\). Це відповідає найнижчій розмірності вкладення, яка, як передбачається, дає розгорнуту реконструкцію просторово-часового стану. Цей метод може не спрацювати в зашумлених сигналах через марну спробу розгорнути шум (а в чисто випадкових сигналах кількість хибних сусідів суттєво не зменшується зі збільшенням \\(m\\)). На рисунку нижче (Рис. 2.24) показано, як проекції на простори більшої розмірності можна використовувати для виявлення хибних найближчих сусідів. Наприклад, червона та жовта точки є сусідами в одновимірному просторі, але не в двовимірному;\n\n\n\n\n\n\n\nРис. 2.24: Основна ідея методу FNN. Найближчі сусіди зеленої точки з’являються у випадку 1-, 2- та 3-вимірних фазових просторів  [22]\n\n\n\n\nсередні хибні сусіди (Average False Neighbors, AFN): Ця модифікація методу FNN розроблена Сао (1997)  [23] і усуває один з його основних недоліків — необхідність евристичного вибору порогових значень \\(r\\). Метод використовує максимальну евклідову відстань для представлення найближчих сусідів і усереднює всі відношення відстані в \\(m+1\\) розмірності до розмірності \\(m\\) та визначає E1 та E2 як параметри. Оптимальна розмірність досягається тоді, коли E1 перестає змінюватися (виходить на плато). Це відбувається при розмірності d0, якщо сигнал надходить від атрактора. Тоді d0+1* є оптимальною мінімальною розмірністю вкладення. E2 є корисною величиною для того, щоб відрізнити детерміновані сигнали від стохастичних. Константа E2, що близька до 1 для будь-якої розмірності вкладень \\(d\\), вказує на випадковість даних, оскільки майбутні значення не залежать від минулих.\n\nПараметри:\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень;\ndelay (int) — часова затримка у відліках. Для вибору оптимального значення цього параметра ми ще скористаємось методом complexity_delay();\ndimension_max (int) — максимальний розмір вкладення для тестування;\nmethod (str) — може бути \"afn\" (середні хибні сусіди), \"fnn\" (найближчий хибний сусід) або \"cd\" (кореляційна розмірність);\nshow (bool) — візуалізувати результат;\nkwargs — інші аргументи, такі як \\(R=10.0\\) або \\(A=2.0\\) (відносне та абсолютне граничне значення, тільки для методу \"fnn\").\n\nПовертає:\n\ndimension (int) — оптимальна розмірність вкладень;\nparameters (dict) — словник python, що містить додаткову інформацію про параметри, які використовуються для обчислення оптимальної розмірності.\n\nСпробуємо отримати оптимальне значення розмірності згідно зазначених процедур. В якості часової затримки можна взяти \\(\\tau=100\\). Приблизно таке значення спостерігалося для кожної процедури.\n\noptimal_dimension, info = nk.complexity_dimension(for_rec,\n                                                  delay=100,\n                                                  dimension_max=10,\n                                                  method='cd',\n                                                  show=True)\n\n\n\n\n\n\n\nРис. 2.25: Оптимальне значення розмірності на основі кореляційної розмірності для часового ряду Біткоїна\n\n\n\n\n\nРис. 2.25 демонструє той факт, що оптимальна розмірність вкладень, за якої досягається найбільш інформативна репрезентація фазового простору, дорівнює 7.\n\noptimal_dimension, info = nk.complexity_dimension(for_rec,\n                                                  delay=100,\n                                                  dimension_max=10,\n                                                  method='fnn',\n                                                  show=True)\n\n\n\n\n\n\n\nРис. 2.26: Оптимальне значення розмірності на основі найближчих хибних сусідів для часового ряду Біткоїна\n\n\n\n\n\nЗ рисунку 2.26 видно, що мінімальна розмірність вкладення дорівнює 3. Саме при переході від 3-ох вимірного фазового простору до 4-ох вимірного ми бачимо, що кількість хибних сусідів стає мінімальною і далі не зростає.\n\noptimal_dimension, info = nk.complexity_dimension(for_rec,\n                                                  delay=20,\n                                                  dimension_max=20,\n                                                  method='afnn',\n                                                  show=True)\n\n\n\n\n\n\n\nРис. 2.27: Оптимальне значення розмірності на основі середніх найближчих хибних сусідів для часового ряду Біткоїна\n\n\n\n\n\nАлгоритм середніх хибних сусідів показує, що тут розмірність вкладень \\(m=5\\) є оптимальною. При подальшому зростанні розмірності, атрактор стає більш стохастичним, що вказує на втрату всіх кореляцій.\nЗгідно з представленими вище алгоритмами автоматичного підбору, розмірність вкладень можна обирати в діапазоні значень від 3 до 7. Тепер на основі отриманих результатів приступимо до побудови рекурентної діаграми.\n\n\n\n\n[1] N. H. Packard, J. P. Crutchfield, J. D. Farmer, and R. S. Shaw, Geometry from a Time Series, Phys. Rev. Lett. 45, 712 (1980).\n\n\n[2] F. Takens, Detecting Strange Attractors in Turbulence, in Dynamical Systems and Turbulence, Warwick 1980, edited by D. Rand and L.-S. Young (Springer Berlin Heidelberg, Berlin, Heidelberg, 1981), pp. 366–381.\n\n\n[3] J.-P. Eckmann, S. O. Kamphorst, and D. Ruelle, Recurrence Plots of Dynamical Systems, Europhysics Letters 4, 973 (1987).\n\n\n[4] V. N. Soloviev and A. Belinskyi, Methods of Nonlinear Dynamics and the Construction of Cryptocurrency Crisis Phenomena Precursors, in Proceedings of the 14th International Conference on ICT in Education, Research and Industrial Applications. Integration, Harmonization and Knowledge Transfer. Volume II: Workshops, Kyiv, Ukraine, May 14-17, 2018, edited by V. Ermolayev, M. C. Suárez-Figueroa, V. Yakovyna, V. S. Kharchenko, V. Kobets, H. Kravtsov, V. S. Peschanenko, Y. Prytula, M. S. Nikitchenko, and A. Spivakovsky, Vol. 2104 (CEUR-WS.org, 2018), pp. 116–127.\n\n\n[5] V. N. Soloviev and A. Belinskiy, Complex Systems Theory and Crashes of Cryptocurrency Market, in Information and Communication Technologies in Education, Research, and Industrial Applications, edited by V. Ermolayev, M. C. Suárez-Figueroa, V. Yakovyna, H. C. Mayr, M. Nikitchenko, and A. Spivakovsky (Springer International Publishing, Cham, 2019), pp. 276–297.\n\n\n[6] K. Shockley and M. Riley, In Recurrence Quantification Analysis: Theory and Best Practices, 1st ed. (Springer, New York, 2015).\n\n\n[7] T. Rawald, Scalable and Efficient Analysis of Large High-Dimensional Data Sets in the Context of Recurrence Analysis, PhD thesis, Humboldt-Universität zu Berlin, Mathematisch-Naturwissenschaftliche Fakultät, 2018.\n\n\n[8] A. M. Fraser and H. L. Swinney, Independent Coordinates for Strange Attractors from Mutual Information, Phys. Rev. A 33, 1134 (1986).\n\n\n[9] J. Theiler, Statistical Precision of Dimension Estimators, Phys. Rev. A 41, 3038 (1990).\n\n\n[10] M. Casdagli, S. Eubank, J. D. Farmer, and J. Gibson, State Space Reconstruction in the Presence of Noise, Physica D: Nonlinear Phenomena 51, 52 (1991).\n\n\n[11] M. T. Rosenstein, J. J. Collins, and C. J. De Luca, A Practical Method for Calculating Largest Lyapunov Exponents from Small Data Sets, Physica D: Nonlinear Phenomena 65, 117 (1993).\n\n\n[12] M. T. Rosenstein, J. J. Collins, and C. J. De Luca, Reconstruction Expansion as a Geometry-Based Framework for Choosing Proper Delay Times, Physica D: Nonlinear Phenomena 73, 82 (1994).\n\n\n[13] H. S. Kim, R. Eykholt, and J. D. Salas, Nonlinear Dynamics, Delay Times, and Embedding Windows, Physica D: Nonlinear Phenomena 127, 48 (1999).\n\n\n[14] J. V. Lyle, M. Nandi, and P. J. Aston, Symmetric Projection Attractor Reconstruction: Sex Differences in the ECG, Frontiers in Cardiovascular Medicine 8, (2021).\n\n\n[15] T. Gautama, D. Mandic, and M. Van Hulle, A Differential Entropy Based Method for Determining the Optimal Embedding Parameters of a Signal, Proceedings 6, 29 (2003).\n\n\n[16] P. Grassberger and I. Procaccia, Measuring the Strangeness of Strange Attractors, Physica D: Nonlinear Phenomena 9, 189 (1983).\n\n\n[17] P. Grassberger and I. Procaccia, Characterization of Strange Attractors, Phys. Rev. Lett. 50, 346 (1983).\n\n\n[18] P. Grassberger, Generalized Dimensions of Strange Attractors, Physics Letters A 97, 227 (1983).\n\n\n[19] M. B. Kennel, R. Brown, and H. D. I. Abarbanel, Determining Embedding Dimension for Phase-Space Reconstruction Using a Geometrical Construction, Phys. Rev. A 45, 3403 (1992).\n\n\n[20] A. Krakovská, K. Mezeiová, and H. Budáčová, Use of False Nearest Neighbours for Selecting Variables and Embedding Parameters for State Space Reconstruction, Journal of Complex Systems 2015, (2015).\n\n\n[21] C. Rhodes and M. Morari, The False Nearest Neighbors Algorithm: An Overview, Computers & Chemical Engineering 21, S1149 (1997).\n\n\n[22] S. G. Stavrinides et al., On the Chaotic Nature of Random Telegraph Noise in Unipolar RRAM Memristor Devices, Chaos, Solitons & Fractals 160, 112224 (2022).\n\n\n[23] L. Cao, Practical Method for Determining the Minimum Embedding Dimension of a Scalar Time Series, Physica D: Nonlinear Phenomena 110, 43 (1997).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Лабораторна робота № 2</span>"
    ]
  },
  {
    "objectID": "lab_3.html",
    "href": "lab_3.html",
    "title": "3  Лабораторна робота № 3",
    "section": "",
    "text": "3.1 Теоретичні відомості\nДля якісного опису системи графічне представлення системи підходить якнайкраще. Однак головним недоліком графічного представлення є те, що воно змушує користувачів суб’єктивно інтуїтивно інтерпретувати закономірності та структури, представлені на рекурентній діаграмі.\nКрім того, зі збільшенням розміру даних, проблематичним представляється аналіз усіх \\(N^2\\) значень. Як наслідок, доводиться працювати з окремими ділянками вихідних даних. Аналіз у такий спосіб може створювати нові дефекти, які спотворюють об’єктивність спостережуваних закономірностей і призводять до неправильних інтерпретацій. Щоб подолати це обмеження і поширити об’єктивну оцінку серед дослідників, на початку 1990-х років Веббером та Збілутом  [1,2] були введені визначення та процедури для кількісної оцінки складності рекурентних діаграм, а згодом вони були розширені Марваном та ін  [3].\nДрібномасштабні кластери можуть являти собою комбінацію ізольованих точок (випадкових рекурентностей). Подібна еволюція в різні періоди часу або в зворотному часовому порядку представлятиме діагональні лінії (детерміновані структури), а також вертикальні/горизонтальні лінії для позначення ламінарних станів (переривчастість) або станів, що предсталяють сингулярності. Для кількісного опису системи системи такі дрібномасштабні кластери слугують основою кількісного рекурентного аналізу (recurrence quantification analysis, RQA)  [4].",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Лабораторна робота № 3</span>"
    ]
  },
  {
    "objectID": "lab_3.html#хід-роботи",
    "href": "lab_3.html#хід-роботи",
    "title": "3  Лабораторна робота № 3",
    "section": "3.2 Хід роботи",
    "text": "3.2 Хід роботи\nПерш ніж переходити до опису кожної з мір та їх розрахунків, визначимось з інструментарієм для виконання RQA. Як і до цього, ми використаємо бібліотеку neuralkit2.\nІмпортуємо бібліотеки для подальшої роботи:\n\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport neurokit2 as nk\nimport yfinance as yf\nimport scienceplots\nimport pandas as pd\nfrom tqdm import tqdm\n\n%matplotlib inline\n\nІ виконаємо налаштування рисунків для виводу:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nsize = 16\nparams = {\n    'figure.figsize': (8, 6),            # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': size,                   # розмір фонтів рисунку\n    'lines.linewidth': 2,                # товщина ліній\n    'axes.titlesize': 'small',           # розмір титулки над рисунком\n    'axes.labelsize': size,              # розмір підписів по осям\n    'legend.fontsize': size,             # розмір легенди\n    'xtick.labelsize': size,             # розмір розмітки по осі Ох\n    'ytick.labelsize': size,             # розмір розмітки по осі Ох\n    \"font.family\": \"Serif\",              # сімейство стилів підписів \n    \"font.serif\": [\"Times New Roman\"],   # стиль підпису\n    'savefig.dpi': 300,                  # якість збережених зображень\n    'axes.grid': False                   # побудова сітки на самому рисунку\n}\n\nplt.rcParams.update(params)              # оновлення стилю згідно налаштувань\n\nРозглянемо можливість використання всіх згаданих показників у якості індикаторів або індикаторів-передвісників кризових явищ. Для прикладу завантажимо часовий ряд фондового індексу Доу-Джонса за період з 1 грудня 1993 по 1 грудня 2022:\n\nsymbol = '^DJI'          # Символ індексу\nstart = \"1993-01-01\"     # Дата початку зчитування даних\nend = \"2022-01-01\"       # Дата закінчення зчитування даних\n\ndata = yf.download(symbol, start, end)  # вивантажуємо дані\ntime_ser = data['Adj Close'].copy()     # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'    # підпис по вісі Ох \nylabel = symbol          # підпис по вісі Оу\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nУвага\n\n\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance, а із власного файлу. Зрозуміло, що й аналіз результатів і висновки залежать від того, з яким рядом ми працюємо\n\n\n\n\nsymbol = 'sMpa11'                  # Символ індексу\n\npath = \"databases\\sMpa11.txt\"      # шлях по якому здійснюється зчитування файлу\ndata = pd.read_csv(path,           # зчитування даних \n                   names=[symbol])\ntime_ser = data[symbol].copy()     # копіюємо значення кривої \n                                   # \"напруга-видовження\" до окремої змінної\n\nxlabel = r'$\\varepsilon$'          # підпис по вісі Ох \nylabel = symbol                    # підпис по вісі Оу\n\n\nВиведемо досліджуваний ряд:\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\n\n\n\nРис. 3.1: Динаміка щоденних змін індексу Доу-Джонса\n\n\n\n\n\nКористуючись тими методами, що ми розглянули в попередній лабораторній роботі, побудуємо атрактор даного ряду та його рекурентну діаграму. Але, перш за все, стандартизуємо наш ряд. Для цього оголосимо функцію transformation(), що прийматиме на вхід часовий сигнал, тип ряду, і повертатиме його перетворення:\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\nДалі приводимо ряд до стандартизованого вигляду:\n\n\n\n\n\n\nПримітка для побудови матриці рекурентності\n\n\n\nЯк уже зазначалось на початку, рекурентна діаграма — це двовимірна квадратна двійкова матриця розміром \\(N^2\\). Тому в подальшому ми пропонуємо будувати рекурентну матрицю не для всього ряду, а тільки для його підмножини. Для цього ми визначимо змінні початкового (idx_beg) та кінцевого (inx_end) відліку в межах якого буде здійснюватись побудова рекурентної матриці\n\n\n\nsignal = time_ser.copy()\nret_type = 6    # вид ряду: 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nidx_beg = 3000    # кінцевий відлік\nidx_end = 7300 # початковий відлік\n\nfragm = signal[idx_beg:idx_end] # виокремлюємо фрагмент ряду\n\nall_series = transformation(signal, ret_type) # виконуємо перетворення всього ряду\n\nfor_rec = transformation(fragm, ret_type) # виконуємо перетворення фрагменту\n\nНа Рис. 3.2 представлено весь трансформований ряд та його фрагмент:\n\nfig, ax = plt.subplots(1, 2)\n\nax[0].plot(all_series)\nax[0].set_title(\"Весь ряд\")\nax[0].set_xlabel(xlabel)\nax[0].set_ylabel(ylabel)\n\nax[1].plot(for_rec)\nax[1].set_title(\"Фрагмент\")\nax[1].set_xlabel(xlabel)\n\nplt.show();\n\n\n\n\n\n\n\nРис. 3.2: Динаміка всього трансформованого ряду з використанням функції transformation() (рисунок зліва) та його фрагменту (рисунок справа)\n\n\n\n\n\nДля всього ряду і для віконної процедури визначимо наступні параметри:\n\nрозмірність вкладень \\(m=3\\);\nчасова затримка \\(\\tau=1\\);\nрадіус багатовимірного околу \\(\\varepsilon = 0.3\\).\n\nЗадамо необхідні параметри для обчислення та виводу:\n\nm = 3                         # розмірність вкладень\ntau = 1                       # часові затримка\neps = 0.3                     # радіус\n\nІ тепер подивимось на фазові траєкторії досліджуваної системи у дво- та тривимірному просторах:\n\nnk.complexity_attractor(nk.complexity_embedding(for_rec, dimension=2, delay=tau), \n                        alpha=1, \n                        color=\"red\"); \n\n\n\n\n\n\n\nРис. 3.3: Двовимірний фазовий портрет стандартизованих вихідних значень досліджуваного ряду Доу-Джонса\n\n\n\n\n\n\nnk.complexity_attractor(nk.complexity_embedding(for_rec, dimension=3, delay=tau), \n                        alpha=1, \n                        color=\"red\"); \n\n\n\n\n\n\n\nРис. 3.4: Тривимірний фазовий портрет стандартизованих вихідних значень досліджуваного ряду Доу-Джонса\n\n\n\n\n\nЯк можна бачити по візуальному огляду траєкторій у фазовому просторі важко робити висновки стосовно передбачуванності або хаотичності системи. Спробуємо ще раз, але тепер послуговуючись рекурентною діаграмою:\n\nrc, _ = nk.recurrence_matrix(for_rec, \n                            delay=1, \n                            dimension=m,\n                            tolerance=eps,\n                            show=True)\n\n\n\n\n\n\n\nРис. 3.5: Рекурентна матриця для стандартизованого вихідного ряду Доу-Джонса\n\n\n\n\n\nНа основі рекурентної діаграми в перспективі ми можемо отримати куди більше інформації стосовно еволюції системи. Видно, що 2008 рік характеризувався найвищим ступенем самоорганізації (рекурентності), про що свідчить доволі велика щільність чорних областей. У той же час можна бачити, що й останні роки характеризуються найменшим ступенем рекурентності. Можливо, прогнозованість подій у межах 2022 року варто було б охарактеризувати за допомогою інших індикаторів, але рекурентна матриця свідчить про те, що події минулих років мало корелюють з сьогоденням.\nМи вже зазначали, що якісна репрезентація рекурентності станів не є достатньо об’єктивною. Найращим варіантом у даному випадку буде використання рекурентного аналізу за алгоритмом рухомого вікна.\n\n3.2.1 Віконна процедура\nДля подальшої роботи створюємо віконну процедуру, в якій знов визначаємо вид ряду та ще декілька параметрів. Потім ми ініціалізуємо масиви для кожної рекурентної міри.\n\nret_type = 6            # вид ряду\nwindow = 250            # ширина вікна\ntstep = 1               # часовий крок вікна \nlength = len(time_ser)  # довжина самого ряду\n\nm = 1                   # розмірність вкладень\ntau = 1                 # часові затримка\neps = 0.3               # радіус\n\n# Ініціалізуємо масиви для збереження віконних значень \n# рекурентних мір\n\nRR = []                 # Частота повторення\nDET = []                # Детермінізм\nDIV = []                # Розбіжність\nAVG_DIAG_LINE = []      # Усереднена довжина діагональних ліній\nENT_DIAG = []           # Ентропія діагональних ліній\nLAM = []                # Ламінарність\nTT = []                 # Час затримки\nENT_VERT = []           # Ентропія вертикальних ліній\nENT_WHITE_VERT = []     # Ентропія білих вертикальних ліній\nAVG_WVERT_LINE = []     # Усереднена довжина білих вертикальних ліній\nVERT_DIV = []           # Розбіжність вертикальних ліній\nRATIO_DET_REC = []      # Відношення детермінізму до частоти повторень\nRATIO_LAM_DET = []      # Відношення ламінарності до детермінізму\nWHITE_VERT_DIV = []     # Розбіжність білих вертикальних ліній\nDIAG_RR = []            # Діагональна частота рекурентних значень\n\nДля подальших розрахунків ми використаємо метод complexity_rqa() бібліотеки neuralkit2. його синтаксис:\ncomplexity_rqa(signal, dimension=3, delay=1, tolerance='sd', min_linelength=2, method='python', show=False)\nПараметри:\n\nsignal (Union[list, np.ndarray, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень;\ndelay (int) — затримка в часі;\ndimension (int) — розмірність вкладень, \\(m\\);\ntolerance (float) — радіус \\(\\varepsilon\\) багатовимірного околу в межах якого шукаються рекурентні траєкторії (часто позначається як \\(r\\)), відстань, на якій дві точки даних вважаються схожими. Якщо \"sd\" (за замовчуванням), буде встановлено значення \\(0.2 \\cdot SD_{signal}\\), де \\(SD_{signal}\\) визначає стандартне відхилення ряду;\nmin_linelength (int) — мінімальна довжина діагональних та вертикальних ліній. За замовчування дорівнює 2;\nmethod (str) — Може бути \"pyrqa\" для виконання рекурентного аналізу, але із використанням бібліотеки PyRQA (потребує додаткового встановлення);\nshow (bool) — візуалізувати рекурентну матрицю.\n\nПовертає:\n\nrqa (DataFrame) — результати процедури RQA;\ninfo (dict) — словник, що містить інформацію відносно параметрів RQA.\n\nТепер можемо приступити до віконної процедури:\n\nfor i in tqdm(range(0,length-window,tstep)):  # фрагменти довжиною window  \n                                              # з кроком tstep\n\n    fragm = time_ser.iloc[i:i+window].copy()  # відбираємо фрагмент\n\n    fragm = transformation(fragm, ret_type)   # виконуємо процедуру \n                                              # трансформації ряду\n    \n    resultRQA, _ = nk.complexity_rqa(fragm,\n                                     delay=tau,\n                                     dimension=m,\n                                     tolerance=eps)\n    \n    # Обчислення відношення ламінарності до детермінізму\n    resultRQA['LamiDet'] = resultRQA['Laminarity']/resultRQA['Determinism']\n\n    # Обчислення дивергенції чорних вертикальних ліній\n    resultRQA['VDiv'] = 1./resultRQA['VMax']\n\n    # Обчислення дивергенції білих вертикальних ліній\n    resultRQA['WVDiv'] = 1./resultRQA['WMax']\n\n    RR.append(resultRQA['RecurrenceRate'])\n    DET.append(resultRQA['Determinism'])\n    DIV.append(resultRQA['Divergence']) \n    AVG_DIAG_LINE.append(resultRQA['L'])\n    ENT_DIAG.append(resultRQA['LEn'])\n    LAM.append(resultRQA['Laminarity']) \n    TT.append(resultRQA['TrappingTime']) \n    ENT_VERT.append(resultRQA['VEn'])\n    ENT_WHITE_VERT.append(resultRQA['WEn'])\n    AVG_WVERT_LINE.append(resultRQA['W']) \n    VERT_DIV.append(resultRQA['VDiv'])\n    WHITE_VERT_DIV.append(resultRQA['WVDiv'])\n    RATIO_DET_REC.append(resultRQA['DeteRec']) \n    RATIO_LAM_DET.append(resultRQA['LamiDet'])\n    DIAG_RR.append(resultRQA['DiagRec'])\n\n100%|██████████| 7054/7054 [01:57&lt;00:00, 60.00it/s]\n\n\nЗберігаємо отримані результати в текстових файлах:\n\nname = f\"RQA_classic_name={symbol}_window={window}_ \\\n    step={tstep}_rettype={ret_type}_m={m}_ \\\n    tau={tau}_eps={eps}.txt\"\n\nnp.savetxt(\"RR\" + name, RR)\nnp.savetxt(\"DIAG_RR\" + name, DIAG_RR)\nnp.savetxt(\"DET\" + name, DET)\nnp.savetxt(\"DIV\" + name, DIV)\nnp.savetxt(\"VERT_DIV\" + name, VERT_DIV)\nnp.savetxt(\"WHITE_VERT_DIV\" + name, WHITE_VERT_DIV)\nnp.savetxt(\"LAM\" + name, LAM)\nnp.savetxt(\"TT\" + name, TT)\nnp.savetxt(\"AVG_DIAG_LINE\" + name, AVG_DIAG_LINE)\nnp.savetxt(\"AVG_WRITE_VERT_LINE\" + name, AVG_WVERT_LINE)\nnp.savetxt(\"ENT_DIAG\" + name, ENT_DIAG)\nnp.savetxt(\"ENT_VERT\" + name, ENT_VERT)\nnp.savetxt(\"ENT_WHITE_VERT\" + name, ENT_WHITE_VERT)\nnp.savetxt(\"RATIO_DET_REC\" + name, RATIO_DET_REC)\nnp.savetxt(\"RATIO_LAM_DET\" + name, RATIO_LAM_DET)\n\n\n\n3.2.2 Рекурентні міри\nЗаймемося побудовою та інтерпретацією отриманих результатів. Для візуалізації графіків визначимо наступну функцію:\n\ndef plot_recurrence_measure(measure, label, clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(time_ser.index[window:length:tstep], \n                  time_ser.values[window:length:tstep], \n                  \"b-\", label=fr\"{ylabel}\")\n    p2, = ax2.plot(time_ser.index[window:length:tstep],\n                   measure, \n                   color=clr, \n                   label=fr'${label}$')\n\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(f\"{ylabel}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(label +\n        f\" RQA_classic_name={symbol}_window={window}_step={tstep}_ \\\n        rettype={ret_type}_m={m}_tau={tau}_eps={eps}.jpg\")\n        \n    plt.show();\n\n\n3.2.2.1 Частота рекурентності (Recurrence rate)\nНайпростішим показником є частота рекурентності, яка визначає щільність рекурентних точок на діаграмі, ігноруючи лінію ідентичності:\n\\[\nRR = \\frac{1}{N^2}\\displaystyle\\sum_{i,j=1}^{N}R(i,j).\n\\]\n\\(N\\) — кількість точок на траєкторії фазового простору.\nЧастота рекурентності відповідає ймовірності того, що певний стан повториться.\n\nplot_recurrence_measure(measure=RR, label='RR')\n\n\n\n\n\n\n\nРис. 3.6: Динаміка індексу Доу-Джонса та частоти рекурентності\n\n\n\n\n\nЯк ми можемо бачити з представленого рисунку (Рис. 3.6), міра рекурентності зростає при крахових подіях, що вказує на зростання ступеня самоорганізації та злагодженості торгівельної активності трейдерів на цьому ринку.\n\n\n3.2.2.2 Діагональна частота рекурентності (Diagonal recurrence rate)\nДаний підхід базується на діагональних рекурентних профілях часового ряду  [5]. Діагональний рекурентний профіль визначає кількість рекурентних точок на різних лагах подібно до функції автокореляцій. Для отримання діагонального профілю рекурентностей просто підраховується частка рекурентних точок на діагоналях, розташованих у нижньому правому або нижньому лівому куті діаграми, і будується графік як функція відстані від головної діагоналі, тобто лагу.\nІншими словами, діагональна частота рекурентності фіксує величину автокореляції на різних лагах.\n\nplot_recurrence_measure(measure=DIAG_RR, label='DRR')\n\n\n\n\n\n\n\nРис. 3.7: Динаміка індексу Доу-Джонса та діагональної частоти рекурентності\n\n\n\n\n\nЗ Рис. 3.7 видно, що діагональна частота рекурентності зростає у передкризові та кризові періоди, вказує на зростання величини автокореляції, що в свою чергу демонструє ріст ступеня самоорганізації.\n\n\n3.2.2.3 Детермінізм (Determinism)\nНаступним показником можна визначити частку рекурентних траєкторій, які формують діагональні лінії мінімальної довжини \\({\\displaystyle \\ell_{\\min}}\\). Ця міра називається детермінізмом і пов’язана з передбачуваністю динамічної системи:\n\\[\nDET= \\sum_{\\ell=\\ell_{\\min }}^{N}\\ell \\cdot P(\\ell) \\Bigg/ \\sum_{\\ell=1}^{N}\\ell \\cdot P(\\ell),\n\\]\nде \\(P(\\ell )\\) — частотний розподіл довжин \\(\\ell\\) діагональних ліній.\n\n\n\n\n\n\nДодаткова інформація по детермінізму\n\n\n\nДетерміновані системи характеризуються значною варіацією діагональних ліній різної довжини. Періодичні сигнали характеризуються довгими діагональними лініями, в той час як для хаотичних сигналів діагональні лінії будуть короткими. Для стохастичним систем діагональні лінії взагалі будуть відсутніми, за винятком випадкових закономріностей, що утворюватимуть дуже короткі діагональні лінії.\nБілий шум, наприклад, мав би рекурентну діаграму з майже ізольованими рекурентними точками та дуже малим відсотком діагональних ліній, тоді як детермінований процес демонстрував би дуже малу кількість поодиноких рекурентностей, але велику щільність довгих діагональних ліній.\n\n\n\nplot_recurrence_measure(measure=DET, label='DET')\n\n\n\n\n\n\n\nРис. 3.8: Динаміка індексу Доу-Джонса та міри \\(DET\\)\n\n\n\n\n\nЯк ми можемо бачити з Рис. 3.8, у передкризові та кризові періоди показник детермінізму починає зростати, що свідчить і про зростання ступеня передбачуваності (впорядкованості) флуктуацій системи.\n\n\n3.2.2.4 Ламінарність (Laminarity)\nПоказник, що характеризує кількість рекурентних станів, які утворюють вертикальні лінії, називається ламінарністю і пов’язаний з кількістю ламінарних фаз (незмінностей) у системі:\n\\[\nLAM = \\sum_{v=v_{\\min}}^{N}v \\cdot P(v) \\Bigg/ \\sum_{v=1}^{N}v \\cdot P(v),\n\\]\nа \\(P(v)\\) — частотний розподіл довжин \\(v\\) вертикальних ліній, які мають довжину принаймні \\(v_{\\min}\\).\n\n\n\n\n\n\nДодаткова інформація по ламінарності\n\n\n\nЛамінарність характеризує ймовірність системи перебувати незмінному стані. Зі збільшенням ізольованих рекурентних точок у системі міра ламінарності спадатиме\n\n\n\nplot_recurrence_measure(measure=LAM, label='LAM')\n\n\n\n\n\n\n\nРис. 3.9: Динаміка індексу Доу-Джонса та ламінарності\n\n\n\n\n\nМожна бачити, що в умовах криз ступінь ламінарності зростає. Зростає і щільність діагональних точок, і загалом кількість рекурентних траєкторій у фазовому просторі. Кризи характеризуються трендостійкістю, персистентністю та детермінованістю своєї поведінки.\n\n\n3.2.2.5 Середня довжина діагональних ліній (Average diagonal lines length)\nТакож можна виміряти середню довжину діагональних ліній. Cередня довжина діагональних лінії визначається як\n\\[\nL = \\sum_{{\\ell=\\ell_{\\min}}}^{N}\\ell \\cdot P(\\ell) \\Bigg/ \\sum_{{\\ell=\\ell_{\\min}}}^{N}P(\\ell).\n\\]\nЗагалом цей показник характеризує середній період часу при якому дві траєкторії фазового простору знаходяться в достатній близькості один до одного.\n\n\n\n\n\n\nДодаткова інформація по середній довжині діагональних ліній\n\n\n\nСередня довжина діагональних ліній визначає середній час при якому система залишається передбачуваною\n\n\n\nplot_recurrence_measure(measure=AVG_DIAG_LINE, label='AVG L')\n\n\n\n\n\n\n\nРис. 3.10: Динаміка індексу Доу-Джонса та середньої довжини діагональних ліній\n\n\n\n\n\nЯк і до цього, ми можемо бачити, що середній час перебування Доу-Джонса у детермінованому стані зростає під час кризових явищ, що свідчить про зростання ступеня колективізації трейдерів на ринку.\n\n\n3.2.2.6 Час захоплення/затримки (Trapping time)\nУсереднена довжина діагональної лінії пов’язана з часом передбачуваності динамічної системи та часом затримки:\n\\[\nTT = \\sum_{{v=v_{\\min}}}^{{N}}v \\cdot P(v) \\Bigg/ \\sum_{{v=v_{\\min}}}^{{N}}P(v).\n\\]\n\n\n\n\n\n\nДодаткова інформація по середній довжині вертикальних ліній\n\n\n\nСередня довжина вертикальних ліній визначає середній час перебування системи в ламінарному стані. Тобто, вона відповідає середньому періоду часу при якому система “завмирає” у певному стані. Очевидно, що зростання цiєї величини характеризує дедалi бiльший час затримки дослiджуваної системи в певному станi\n\n\n\nplot_recurrence_measure(measure=TT, label='TT')\n\n\n\n\n\n\n\nРис. 3.11: Динаміка індексу Доу-Джонса та час затримки\n\n\n\n\n\nНа представленому рисунку (Рис. 3.11) видно, що \\(TT\\) зростає в (перед-)кризові стани, що вказує на намагання системи перебувати ще деякий деякий час у стані кризи.\n\n\n3.2.2.7 Середня довжина білих вертикальних лінії (Average white vertical lines length)\nСередня довжина білих вертикальних ліній може бути визначена як\n\\[\nWVL_{mean} = \\sum_{w=w_{min}}^{N} w \\cdot P(w) \\Bigg/ \\sum_{w=w_{min}}^{N} P(w).\n\\]\n\\(P(w)\\) — це частотний розподіл білих вертикальних ліній довжиною \\(w\\), а \\(w_{min}\\) відповідає найменшій довжині білих вертикальних ліній (найменшому періоду повернення до стану рекурентності).\n\n\n\n\n\n\nДодаткова інформація по середній довжині білих вертикальних ліній\n\n\n\nПредставлену міру можна охарактеризувати як середній горизонт непередбачуваності системи\n\n\n\nplot_recurrence_measure(measure=AVG_WVERT_LINE, label='WVL_{mean}')\n\n\n\n\n\n\n\nРис. 3.12: Динаміка індексу Доу-Джонса та середньої довжини білих вертикальних ліній\n\n\n\n\n\nЗростання середньої довжини бiлих вертикальних лiнiй на Рис. 3.12 демонструє, що кризовi подiї характеризуються не лише детермiнiзмом динамiки фондового ринку, але й несхожiстю даних подiй у порiвняннi з попереднiми станами.\n\n\n3.2.2.8 Ентропія діагональних ліній (Diagonal lines entropy)\nДля відповідних діагональних сегментів можна розрахувати необхідну кількість інформації для опису всього розподілу цього типу ліній. Імовірність \\(p(\\ell )\\) того, що діагональна лінія має довжину \\(\\ell\\), можна оцінити за частотним розподілом \\(P(\\ell )\\) із \\(p( \\ell ) = P(\\ell) \\Big/ \\sum_{{\\ell=\\ell_{\\min}}}^{N}P(\\ell)\\). Ентропія Шеннона цієї ймовірності:\n\\[\nDLEn = -\\sum_{{\\ell =\\ell _{\\min }}}^{N}p(\\ell )\\ln p(\\ell ).\n\\]\nДаний показник відображає складність досліджуваної структури.\n\n\n\n\n\n\nДодаткова інформація по ентропії діагональних ліній\n\n\n\nДля некорельованого шуму чи осциляцiй ми тримали б мале значення ентропiї, що вказувало б на асиметричний розподіл діагональних ліній: існувала б невеличка частка діагональних ліній конкретної довжини, що характеризувала б рекурентність досліджуваної системи. Зростання даної ентропії свідчило б про зростання симетричності розподілу довжин діагональних ліній\n\n\n\nplot_recurrence_measure(measure=ENT_DIAG, label='DLEn')\n\n\n\n\n\n\n\nРис. 3.13: Динаміка індексу Доу-Джонса та ентропії діагональних ліній\n\n\n\n\n\nНа Рис. 3.13 видно, що ентропія діагональних ліній зростає під час кризових явищ, що вказує на зростання впливу детермінованих процесів із різним ступенем передбачуваності.\n\n\n3.2.2.9 Ентропія вертикальних ліній (Vertical lines entropy)\nМи можемо визначити Шеннонівську ентропію для розподілу вертикальних структур рекурентної діаграми. Імовірність \\(p( v )\\) того, що вертикальна лінія має довжину \\(v\\), можна оцінити за частотним розподілом \\(P(v)\\) із \\(p(v) = P(v) \\Big/ \\sum_{{v=v_{\\min}}}^{N}P(v)\\). Ентропія Шеннона цієї ймовірності визначається як\n\\[\nVLEn =-\\sum_{{ v = v_{\\min }}}^{N}p( v )\\ln p( v ).\n\\]\nЦя міра, по аналогії до попередньої ентропії, також є мірою складності системи.\n\n\n\n\n\n\nДодаткова інформація по ентропії вертикальних ліній\n\n\n\nДля синусоїдального процесу ми би очікували мале значення даної ентропії, оскільки це простий періодичний процес. Для складного процесу з пам’ятю очiкуємо високе значення цього типу рекурентної ентропiї. Це означатиме, що ламiнарнiсть процесу характеризуються рiзноманiтними перiодами довгостроковості пам’яті системи\n\n\n\nplot_recurrence_measure(measure=ENT_VERT, label='VLEn')\n\n\n\n\n\n\n\nРис. 3.14: Динаміка індексу Доу-Джонса та ентропії вертикальних ліній\n\n\n\n\n\nНа Рис. 3.14 видно, що ентропія вертикальних ліній починає зростати під час крахових явищ, що вказує на зростання ступеня ламінарності, тобто зростання рівномірності розподілу вертикальних ліній різних довжин.\n\n\n3.2.2.10 Дивергенція (Divergence)\nПоказник \\(L_{\\max }\\) може надати нам інформацію про максимальний ступінь передбачуваності досліджуваного періоду. Зворотнє значення максимальної довжини діагональних ліній \\(L_{\\max }\\) або дивергенція (розбіжність) може вказати нам на швидкість та тривалість розбіжності досліджуваних траєкторій. Даний показник можна визначити як\n\\[\nDIV = 1 \\big/ L_{\\max}.\n\\]\nДана міра схожа на старший показник Ляпунова  [6]. Однак взаємозв’язок між цією мірою та позитивним максимальним показником Ляпунова набагато складніший (щоб обчислити показник Ляпунова з RP, необхідно враховувати весь розподіл частот діагональних ліній).\n\n\n\n\n\n\nДодаткова інформація по дивергенції\n\n\n\nЧим вище значення дивергенції, тим швидше розбігаються траєкторії фазового простору. І навпаки, чим нижче значення дивергенції, тим ближче досліджувані траєкторії прилягають одна до одної\n\n\n\nplot_recurrence_measure(measure=DIV, label='DIV')\n\n\n\n\n\n\n\nРис. 3.15: Динаміка індексу Доу-Джонса та дивергенції\n\n\n\n\n\nРис. 3.15 показує, що дивергенція діагональних ліній починає спадати в кризові та передкризові періоди, що також вказує на зростання ступеня впорядкованості динаміки системи в дані періоди часу.\n\n\n3.2.2.11 Дивергенція вертикальних ліній (Vertical line divergence)\nЗворотнє значення максимальної довжини вертикальних ліній \\(V_{max}\\) або розбіжність вертикальних ліній можна визначити як\n\\[\nVDIV = 1 \\big/ V_{\\max}.\n\\]\n\n\n\n\n\n\nДодаткова інформація по дивергенції вертикальних ліній\n\n\n\nМаксимальна довижна вертикальних ліній надавала нам інформацію про максимальний ступінь незмінюваності системи. Вертикальна дивергенція дозволяє нам охарактеризувати швидкість настання або спаду ламінарності у системі. Чим вище значення \\(VDIV\\), тим швидше система виходить із ламінарного стану і навпаки\n\n\n\nplot_recurrence_measure(measure=VERT_DIV, label='VDIV')\n\n\n\n\n\n\n\nРис. 3.16: Динаміка індексу Доу-Джонса та дивергенції вертикальних ліній\n\n\n\n\n\nНа даному рисунку (Рис. 3.16) видно, що періоди криз характеризуються спадом вертикальної дивергенції, тобто зростанням кількості вертикальних структур, що характеризують ще більший ступінь ламінарності станів.\n\n\n3.2.2.12 Дивергенція білих вертикальних ліній\nЗворотнє значення максимальної довжини білих вертикальних ліній (\\(WVL_{max}\\)) можна охарактеризувати як дивергенцію білих вертикальних ліній. Її можна визначити наступним чином:\n\\[\nWVDIV = 1 \\big/ WVL_{max}.\n\\]\nЗростання даного показника має вказувати на зростання ступеня рекурентності системи, а його спад має демонструвати зростання непередбачуваності.\n\nplot_recurrence_measure(measure=WHITE_VERT_DIV, label='WVDIV')\n\n\n\n\n\n\n\nРис. 3.17: Динаміка індексу Доу-Джонса та дивергенції білих вертикальних ліній\n\n\n\n\n\nНа Рис. 3.17 видно, що дивергенція білих вертикальних ліній представляє доволі зашумлену динаміку, а тому не може бути використана в якості ефективного індикатора кризових явищ.\n\n\n3.2.2.13 Ентропія білих вертикальних ліній (White vertical lines entropy)\nІмовірність \\(p( \\omega )\\) того, що біла вертикальна лінія має довжину \\(\\omega\\), можна оцінити за частотним розподілом \\(P(\\omega)\\) із \\(p(\\omega) = P(\\omega) \\Big/ \\sum_{{\\omega=\\omega_{\\min}}}^{N} P(\\omega)\\). Ентропія Шеннона цієї ймовірності,\n\\[\nWVertEn=-\\sum_{{\\omega =\\omega _{\\min }}}^{N}p(\\omega )\\ln p(\\omega ),\n\\]\nде \\(\\omega_{min}\\) — мінімальна довжина білої вертикальної лінії.\n\nplot_recurrence_measure(measure=ENT_WHITE_VERT, label='WVertEn')\n\n\n\n\n\n\n\nРис. 3.18: Динаміка індексу Доу-Джонса та ентропії білих вертикальних ліній\n\n\n\n\n\nВидно, що ентропія білих вертикальних ліній спадає у кризові та передкризові періоди фондового ринку і вказує на зростання загальної передбачуваності системи та зміщення розподілу білих вертикальних ліній до конкретних довжин. Тобто, їх розподіл у періоди криз стає менш симетричним і сигналізує про поступове заміщення білих вертикальних ліній чорними.\n\n\n3.2.2.14 Співвідношення частоти рекурентності до детермінізму \\(DET/RR\\)\nСпіввідношення між \\(DET\\) і \\(RR\\) (\\(RATIO\\)) можна використовувати для виявлення прихованих фазових переходів у системи:\n\\[\nRATIO_1 = DET \\Big/ RR = N^2 \\cdot \\left( \\sum_{l=l_{min}}^{N}l \\cdot P(l) \\right) \\Bigg/ \\left(\\sum_{l=1}^{N}lP(l)\\right)^2.\n\\]\n\nplot_recurrence_measure(measure=RATIO_DET_REC, label='RATIO_1')\n\n\n\n\n\n\n\nРис. 3.19: Динаміка індексу Доу-Джонса та співвідношення між мірою передбачуваності та рекурентності\n\n\n\n\n\nДаний показник спадає під час кризових явищ фондового ринку. Це говорить про те, що має зростати загальна щільність рекурентних точок, як ізольованих, так і всього розподілу вертикальних структур. У кризові періоди \\(RR\\) є вищою за \\(DET\\).\n\n\n3.2.2.15 Співвідношення ламінарності до детермінізму (LAM/DET)\nТак само як і попередня міра, відношення ламінарності до детермінізму може дозволити нам виокремити приховані переходи в досліджуваному сигналі:\n\\[\nRATIO_2 = LAM \\big/ DET.\n\\]\n\nplot_recurrence_measure(measure=RATIO_LAM_DET, label='RATIO_2')\n\n\n\n\n\n\n\nРис. 3.20: Динаміка індексу Доу-Джонса та співвідношення між мірою ламінарності та детермінізмом\n\n\n\n\n\nЯкщо виходити з динаміки показника \\(RATIO_2\\), можна сказати, що загальний ступінь детермінізму починає переважати над ламінарністю під час кризових явищ.\nЗа результатами представлених показників ми можемо сказати, що досліджувані крахові та передкрахові події характеризуються зростанням рекурентності, і подібного роду поведінка може бути використана в якості передвісника можливих кризових явищ.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Лабораторна робота № 3</span>"
    ]
  },
  {
    "objectID": "lab_3.html#завдання-для-самостійної-роботи",
    "href": "lab_3.html#завдання-для-самостійної-роботи",
    "title": "3  Лабораторна робота № 3",
    "section": "3.3 Завдання для самостійної роботи",
    "text": "3.3 Завдання для самостійної роботи\n\nВиберіть за рекомендацією викладача свій варіант часового ряду\nПроведіть дослідження динаміки кількісних мір рекурентності згідно інструкції\nЗробити висновки\n\n\n\n\n\n[1] C. L. Webber and J. P. Zbilut, Dynamical Assessment of Physiological Systems and States Using Recurrence Plot Strategies, Journal of Applied Physiology 76, 965 (1994).\n\n\n[2] J. P. Zbilut and C. L. Webber, Embeddings and Delays as Derived from Quantification of Recurrence Plots, Physics Letters A 171, 199 (1992).\n\n\n[3] N. Marwan, N. Wessel, U. Meyerfeldt, A. Schirdewan, and J. Kurths, Recurrence-Plot-Based Measures of Complexity and Their Application to Heart-Rate-Variability Data, Phys. Rev. E 66, 026702 (2002).\n\n\n[4] A. O. Bielinskyi, V. N. Soloviev, V. Solovieva, S. O. Semerikov, and M. A. Radin, Recurrence Quantification Analysis of Energy Market Crises: A Nonlinear Approach to Risk Management, in Proceedings of the Selected and Revised Papers of 10th International Conference on Monitoring, Modeling & Management of Emergent Economy (M3E2-MLPEED 2022), Virtual Event, Kryvyi Rih, Ukraine, November 17-18, 2022, edited by H. B. Danylchuk and S. O. Semerikov, Vol. 3465 (CEUR-WS.org, 2022), pp. 110–131.\n\n\n[5] A. Tomashin, G. Leonardi, and S. Wallot, Four Methods to Distinguish Between Fractal Dimensions in Time Series Through Recurrence Quantification Analysis, Entropy 24, (2022).\n\n\n[6] J.-P. Eckmann, S. O. Kamphorst, and D. Ruelle, Recurrence Plots of Dynamical Systems, Europhysics Letters 4, 973 (1987).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Лабораторна робота № 3</span>"
    ]
  },
  {
    "objectID": "lab_4.html",
    "href": "lab_4.html",
    "title": "4  Лабораторна робота № 4",
    "section": "",
    "text": "4.1 Теоретичні відомості",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Лабораторна робота № 4</span>"
    ]
  },
  {
    "objectID": "lab_4.html#теоретичні-відомості",
    "href": "lab_4.html#теоретичні-відомості",
    "title": "4  Лабораторна робота № 4",
    "section": "",
    "text": "4.1.1 Складність. Кількісні міри складності. Інформаційні методи оцінки складності.\nДане століття називають століттям складності. Сьогодні питання “що таке складність?” вивчають фізики, біологи, математики і інформатики, хоча при теперішніх досягненнях у розумінні оточуючого світу, однозначної відповіді на це питання немає.\nЗ цієї причини, відповідно до ідеї І. Пригожина, будемо досліджувати прояви складності системи, застосовуючи при цьому сучасні методи кількісного аналізу складності  [1].\nСеред таких методів на увагу заслуговують:\n\nінформаційно-ентропійні;\nзасновані на теорії хаосу;\nскейлінгово-мультифрактальні.\n\nЗрозуміло, виходячи з різної природи методів, покладених в основу формування міри складності, вони приділяють певні вимоги до часових рядів, що слугують вхідними даними. Наприклад, перші дві групи методів вимагають стаціонарності вхідних даних. При цьому мають різну чутливість до таких характеристик, як детермінованність, стохастичність, причинність та кореляції. Тому у подальшому, порівнюючи комплексно ефективність різних показників складності, на вказані обставини ми будемо звертати увагу, підкреслюючи спеціально застосовність того чи іншого показника для характеристики різних сторін складності досліджуваних систем.\nРозгляд першої групи методів почнемо з добре відомої міри складності, запропонованої А. Колмогоровим  [2].\nКолмогорівська складність. Поняття колмогорівської складності (або, як ще говорять, алгоритмічної ентропії) з’явилося в 1960-і роки на стику теорії алгоритмів, теорії інформації і теорії ймовірностей.\nІдея А. Колмогорова полягала в тому, щоб вимірювати кількість інформації, що міститься в індивідуальних скінчених об’єктах (а не у випадкових величинах, як у шеннонівській теорії інформації). Виявилось, що це можливо (хоча лише з точністю до обмеженого доданку). А. Колмогоров запропонував вимірювати кількість інформації в скінчених об’єктах за допомогою теорії алгоритмів, визначивши складність об’єкту як мінімальну довжину програми, що породжує цей об’єкт. Дане визначення стало базисом алгоритмічної теорії інформації, а також алгоритмічної теорії ймовірностей: об’єкт вважається випадковим, якщо його складність наближена до максимальної.\nЩо ж собою являє колмогорівська складність і як її виміряти? На практиці ми часто стикаємося з програмами, які стискують файли (для економії місця в архіві). Найбільш поширені називаються zip, gzip, compress, rar, arj та інші. Застосувавши таку програму до деякого файлу (з текстом, даними, програмою), ми отримуємо його стислу версію (яка, як правило, коротше початкового файлу). За нею можна відновити початковий файл з допомогою парної програми-“декомпресора”. Отже, у першому наближенні колмогорівську складність файлу можна описати як довжину його стислої версії. Тим самим файл, що має регулярну структуру і добре стискуваний, має малу колмогорівську складність (порівняно з його довжиною). Навпаки, погано стискуваний файл має складність, близьку до довжини.\nПрипустимо, що ми маємо фіксований спосіб опису (декомпресор) \\(D\\). Для даного слова \\(x\\) розглянемо всі його описи, тобто всі слова \\(y\\), для яких \\(D(y)\\) визначене й рівне \\(x\\). Довжину найкоротшого з них \\(l(y)\\) і називають колмогорівською складністю слова \\(x\\) при даному способі опису \\(D\\):\n\\[\nKS_{D}(x) = \\min\\{l(y)\\,|\\,D(y)=x\\},\n\\]\nде \\(l(y)\\) позначає довжину слова \\(y\\). Індекс \\(D\\) підкреслює, що визначення залежить від вибору способу \\(D\\).\nМожна показати, що існують оптимальні способи опису. Спосіб опису тим кращий, чим він коротший. Тому природно дати таке визначення: спосіб \\(D_1\\) не гірше за спосіб \\(D_2\\), якщо \\(KS_{D_1}(x) \\leq KS_{D_2}(x)+c\\) при деякому \\(c\\) і при всіх \\(x\\).\nОтже, за Колмогоровим, складність об’єкту (наприклад, тексту — послідовності символів) — це довжина мінімальної програми яка виводить даний текст, а ентропія — це складність, що ділиться на довжину тексту. Також можна розглядати алгоритмічну складність як мінімальний час (або інші обчислювальні ресурси), необхідний для виконання цієї задачі на комп’ютері. А ще ми можемо говорити про комунікаційну складність завдань, в яких задіяно більше одного процесора: це кількість бітів, які потрібно передати при розв’язанні цього завдання  [3,4]. На жаль, це визначення чисто умоглядне. Надійного способу однозначно визначити цю програму не існує. Але є алгоритми, які фактично якраз і намагаються обчислити колмогорівську складність тексту  [5] і ентропію  [6].\n\n\n4.1.2 Оцінка складності Колмогорова за схемою Лемпела-Зіва\nУніверсальна (в сенсі застосовності до різних мовних систем) міра складності кінцевої символьної послідовності була запропонована Лемпелем і Зівом (LZ)  [7]. Складність Лемпеля-Зіва (LZC) є класичною мірою, яка для ергодичних джерел пов’язує поняття складності (у розумінні Колмогорова-Чайтіна) та швидкості ентропії  [8,9]. Для ергодичного динамічного процесу кількість нової інформації, отриманої за одиницю часу (швидкість ентропії), може бути оцінена шляхом вимірювання здатності цього джерела генерувати нові патерни. Завдяки простоті методу LZC, швидкість ентропії може бути оцінена з однієї дискретної послідовності вимірювань з низькими обчислювальними витратами  [10]. У рамках їх підходу складність послідовності оцінюється числом кроків процесу, що її породжує. Припустимими (редакційними) операціями при цьому є:\n\nгенерація символу (необхідна, як мінімум, для синтезу елементів алфавіту) і\nкопіювання “готового” фрагмента з передісторії (тобто з уже синтезованої частини тексту).\n\nНехай \\(\\Sigma\\) — скінчений алфавіт, \\(S\\) — текст (послідовність символів), складений з елементів \\(\\Sigma\\); \\(S[i]\\) — \\(i\\)-й символ тексту; \\(S[i:j]\\) — фрагмент тексту з \\(i\\)-го по \\(j\\)-й символ включно \\((i&lt;j)\\); \\(N=|S|\\) — довжина тексту \\(S\\). Тоді схему синтезу послідовності можна представити у вигляді конкатенації\n\\[\nH(S)=S[1:i_1]S[i_1+1:i_2]...S[i_{k-1}+1:i_k]...S[i_{m−1}+1:N],\n\\tag{4.1}\\]\nде \\(S[i_{k−1}+1:i_k]\\) — фрагмент \\(S\\), породжуваний на \\(k\\)-му кроці, а \\(m=m_{H}(S)\\) — число кроків процесу. З усіляких схем породження \\(S\\) обирається мінімальна за числом кроків. Таким чином, складність послідовності \\(S\\) за LZ\n\\[\nc_{LZ}(S) = \\min_{H}\\{ m_{H}(S) \\}.\n\\]\nМінімальність числа кроків забезпечується вибором для копіювання на кожному кроці максимально довгого прототипу з передісторії. Якщо позначити через \\(j(k)\\) номер позиції, з якої починається копіювання на \\(k\\)-му кроці, то довжина фрагмента копіювання\n\\[\nl_{j(k)} = i_k - i_{k-1} - 1 = \\max_{j \\leq i_{k-1}}\\{ l_{j} : S[i_{k-1}+1:i_{k-1}+l_j]=S[j:j+l_{j}-1] \\},\n\\tag{4.2}\\]\nа сам \\(k\\)-й компонент складнісного розкладання (4.1) можна записати у вигляді\n\\[\nS[i_{k-1}+1:i_{k}] =\n\\begin{cases}\n    S[j(k):j(k)+l_{j(k)}-1] & \\textrm{якщо} \\; j(k) \\neq 0, \\\\\n    S[i_{k-1}+1] & \\textrm{якщо} \\; j(k) = 0.\n\\end{cases}\n\\tag{4.3}\\]\nВипадок \\(j(k) = 0\\) відповідає ситуації, коли в позиції \\(i_{k−1}+1\\) стоїть символ, який раніше не зустрічався. При цьому ми застосовуємо операцію генерації символу.\nБудемо знаходити складність за LZ для часового ряду, який являє собою, наприклад, щоденні значення фінансового індексу. Для дослідження динаміки LZ та порівняння з іншими складними системами будемо знаходити дану міру складності для підряду фіксованої довжини (вікна). Для цього обчислимо логарифмічні прибутковості та перетворимо їх у послідовність бітів. При цьому можна задавати кількість станів, що диференційовані (система числення). Так, для двох різних станів маємо 0, 1, для трьох — 0, 1, 2 і т.д. Для двійкової системи кодування буде задаватися поріг по середньому значенню і стани, наприклад, прибутковостей (ret) кодуватимуться наступним чином  [11–13]:\n\\[\nret = \\begin{cases}\n    0, & ret_t &lt; \\langle ret \\rangle, \\\\\n    1, & ret_t &gt; \\langle ret \\rangle.\n    \\end{cases}\n\\tag{4.4}\\]\nТакож можна визначити так звану пермутаційну складність Лемпеля-Зіва (PLZС)  [14,15]. У даному випадку би будемо опиратись на процедуру реконструкції фазового простору, що згадувалась у лабораторних 2 і 3. Згідно пермутаційній процедурі ми будемо брати фрагмент ряду довжини \\(m\\), що слугує розмірностю реконструйованого атрактора, та замінювати кожне значення ряду його порядковим індексом. На Рис. 4.1 представлено часовий ряд та його можливі порядкові шаблони:\n\n\n\n\n\n\nРис. 4.1: Фрагмент часового ряду (а) та 6 можливих порядкових шаблонів, що можуть зустрічатись у цьому сигналі (b)  [16]\n\n\n\nАлгоритм Лемпеля-Зіва виконує дві операції: (1) додає новий біт в уже існуючу послідовність; (2) копіює вже сформовану послідовність. Алгоритмічна складність представляє собою кількість таких операцій, необхідних для формування заданої послідовності.\nДля випадкової послідовності довжини \\(n\\) алгоритмічна складність обчислюється за виразом \\(LZC_r = n \\big/ \\log(n)\\). Тоді відносна алгоритмічна складність знаходиться як відношення отриманої складності до складності випадкової послідовності: \\(LZC = LZC \\big/ LZC_{r}\\).\nОднак навіть цього підходу може бути недостатньо. Справа в тому, що складні сигнали проявляють притаманну їм складність на різних просторових і часових масштабах, тобто мають масштабно інваріантні властивості. Вони, зокрема проявляються через степеневі закони розподілу. Тому мономасштабні розрахунки алгоритмічної складності можуть бути неприйнятними і призводити до помилкових висновків.\nДля подолання таких труднощів використовуються мультимасштабні методи, до розгляду яких ми і переходимо.\n\n\n4.1.3 Процедура грануляції для мультискейлінгового дослідження часових рядів. Мультимасштабні міри складності\nІдея цієї групи методів включає дві послідовно виконувані процедури:\n\nпроцес “грубого дроблення” (coarse graining — “грануляції”) початкового часового ряду — усереднення даних на сегментах, що не перетинаються, розмір яких (вікно усереднення) збільшуватиметься на одиницю при переході на наступний за величиною масштаб;\nобчислення на кожному з масштабів певного (до сих пір мономасштабного) показника складності.\n\nПроцес “грубого дроблення” (“грануляція”) полягає в усереднені послідовних відліків ряду в межах вікон, що не перетинаються, а розмір яких \\(\\tau\\) — збільшується при переході від масштабу до масштабу. Кожен елемент “гранульованого” часового ряду \\(y_{j}^{\\tau}\\) знаходиться у відповідності до виразу  [17]:\n\\[\ny_{j}^{\\tau} = \\frac{1}{\\tau}\\sum_{i=(j-1)\\tau+1}^{j\\tau}x_i, \\; 1 \\leq j \\leq N/\\tau,\n\\]\nде \\(\\tau\\) характеризує фактор масштабування. Довжина кожного “гранульованого” ряду залежить від розміру вікна \\(і\\) рівна \\(N \\big/ \\tau\\). Для масштабу рівного 1 “гранульований” ряд просто тотожний оригінальному.\n\n\n\n\n\n\nРис. 4.2: Схематична ілюстрація процесу грубого дроблення (“грануляції”) початкового часового ряду для масштабів 2 і 3\n\n\n\nБібліотека neurokit2 представляє метод для обчислення як мономасштабного показника складності Лемпеля-Зіва, так і його мультимасштабного аналогу.\nСинтаксис мономасштабної процедури виглядає наступним чином:\ncomplexity_lempelziv(signal, delay=1, dimension=2, permutation=False, symbolize='mean', **kwargs)\nПараметри:\n\nsignal (Union[list, np.array, pd.Series]) — сигнал;\ndelay (int) — часова затримка, \\(\\tau\\). Використовується лише тоді, коли permutation=True;\ndimension (int) — розмірність вкладень, \\(m\\). Використовується лише коли permutation=True;\npermutation (bool) — якщо значення True, поверне складність Лемпеля-Зіва на основі порядкових патернів;\nsymbolize (str) — використовується тільки коли permutation=False. Метод перетворення неперервного сигналу на вході у символьний (дискретний) сигнал. За замовчуванням присвоює 0 та 1 значенням нижче та вище середнього. Може мати значення None, щоб пропустити процес (якщо вхідний сигнал вже є дискретним). Можна скористатися методом complexity_symbolize() для застосування іншої процедури символізації ряду;\nkwargs — інші аргументи, які передаються до complexity_ordinalpatterns() (якщо permutation=True) або complexity_symbolize().\n\nПовертає:\n\nlzc (float) — складність Лемпеля-Зіва (LZC);\ninfo (dict) — словник, містить додаткову інформацію про параметри, що використовуються для обчислення LZC.\n\nСинтаксис мультимасштабної процедури вже інший:\nentropy_multiscale(signal, scale='default', dimension=3, tolerance='sd', method='MSEn', show=False, **kwargs)\nПараметри:\n\nsignal (Union[list, np.array, pd.Series]) — сигнал;\nscale (str або int або list) — список масштабних коефіцієнтів, що використовуються для процедури крос-грануляції часового ряду. Якщо значення \"default\", буде використано range(len(signal) / (dimension + 10)). Якщо \"max\", використовуватиме всі масштаби до половини довжини сигналу. Якщо ціле число, створить діапазон до вказаного цілого числа;\ndimension (int) — розмірність вкладення \\(m\\);\ntolerance (float) — поріг пропускання \\(\\varepsilon\\);\nmethod (str) — яку версію мультимасштабного показника обчислювати. Переважна кількість показників за цим методом відповідають ентропійним підходам. Нас цікавитиме саме \"LZC\";\nshow (bool) — візуалізувати залежність показника від масштабу;\nkwargs — необов’язкові аргументи.\n\nПовертає:\n\nfloat — точкова оцінка мультимасштабного показника окремого часового ряду, що відповідає площі під кривою значень цього показника, яка, по суті, є сумою вибіркових значень, наприклад, \"LZC\" в діапазоні масштабних коефіцієнтів;\ndict — словник, що містить додаткову інформацію щодо використаних для обчислення мультимасштабного показника параметрів. Значення показника, що відповідають кожному фактору \"Scale\", зберігаються під ключем \"Value\".\n\n\n\n4.1.4 Шеннонівська складність\nЕнтропійний аналіз часових рядів за допомогою ентропійних показників різного роду буде проведено у наступних роботах. Зараз же ми розглянемо найпростішу з ентропій — ентропію Шеннона та порівняємо її можливості кількісно оцінювати складність часових послідовностей у порівнянні з мірою Лемпеля-Зіва.\nЕнтропія Шеннона — це статистичний квантифікатор, який широко використовується для характеристики складних процесів. Поняття ентропії було використано Шенноном в теорії інформації для передачі даних  [6].\nЕнтропія - це міра невизначеності та випадковості системі. Якщо припустити, що всі наявні дані належать до одного класу, то неважко передбачити клас нових даних. Невизначеність, що виникає, коли подія \\(E\\) відбувається з ймовірністю \\(p\\), можна позначити як \\(S(p)\\). Якщо ймовірність появи класу дорівнює 1, тоді ентропія мінімальна, \\(S(1) = 0\\). Відповідно до концепції Шеннона, якщо у нас наявні ймовірності реалізації певної події \\(p_1, p_2, p_3, ..., p_n\\), на виході отримується кількість інформації, що необхідна для опису цієї події. Тоді, Шеннонівська ентропія може бути визначена як\n\\[\nS = -\\sum_{i=1}^{n}p_i \\ln p_{i}.  \n\\]\nСинтаксис методу для розрахунку Шеннонівської ентропії має вигляд:\nentropy_shannon(signal=None, base=2, symbolize=None, show=False, freq=None, **kwargs)\nПараметри:\n\nsignal (Union[list, np.array, pd.Series]) — сигнал;\nbase (float) — основа логарифму (за замовчуванням дорівнює 2). scipy.stats.entropy() за замовчуванням використовує число Ейлера (np.e) (натуральний логарифм), що дає міру інформації, виражену в натах;\nsymbolize (str) — метод приведення неперервного сигналу на вході у символьний (дискретний) сигнал. За замовчуванням дорівнює нулю, що пропускає процес (вважається, що вхідні дані вже є дискретними);\nshow (bool) — якщо значення True, виводить часовий ряд, де кожне значення розфарбоване у відповідності до класу до якого воно належить;\nfreq (np.array) — замість сигналу можна надати вектор ймовірностей;\nkwargs — необов’язкові аргументи. Наразі не використовуються.\n\nПовертає:\n\nshanen (float) — Шеннонівську ентропію;\ninfo (dict) — словник, що містить додаткову інформацію про параметри обчислення Шеннонівської ентропії.\n\n\n\n4.1.5 Інформація Фішера\nІнформацію Фішера було введено Р. А. Фішером у 1922 році як міру “внутрішньої точності” в теорії статистичних оцінок  [18]. Вона є центральною для багатьох статистичних застосувань, що виходять далеко за межі теорії складності. Даний показник вимірює кількість інформації, яку спостережувана випадкова величина несе про невідомий параметр. В аналізі складності вимірюється кількість інформації системи “про себе”. Він базується на розкладанні за сингулярними значеннями реконструйованого фазового простору. Величина показника Фішера зазвичай антикорельована з іншими показниками складності (чим більше інформації система приховує про себе, тим більш передбачуваною і, відповідно, менш складною вона є).\nІнформацію Фішера можна визначити, використовуючи метод fisher_information() бібліотеки neurokit2. Її синтаксис виглядає наступним чином:\nfisher_information(signal, delay=1, dimension=2) з визначеними вже раніше параметрами\nПовертає:\n\nfi (float) — обчислена міра інформації Фішера;\ninfo (dict) — словник, що містить додаткову інформацію про параметри обчислення інформації Фішера.\n\n\n\n4.1.6 Складність та параметри Хьорта\nПараметри Хьорта — це показники статистичних властивостей, які спочатку були введені Хьортом  [19] для опису загальних характеристик сигналів електроенцифалограми. Параметрами є активність, рухливість і складність:\n\nПараметр активності (\\(Activity\\)) — це просто дисперсія сигналу, яка відповідає середній потужності сигналу (якщо його середнє значення дорівнює 0):\n\n\\[\nActivity = \\sigma^{2}_{signal}.\n\\]\n\nПараметр рухливості (\\(Mobility\\)) являє собою середню частоту або частку середньоквадратичного відхилення спектра потужності. Він визначається як квадратний корінь з дисперсії першої похідної сигналу, поділений на дисперсію сигналу:\n\n\\[\nMobility = \\frac{\\sigma_{dd}/\\sigma_{d}}{Complexity}.\n\\]\n\nПараметр складності (\\(Complexity\\)) дає оцінку смуги пропускання сигналу, що вказує на схожість форми сигналу з чистою синусоїдою (для якої значення сходиться до 1). Іншими словами, це характеристика “надмірної деталізації” по відношенню до “найм’якшої” можливої форми кривої. Параметр “Складність” визначається як відношення рухливості першої похідної сигналу до рухливості самого сигналу:\n\n\\[\nComplexity = \\sigma_d \\big/ \\sigma_{signal},\n\\]\nде \\(d\\) та \\(dd\\) представляють перші та другі похідні сигналу, відповідно.\nБібліотека neurokit2 представляє метод для отримання відповідних показників. Її синтаксис:\ncomplexity_hjorth(signal)\nПовертає:\n\nhjorth (float) — показник складності Хьорта;\ninfo (dict) — словник, що містить додаткові показники Хьорта \"Mobility\" та \"Activity\".\n\n\n\n4.1.7 Час декореляції\nЧас декореляції (decorrelation time, DT) визначається як час (у відліках) першого перетину нуля функції автокореляції. Коротший час декореляції відповідає менш корельованому сигналу. Наприклад, зменшення часу декореляції в сигналах електроенцифалограми спостерігається перед нападами, що пов’язано зі зменшенням потужності низьких частот  [20].\nБібліотека neurokit2 представляє функціонал для визначення часу декореляції, а саме метод complexity_decorrelation(). Її синтаксис:\ncomplexity_decorrelation(signal)\nПовертає:\n\nfloat — час декореляції;\ndict — словник, що містить додаткову інформацію про додаткові показники.\n\n\n\n4.1.8 Відносна грубість (нерівність, шорсткість)\nВідносна шорсткість — це відношення локальної дисперсії (автоковаріації з лагом 1) до глобальної дисперсії (автоковаріації з лагом 0), яке можна використовувати для класифікації різних “шумів”. Цей показник також можна використовувати як індекс застосовності фрактального аналізу (показники фрактальності будуть описані в наступних роботах)  [21].\nСинтаксис даного методу в бібліотеці neurokit2 виглядає наступним чином:\ncomplexity_relativeroughness(signal, **kwargs)\nПовертає:\n\nrr (float) — значення відносної грубості;\ninfo (dict) — словник, що містить інформацію відносно параметрів для обчислення показника грубості.\n\n\n\n4.1.9 Взаємна інформація\nКоли йдеться про виявлення зв’язків між змінними, ми часто використовуємо кореляцію Пірсона. Проблема полягає в тому, що цей показник знаходить лише лінійні зв’язки, що іноді може призвести до неправильної інтерпретації зв’язку між двома змінними. Інші статистичні методи вимірюють нелінійні зв’язки, такі як, наприклад, взаємна інформація (mutual information, MI)  [22].\nВзаємна інформація між двома випадковими величинами вимірює нелінійний зв’язок між ними. Крім того, вона показує, яку кількість інформації можна отримати з випадкової величини, спостерігаючи за іншою випадковою величиною.\nВона тісно пов’язана з поняттям ентропії. Тобто, зменшення невизначеності випадкової величини корелює з отриманням інформації з іншої випадкової величини. І високе значення взаємної інформації вказує на помітне зменшення невизначеності. Якщо взаємна інформація дорівнює нулю, дві випадкові величини є незалежними.\nВзаємну інформацію можна розрахувати наступним чином:\n\\[\nI(X; Y) = \\sum_{y \\in Y}\\sum_{x \\in X}p(x, y) \\cdot \\log{\\left[ p(x,y)/p(x) \\cdot p(y) \\right]},\n\\]\nде \\(p(x)\\) та \\(p(y)\\) ймовірності спостереження окремо \\(x\\) або \\(y\\), а \\(p(x,y)\\) ймовірність спостереження одночасно \\(x\\) та \\(y\\).\nОсновна відмінність між кореляцією та взаємною інформацією полягає в тому, що кореляція є мірою лінійної залежності, тоді як взаємна інформація вимірює загальну залежність (включаючи нелінійні зв’язки). Взаємна інформація дорівнює нулю, коли дві випадкові величини є строго незалежними.\nБібліотека neurokit2 представляє інструментарій для знаходження взаємної інформації між двома сигналами \\(x\\) та \\(y\\). У даній роботі ми спробуємо віднайти взаємну інформацію як між двома часовими рядами, так і авто-взаємну інформацію, подібно до автокореляції.\nСинтаксис потрібної нам процедури:\nmutual_information(x, y, method='varoquaux', bins='default', **kwargs)\nПараметри:\n\nx і y (Union[list, np.array, pd.Series]) — масив значень;\nmethod (str) — метод для обчислення взаємної інформації: \"nolitsa\", \"varoquaux\", \"knn\", \"max\"’;\nbins (int) — кількість бінів гістограми. Використовується лише для \"nolitsa\" та \"varoquaux\". Якщо \"default\", кількість бінів оцінюється згідно методики описаної в  [23];\nkwargs — додаткові ключові аргументи для обраного методу.\n\nПовертає:\n\nfloat — розрахована взаємна інформація.\n\nІснують різноманітні підходи до розрахунку взаємної інформації:\n\nnolitsa: класична взаємна інформація;\nvaroquaux: застосовує фільтр Гауса до об’єднаної гістограми. Величину згладжування можна налаштовувати за допомогою аргументу sigma (за замовчуванням sigma=1);\nknn: непараметрична (тобто не заснована на біннінгу) оцінка за найближчими сусідами. Додаткові параметри включають k (за замовчуванням, k=3), кількість найближчих сусідів для використання;\nmax: максимальний коефіцієнт взаємної інформації, тобто \\(MI\\) є максимальним при певній комбінації кількості бінів.\n\nРозглянемо ефективність використання зазначених показників у якості індикаторів або індикаторів-передвісників крахових подій.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Лабораторна робота № 4</span>"
    ]
  },
  {
    "objectID": "lab_4.html#хід-роботи",
    "href": "lab_4.html#хід-роботи",
    "title": "4  Лабораторна робота № 4",
    "section": "4.2 Хід роботи",
    "text": "4.2 Хід роботи\nСпочатку імпортуємо необхідні модулі для подальшої роботи:\n\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport neurokit2 as nk\nimport yfinance as yf\nimport pandas as pd\nimport scienceplots\nfrom tqdm import tqdm\n\n%matplotlib inline\n\nІ виконаємо налаштування рисунків для виведення:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nsize = 16\nparams = {\n    'figure.figsize': (8, 6),            # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': size,                   # розмір фонтів рисунку\n    'lines.linewidth': 2,                # товщина ліній\n    'axes.titlesize': 'small',           # розмір титулки над рисунком\n    'axes.labelsize': size,              # розмір підписів по осям\n    'legend.fontsize': size,             # розмір легенди\n    'xtick.labelsize': size,             # розмір розмітки по осі Ох\n    'ytick.labelsize': size,             # розмір розмітки по осі Ох\n    \"font.family\": \"Serif\",              # сімейство стилів підписів \n    \"font.serif\": [\"Times New Roman\"],   # стиль підпису\n    'savefig.dpi': 300,                  # якість збережених зображень\n    'axes.grid': False                   # побудова сітки на самому рисунку\n}\n\nplt.rcParams.update(params)              # оновлення стилю згідно налаштувань\n\nЦього разу розглянему можливість побудови індикаторів-передвісників на прикладі фондового індексу S&P 500, але, окрім цього, додамо ще Біткоїн для розрахунку взаємної інформації між фондовим та криптовалютним ринками. Очевидно, що фондовий індекс S&P 500 має довшу ніж Біткоїн історію. До того ж, криптовалютний ринок працює безперервно на відміну від фондового. Тому треба буде об’єднати значення двох активів за тими датами, що співпадають.\nВиконуємо зчитування фондового індексу:\n\nsymbol_1 = '^GSPC'         # Символ першого індексу\nstart_1 = \"2014-01-01\"     # Дата початку зчитування даних\nend_1 = \"2023-08-24\"       # Дата закінчення зчитування даних\n\ndata_1 = yf.download(symbol_1, start_1, end_1)  # вивантажуємо дані\ntime_ser_1 = data_1['Adj Close'].copy()         # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'      # підпис по вісі Ох \nylabel_1 = symbol_1        # підпис по вісі Оу\n\n[*********************100%%**********************]  1 of 1 completed\n\n\nВиконуємо зчитування криптовалютного індексу:\n\nsymbol_2 = 'BTC-USD'       # Символ другого індексу\nstart_2 = \"2014-01-01\"     # Дата початку зчитування даних\nend_2 = \"2023-08-24\"       # Дата закінчення зчитування даних\n\ndata_2 = yf.download(symbol_2, start_2, end_2)  # вивантажуємо дані\ntime_ser_2 = data_2['Adj Close'].copy()         # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'      # підпис по вісі Ох \nylabel_2 = symbol_2        # підпис по вісі Оу\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nУвага\n\n\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance, а із власного файлу. Зрозуміло, що й аналіз результатів, і висновки залежать від того, з яким рядом ми працюємо\n\n\n\n\nsymbol = 'sMpa11'                  # Символ індексу\n\npath = \"databases\\sMpa11.txt\"      # шлях по якому здійснюється зчитування файлу\ndata = pd.read_csv(path,           # зчитування даних \n                   names=[symbol])\ntime_ser = data[symbol].copy()     # копіюємо значення кривої \n                                   # \"напруга-видовження\" до окремої змінної\n\nxlabel = r'$\\varepsilon$'          # підпис по вісі Ох \nylabel = symbol                    # підпис по вісі Оу\n\n\nТепер створимо новий масив даних, що об’єднуватиме в собі значення S&P 500 та BTC за їх спільними датами:\n\n# приводимо значення індексів до типу DataFrame, щоб мати змогу їх об'єднати \n# за допомогою бібліотеки pandas\ndf_time_ser_1 = pd.DataFrame(time_ser_1) \ndf_time_ser_2 = pd.DataFrame(time_ser_2)\n\n\njoined = df_time_ser_1.merge(df_time_ser_2, # об'єднуємо по датам тієї бази, що містить \n                             on='Date',     # більше дат\n                             how='left')  \n\njoined = joined.rename(columns={joined.columns[0]: symbol_1,  # переіменовуємо колонки по \n                                joined.columns[1]: symbol_2}) # змінним symbol_1 та symbol_2\n\njoined = joined.dropna()  # видаляємо рядки, що містять нульові значення\n\nВиводимо отриману базу:\n\njoined\n\n\n\n\n\n\n\n\n\n^GSPC\nBTC-USD\n\n\nDate\n\n\n\n\n\n\n2014-09-17\n2001.569946\n457.334015\n\n\n2014-09-18\n2011.359985\n424.440002\n\n\n2014-09-19\n2010.400024\n394.795990\n\n\n2014-09-22\n1994.290039\n402.152008\n\n\n2014-09-23\n1982.770020\n435.790985\n\n\n...\n...\n...\n\n\n2023-08-17\n4370.359863\n26664.550781\n\n\n2023-08-18\n4369.709961\n26049.556641\n\n\n2023-08-21\n4399.770020\n26124.140625\n\n\n2023-08-22\n4387.549805\n26031.656250\n\n\n2023-08-23\n4436.009766\n26431.640625\n\n\n\n\n2249 rows × 2 columns\n\n\n\n\nІ візуалізуємо сам графік. Спочатку оголосимо функцію для попарної візуалізації рядів зі збереженням їх абсолютних значень:\n\ndef plot_pair(x_values, y_values, x_label, y_label, file_name, clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(x_values, \n                  y_values[0], \n                  \"b-\", label=fr\"{y_label[0]}\")\n    p2, = ax2.plot(x_values,\n                   y_values[1], \n                   color=clr, \n                   label=fr'${y_label[1]}$')\n\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(f\"{y_label[0]}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(file_name + \".jpg\")\n        \n    plt.show();\n\nІ тепер візуалізуємо отримані ряди:\n\nvalues_plot = joined.iloc[:, 0].values, joined.iloc[:, 1].values\nylabels = ylabel_1, ylabel_2\nfile_name = f'joined {symbol_1}_{symbol_2}'\n\n\nplot_pair(joined.index, values_plot, xlabel, ylabels, file_name)\n\n\n\n\n\n\n\nРис. 4.3: Динаміка індексу S&P 500 та Біткоїна за досліджуваний період\n\n\n\n\n\n\n\n\n\n\n\nВажливо\n\n\n\nНе виконуйте блоки коду, що відповідають секції “Розрахунок взаємної інформації”, якщо ви працюєте з текстовим файлом\n\n\n\n4.2.1 Розрахунок взаємної інформації\nРозглянемо взаємну інформацію як індикатор нелінійної кореляції між двома фінансовими активами, і спробуємо визначити, чи є між ними “істинний” взаємозв’язок. Виконаємо розрахунки із використанням алгоритму руховому вікна. Також визначимо функцію transform() для нормалізації ряду.\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\n\nret_type = 6                           # вид ряду\nwindow = 100                           # ширина вікна\ntstep = 1                              # часовий крок вікна \nlength = len(joined.iloc[:,0].values)  # довжина самого ряду\n\nMI = []                                # масив для віконної взаємної інформації\n\nТепер приступимо до розрахунків:\n\nfor i in tqdm(range(0,length-window,tstep)):       # фрагменти довжиною window  \n                                                   # з кроком tstep\n\n    # відбираємо фрагменти\n    fragm_1 = joined[symbol_1][i:i+window]  \n    fragm_2 = joined[symbol_2][i:i+window]\n\n    # виконуємо процедуру трансформації ряду \n    fragm_1 = transformation(fragm_1, ret_type)    \n    fragm_2 = transformation(fragm_2, ret_type)\n\n    # розраховуємо взаємну інформацію \n    mut_inf = nk.mutual_information(fragm_1, fragm_2)\n    \n    # та додаємо результат до масиву значень\n    MI.append(mut_inf)\n\n100%|██████████| 2149/2149 [00:02&lt;00:00, 930.50it/s]\n\n\nЗберігаємо отриманий результат у текстовому файлі:\n\nnp.savetxt(f\"mutual_inf_name1={symbol_1}_name2={symbol_2}_ \\\n    window={window}_step={tstep}_rettype={ret_type}.txt\" , MI)\n\nВізуалізуємо результат між відповідними показниками:\n\nfig, ax = plt.subplots(1, 1)\n\nax2 = ax.twinx()\nax3 = ax.twinx()\nax2.spines.right.set_position((\"axes\", 1.03))\nax3.spines.right.set_position((\"axes\", 1.15))\n\np1, = ax.plot(joined.index[window:length:tstep], \n                joined[symbol_1][window:length:tstep].values, \n                \"b-\", \n                label=fr\"{symbol_1}\")\np2, = ax2.plot(joined.index[window:length:tstep],\n                joined[symbol_2][window:length:tstep].values,\n                'red', \n                label=fr\"{symbol_2}\")\np3, = ax3.plot(joined.index[window:length:tstep],\n                MI,\n                'magenta', \n                label=r\"$MI$\")               \n\nax.set_xlabel(xlabel)\nax.set_ylabel(f\"{symbol_1}\")\nax.yaxis.label.set_color(p1.get_color())\nax2.yaxis.label.set_color(p2.get_color())\nax3.yaxis.label.set_color(p3.get_color())\n\ntkw = dict(size=3, width=1.5)\n\nax.tick_params(axis='x', **tkw)\nax.tick_params(axis='y', colors=p1.get_color(), **tkw)\nax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\nax3.tick_params(axis='y', colors=p3.get_color(), **tkw)\nax3.legend(handles=[p1, p2, p3])\n\nplt.savefig(f\"mutual_inf_name1={symbol_1}_name2={symbol_2}_ \\\n    window={window}_step={tstep}_rettype={ret_type}.jpg\")\n\nplt.show();\n\n\n\n\n\n\n\nРис. 4.4: Динаміка індексу S&P 500, Біткоїна та взаємної інформації\n\n\n\n\n\nЯк ми можемо бачити з представленого рисунку, як на фондовому так і криптовалютному ринках дійсно спостерігалися фази зростання взаємної інформації. Найкраще це видно напередодні кризи 2018-го року, під час 2019 року, після коронавірусної пандемії та напередодні 2023 року. Для даного індикатора залишається простір для експериментів, що можуть вивести його на рівень достатньо потужного передвісника криз на фінансових ринках.\nЯк вже зазначалося, окрім обчислення взаємної інформації для двох пар часових сигналів, ми можемо обчислити автовзаємну інформацію.\nДля цього визначимо наступну функцію:\n\ndef automut(x, maxlag):\n    n = len(x)                               # визначаємо довжину сигналу\n    lags = np.arange(0, maxlag, dtype=\"int\") # оголошуємо масив лагів від 0 до maxlag\n    mi = np.zeros(len(lags))                 # оголошуємо масив під значення взаємної інформації\n    for i, lag in enumerate(lags):           # проходимось по кожному лагу\n        \n        # виконуємо зміщення на lag значень \n        y1 = x[:n-lag].copy()\n        y2 = x[lag:].copy()\n\n        # і розраховуємо взаємну інформацію між часовим рядом y1\n        # та його зміщенною на lag кроків копією \n        mi[i] = nk.mutual_information(y1, y2, bins=100)\n\n    return mi\n\nВиведемо залежність автовзаємної інформації від лагу для всього ряду S&P 500 та Біткоїна. Спочатку розрахуємо вихідні значення ряду, далі прибутковості і потім волатильності. Для кожного з відповідних сигналів виведемо взаємну інформацію.\nВиконуємо перетворення S&P 500 та Біткоїна:\n\nsp_init = transformation(time_ser_1, ret_type=1)\nsp_ret = transformation(time_ser_1, ret_type=4)\nsp_vol = np.abs(sp_ret.copy())\n\nbtc_init = transformation(time_ser_2, ret_type=1)\nbtc_ret = transformation(time_ser_2, ret_type=4)\nbtc_vol = np.abs(btc_ret.copy())\n\nРозраховуємо автовзаємну інформацію S&P 500 та Біткоїна:\n\nmax_lag = 100\n\nmu_sp_init = automut(sp_init, max_lag)\nmu_sp_ret = automut(sp_ret, max_lag)\nmu_sp_vol = automut(sp_vol, max_lag)\n\nmu_btc_init = automut(btc_init, max_lag)\nmu_btc_ret = automut(btc_ret, max_lag)\nmu_btc_vol = automut(btc_vol, max_lag)\n\nlags = np.arange(0, max_lag, dtype=\"int\") # оголошуємо масив лагів від 0 до maxlag\n\n\nfig, ax = plt.subplots(1, 1)                # Створюємо порожній графік\n\nax.plot(lags, mu_sp_init, label=r'$MI $ ' + f'{symbol_1}')  # Додаємо дані до графіку\nax.plot(lags, mu_sp_ret, label=r'$MI$ ' + r'$g(t)$')                          \nax.plot(lags, mu_sp_vol, label=r'$MI$ ' +  r'$V_{T}$') \n\nax.legend()                                 # Додаємо легенду\nax.set_xlabel(\"Lag\")                        # Додаємо підпис для вісі Ох\nax.set_ylabel(\"Automutual information\")     # Додаємо підпис для вісі Оу\n\nplt.savefig(f'Automutual information {symbol_1}.jpg')  # Зберігаємо графік \nplt.show();                                            # Виводимо графік\n\n\n\n\n\n\n\nРис. 4.5: Зміна з часом автовзаємної інформації для вихідного ряду x, нормалізованих прибутковостей g та модулів mod(g) фондового індексу S&P 500\n\n\n\n\n\n\nfig, ax = plt.subplots()                     # Створюємо порожній графік\n\nax.plot(lags, mu_btc_init, label=r'$MI $ ' + f'{symbol_2}')  # Додаємо дані до графіку\nax.plot(lags, mu_btc_ret, label=r'$MI$ ' + r'$g(t)$')                          \nax.plot(lags, mu_btc_vol, label=r'$MI$ ' +  r'$V_{T}$') \n\nax.legend()                                 # Додаємо легенду\nax.set_xlabel(\"Lag\")                        # Додаємо підпис для вісі Ох\nax.set_ylabel(\"Automutual information\")     # Додаємо підпис для вісі Оу\n\nplt.savefig(f'Automutual information {symbol_2}.jpg')  # Зберігаємо графік \nplt.show();                                            # Виводимо графік\n\n\n\n\n\n\n\nРис. 4.6: Зміна з часом автовзаємної інформації для вихідного ряду x, нормалізованих прибутковостей g та модулів mod(g) криптовалютного індексу BTC\n\n\n\n\n\nОчевидно, ступінь взаємної інформації це показник, що найкращим чином працює саме для вихідних значень часових сигналів. Для вихідного ряду ступінь взаємної інформації залишається доволі високим. Для прибутковостей і волатильностей взаємна інформація спадає одразу на першому лагу, що свідчить про незалежність значень на подальших часових затримках.\n\n\n4.2.2 Розрахунок мономасштабної складності Лемпеля-Зіва\nПродовжимо розраховувати й інші показники складності. Розглянемо можливість використання показника складності Лемпеля-Зіва в якості індикатора катастрофічних подій.\n\nret_type = 4                           # вид ряду\nwindow = 250                           # ширина вікна\ntstep = 1                              # часовий крок вікна \nlength = len(time_ser_1.values)        # довжина самого ряду\nm = 4                                  # розмірність вкладень \ntau = 1                                # часова затримка         \n\nLZC = []                               # класична складність Лемпеля-Зіва\nPLZC = []                              # пермутаційна складність Лемпеля-Зіва\n\n\nfor i in tqdm(range(0,length-window,tstep)):    # фрагменти довжиною window  \n                                                # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо класичну складність Лемпеля-Зіва \n    lzc, _ = nk.complexity_lempelziv(fragm)\n\n    # та пермутаційну складність Лемпеля-Зіва\n    plzc, _ = nk.complexity_lempelziv(fragm, \n                                      delay=tau, \n                                      dimension=m, \n                                      permutation=True)\n\n    # та додаємо результати до масиву значень\n    LZC.append(lzc)\n    PLZC.append(plzc)\n\n100%|██████████| 2177/2177 [00:12&lt;00:00, 179.21it/s]\n\n\nЗберігаємо результати в текстових файлах:\n\nnp.savetxt(f\"lzc_name={symbol_1}_window={window}_step={tstep}_rettype={ret_type}.txt\" , LZC)\nnp.savetxt(f\"plzc_name={symbol_1}_window={window}_step={tstep}_ \\\n    rettype={ret_type}_m={m}_tau={tau}.txt\" , PLZC)\n\nТа візуалізуємо їх:\n\nfig, ax = plt.subplots(1, 1)\n\nax2 = ax.twinx()\nax3 = ax.twinx()\nax2.spines.right.set_position((\"axes\", 1.03))\nax3.spines.right.set_position((\"axes\", 1.12))\n\np1, = ax.plot(time_ser_1.index[window:length:tstep], \n                time_ser_1.values[window:length:tstep], \n                \"b-\", \n                label=fr\"{symbol_1}\")\np2, = ax2.plot(time_ser_1.index[window:length:tstep],\n                LZC,\n                'gold', \n                label=fr\"$LZC$\")\np3, = ax3.plot(time_ser_1.index[window:length:tstep],\n                PLZC,\n                'red', \n                label=fr\"$PLZC$\")               \n\n\nax.set_xlabel(xlabel)\nax.set_ylabel(f\"{symbol_1}\")\nax.yaxis.label.set_color(p1.get_color())\nax2.yaxis.label.set_color(p2.get_color())\nax3.yaxis.label.set_color(p3.get_color())\n\ntkw = dict(size=3, width=1.5)\n\nax.tick_params(axis='x', **tkw)\nax.tick_params(axis='y', colors=p1.get_color(), **tkw)\nax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\nax3.tick_params(axis='y', colors=p3.get_color(), **tkw)\nax3.legend(handles=[p1, p2, p3])\n\nplt.savefig(f\"plzc_lzc_name={symbol_1}_ \\\n    window={window}_step={tstep}_ \\\n    rettype={ret_type}_m={m}_tau={tau}.jpg\")\n\nplt.show();\n\n\n\n\n\n\n\nРис. 4.7: Динаміка індексу S&P 500, класичної мономасштабної складності Лемпеля-Зіва та її пермутаційного аналогу\n\n\n\n\n\nНа даному рисунку видно, що 2 міри поводять себе асиметрично по відношенню один до одного: \\(LCZ\\) вказує на зростання складності, наприклад, події 2019 року. У той же час \\(PLCZ\\) вказує на спад складності системи в цей період. Варто дослідити мультимасштабну динаміку міри Лемпеля-Зіва для більш змістовних висновків.\n\n\n4.2.3 Обчислення мультимасштабної складності Лемпеля-Зіва\nРозрахуємо віконну динаміку мультимасштабних показників Лемпеля-Зіва. Ми повертаємо сумарну складність Лемпеля-Зіва за всіма масштабами.\n\nret_type = 4                      # вид ряду\nwindow = 250                      # ширина вікна\ntstep = 1                         # часовий крок вікна \nlength = len(time_ser_1.values)   # довжина самого ряду\nm = 4                             # розмірність вкладень \ntau = 1                           # часова затримка         \n\nMSLZC = []                        # мультимасштабна складність Лемпеля-Зіва\nMSPLZC = []                       # мультимасштабна пермутаційна складність Лемпеля-Зіва\n\n\nfor i in tqdm(range(0,length-window,tstep)):    # фрагменти довжиною window  \n                                                # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо мультимасштабну складність Лемпеля-Зіва \n    mslzc, _ = nk.entropy_multiscale(fragm)\n\n    # та мультимасштабну пермутаційну складність Лемпеля-Зіва\n    msplzc, _ = nk.entropy_multiscale(fragm, \n                                      delay=tau, \n                                      dimension=m, \n                                      permutation=True)\n\n\n    # та додаємо результати до масиву значень\n    MSLZC.append(mslzc)\n    MSPLZC.append(msplzc)\n\n100%|██████████| 2177/2177 [00:49&lt;00:00, 43.75it/s]\n\n\n\nnp.savetxt(f\"mslzc_name={symbol_1}_window={window}_step={tstep}_ \\\n    rettype={ret_type}.txt\" , MSLZC)\nnp.savetxt(f\"msplzc_name={symbol_1}_window={window}_step={tstep}_ \\\n    rettype={ret_type}_m={m}_tau={tau}.txt\" , MSPLZC)\n\n\nfig, ax = plt.subplots(1, 1)\n\nax2 = ax.twinx()\nax3 = ax.twinx()\nax2.spines.right.set_position((\"axes\", 1.03))\nax3.spines.right.set_position((\"axes\", 1.12))\n\np1, = ax.plot(time_ser_1.index[window:length:tstep], \n                time_ser_1.values[window:length:tstep], \n                \"b-\", \n                label=fr\"{symbol_1}\")\np2, = ax2.plot(time_ser_1.index[window:length:tstep],\n                MSLZC,\n                'gold', \n                label=fr\"$MSLZC$\")\np3, = ax3.plot(time_ser_1.index[window:length:tstep],\n                MSPLZC,\n                'red', \n                label=fr\"$MSPLZC$\")               \n\nax.set_xlabel(xlabel)\nax.set_ylabel(f\"{symbol_1}\")\nax.yaxis.label.set_color(p1.get_color())\nax2.yaxis.label.set_color(p2.get_color())\nax3.yaxis.label.set_color(p3.get_color())\n\ntkw = dict(size=3, width=1.5)\n\nax.tick_params(axis='x', **tkw)\nax.tick_params(axis='y', colors=p1.get_color(), **tkw)\nax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\nax3.tick_params(axis='y', colors=p3.get_color(), **tkw)\nax3.legend(handles=[p1, p2, p3])\n\nplt.savefig(f\"msplzc_mslzc_name={symbol_1}_ \\\n    window={window}_step={tstep}_ \\\n    rettype={ret_type}_m={m}_tau={tau}.jpg\")\n\nplt.show();\n\n\n\n\n\n\n\nРис. 4.8: Динаміка індексу S&P 500, класичної мультимасштабної складності Лемпеля-Зіва та її пермутаційного аналогу\n\n\n\n\n\nТепер бачимо однозначну картину: обидві міри поводять себе синхронно, та спадають у кризові та передкризові періоди, що вказує на зростання ступеня детермінованості та самоорганізації ринку.\n\n\n4.2.4 Обчислення Шеннонівської ентропії\nЯк уже зазначалося, Шеннонівська ентропія — це міра непередбачуваності стану, або, еквівалентно, його середнього інформаційного вмісту. Ентропія Шеннона є однією з перших і найбільш базових мір ентропії та фундаментальним поняттям теорії інформації.\nРозраховуватимемо її в ковзному вікні:\n\nret_type = 1                      # вид ряду\nwindow = 250                      # ширина вікна\ntstep = 1                         # часовий крок вікна \nlength = len(time_ser_1.values)   # довжина самого ряду  \nlog_base = np.exp(1)      \n\nshannon = []                      # ентропія Шеннона\n\n\nfor i in tqdm(range(0,length-window,tstep)):       # фрагменти довжиною window  \n                                                   # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо ентропію Шеннона\n    p, be = np.histogram(fragm,         # розраховуємо щільність ймовірностей\n                        bins='auto', \n                        density=True)  \n    r = be[1:] - be[:-1]                # знаходимо dx\n    P = p * r                           # представляємо ймовірність як f(x)*dx\n    P = P[P!=0]                         # фільтруємо по всім ненульовим ймовірностям\n\n    sh_ent, _ = nk.entropy_shannon(freq=P, base=log_base) # розраховуємо ентропію \n    sh_ent /= np.log(len(P))                              # та нормалізуємо\n\n    # та додаємо результат до масиву значень\n    shannon.append(sh_ent)\n\n100%|██████████| 2177/2177 [00:00&lt;00:00, 2513.29it/s]\n\n\n\nnp.savetxt(f\"shannon_ent_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}.txt\" , shannon)\n\n\nvalues_plot = time_ser_1.values[window:length:tstep], shannon\nylabels = ylabel_1, \"ShEn\"\nfile_name = f\"shannon_ent_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}\"\n\n\nplot_pair(time_ser_1.index[window:length:tstep], \n            values_plot, xlabel, ylabels, file_name)\n\n\n\n\n\n\n\nРис. 4.9: Динаміка індексу S&P 500 та ентропії Шеннона\n\n\n\n\n\nЯк ми можемо бачити з представленого рисунку, ентропія Шеннона реагує спадом на кризові періоди індексу S&P 500, що вказує на приріст ступеня кореляції системи, її детермінованості.\n\n\n4.2.5 Розрахунок інформаційного показника Фішера\nПерш за все задаємо параметри для розрахунків:\n\nret_type = 1                      # вид ряду\nwindow = 250                      # ширина вікна\ntstep = 1                         # часовий крок вікна \nlength = len(time_ser_1.values)   # довжина самого ряду  \nm = 3                             # розмірність вкладень\ntau = 1                           # часова затримка\n\nfisher = []                       # інформація Фішера\n\n\nfor i in tqdm(range(0,length-window,tstep)):       # фрагменти довжиною window  \n                                                   # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    fish_inf, _ = nk.fisher_information(signal=fragm,\n                                        dimension=m, \n                                        delay=tau) \n\n    # та додаємо результат до масиву значень\n    fisher.append(fish_inf)\n\n100%|██████████| 2177/2177 [00:00&lt;00:00, 5321.53it/s]\n\n\n\nnp.savetxt(f\"fisher_inf_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}_dimension={m}_delay={tau}.txt\", fisher)\n\n\nvalues_plot = time_ser_1.values[window:length:tstep], fisher\nylabels = ylabel_1, \"FI\"\nfile_name = f\"fisher_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}_dimension={m}_delay={tau}\"\n\n\nplot_pair(time_ser_1.index[window:length:tstep], values_plot, xlabel, ylabels, file_name)\n\n\n\n\n\n\n\nРис. 4.10: Динаміка індексу S&P 500 та інформаційного показника Фішера\n\n\n\n\n\nЗ Рис. 4.10 видно, що показник Фішера спадає у кризові та передкризові періоди, що говорить про спад кількості необхідної для опису самоорганізованої динаміки фінансових криз інформації, зростання корельованості між діями трейдерів на ринку.\n\n\n4.2.6 Обчислення часу декореляції\n\nret_type = 1                      # вид ряду\nwindow = 250                      # ширина вікна\ntstep = 1                         # часовий крок вікна \nlength = len(time_ser_1.values)   # довжина самого ряду \n\ndecorrelation_time = []           # час декореляції\n\n\nfor i in tqdm(range(0,length-window,tstep)):       # фрагменти довжиною window  \n                                                   # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    dec_time, _ = nk.complexity_decorrelation(fragm) \n\n    # та додаємо результат до масиву значень\n    decorrelation_time.append(dec_time)\n\n100%|██████████| 2177/2177 [00:00&lt;00:00, 2622.31it/s]\n\n\n\nnp.savetxt(f\"dec_time_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}.txt\", decorrelation_time)\n\n\nvalues_plot = time_ser_1.values[window:length:tstep], decorrelation_time\nylabels = ylabel_1, \"DT\"\nfile_name = f\"dec_time_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}\"\n\n\nplot_pair(time_ser_1.index[window:length:tstep], values_plot, \n            xlabel, ylabels, file_name)\n\n\n\n\n\n\n\nРис. 4.11: Динаміка індексу S&P 500 та часу декореляції\n\n\n\n\n\nЧас декореляції зростає у передкраховий період, що вказує на посилення кореляції системи в цей період.\n\n\n4.2.7 Обчислення відносної шорсткості\n\nret_type = 1                      # вид ряду\nwindow = 250                      # ширина вікна\ntstep = 1                         # часовий крок вікна \nlength = len(time_ser_1.values)   # довжина самого ряду \n\nrelative_roughness = []           # відносна шорсткість\n\n\nfor i in tqdm(range(0,length-window,tstep)): # фрагменти довжиною window  \n                                             # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    rr, _ = nk.complexity_relativeroughness(fragm) \n\n    # та додаємо результат до масиву значень\n    relative_roughness.append(rr)\n\n100%|██████████| 2177/2177 [00:00&lt;00:00, 2783.23it/s]\n\n\n\nnp.savetxt(f\"rel_rough_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}.txt\", relative_roughness)\n\n\nvalues_plot = time_ser_1.values[window:length:tstep], relative_roughness\nylabels = ylabel_1, \"RR\"\nfile_name = f\"rel_rough={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}\"\n\n\nplot_pair(time_ser_1.index[window:length:tstep], values_plot, \n            xlabel, ylabels, file_name)\n\n\n\n\n\n\n\nРис. 4.12: Динаміка індексу S&P 500 та показника відносної шорсткості\n\n\n\n\n\nПоказник відносної шорсткості демонструє, що крахові події як, наприклад, у 2015, 2016, 2019, 2020 та 2023 роках характеризуються зростанням шорсткості. Подібного роду поведінка є індикатором зростання шумової активності ринку: кореляційних характеристик та загальної варіації ринку в цілому.\n\n\n4.2.8 Розрахунок показників складності Хьорта\nЗавершуємо хід роботи показниками складності Хьорта:\n\nret_type = 1                      # вид ряду\nwindow = 250                      # ширина вікна\ntstep = 1                         # часовий крок вікна \nlength = len(time_ser_1.values)   # довжина самого ряду \n\nactivity = []                     # параметр активності\nmobility = []                     # параметр рухливості\ncomplexity = []                   # параметр складності\n\n\nfor i in tqdm(range(0,length-window,tstep)): # фрагменти довжиною window  \n                                             # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser_1.iloc[i:i+window].copy()   \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо показники складності Хьорта\n    cmpl, info = nk.complexity_hjorth(fragm) \n\n    # та додаємо результат до масиву значень\n    activity.append(info['Activity'])\n    mobility.append(info['Mobility'])\n    complexity.append(cmpl)\n\n 61%|██████    | 1332/2177 [00:00&lt;00:00, 4349.96it/s]100%|██████████| 2177/2177 [00:00&lt;00:00, 4353.02it/s]\n\n\n\nnp.savetxt(f\"activity_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}.txt\", activity)\nnp.savetxt(f\"mobility_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}.txt\", mobility)\nnp.savetxt(f\"complexity_name={symbol_1}_window={window}_ \\\n    step={tstep}_rettype={ret_type}.txt\", complexity)    \n\n\nfig, ax = plt.subplots(1, 1)\n\nax2 = ax.twinx()\nax3 = ax.twinx()\nax4 = ax.twinx()\n\nax2.spines.right.set_position((\"axes\", 1.03))\nax3.spines.right.set_position((\"axes\", 1.16))\nax4.spines.right.set_position((\"axes\", 1.24))\n\np1, = ax.plot(time_ser_1.index[window:length:tstep], \n              time_ser_1.values[window:length:tstep], \n              \"b-\", label=fr\"{ylabel_1}\")\np2, = ax2.plot(time_ser_1.index[window:length:tstep], \n               activity, \"r--\", label=r\"$Act$\")\np3, = ax3.plot(time_ser_1.index[window:length:tstep], \n               mobility, \"g-\", label=r\"$Mob$\")\np4, = ax4.plot(time_ser_1.index[window:length:tstep],\n               complexity, \"m-\", label=r\"$Comp$\")\n\nax.set_xlabel(xlabel)\nax.set_ylabel(f\"{ylabel_1}\")\nax.yaxis.label.set_color(p1.get_color())\nax2.yaxis.label.set_color(p2.get_color())\nax3.yaxis.label.set_color(p3.get_color())\nax4.yaxis.label.set_color(p4.get_color())\n\ntkw = dict(size=4, width=1.5)\n\nax.tick_params(axis='y', colors=p1.get_color(), **tkw)\nax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\nax3.tick_params(axis='y', colors=p3.get_color(), **tkw)\nax4.tick_params(axis='y', colors=p4.get_color(), **tkw)\nax.tick_params(axis='x', **tkw)\nax4.legend(handles=[p1, p2, p3, p4])\n\nplt.savefig(f\"hjorth_name={symbol_1}_ret={ret_type}_wind={window}_step={tstep}.jpg\")\nplt.show();\n\n\n\n\n\n\n\nРис. 4.13: Динаміка індексу S&P500 наряду з показниками активності, мобільності та складності Хьорта\n\n\n\n\n\nОчевидно, що параметр активності (\\(Act\\)) представляється найменш інформативним, оскільки він вказує тільки на зростання сукупної дисперсії сигналу. Активність почала помітно зростати напередодні 2022 року, але для попередніх кризових станів ми не бачимо передвісницької поведінки цього індикатора. Питання передчасної ідентифікації наростання кризового явища найкраще вирішує показник мобільності (\\(Mob\\)). Ми бачимо, що даний показник зростає під час 2015-2016 років, напередодні 2019 року, при настанні коронавірусної пандемії, перед 2023 та 2024 роками. Показник складності Хьорта (\\(Comp\\)) реагує асиметричним чином: у той час коли мобільність зростає, показник складності спадає, вказуючи на те, що система прагне до вищого ступеня періодичності або корельованості.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Лабораторна робота № 4</span>"
    ]
  },
  {
    "objectID": "lab_4.html#висновок",
    "href": "lab_4.html#висновок",
    "title": "4  Лабораторна робота № 4",
    "section": "4.3 Висновок",
    "text": "4.3 Висновок\nТаким чином, розглянуті інформаційні міри складності дозволяють дослідити певні аспекти складності систем будь-якої природи. Особливо продуктивним являється мультимасштабна версія введених мір. Ретельний аналіз часових рядів для систем різної природи, різного рівня складності, порівняння їх із тестовими сигналами, вивчення поведінки систем у різних (не обов’язково рівноважних, стаціонарних) умовах дозволить зрозуміти природу складності і спрогнозувати можливу поведінку систем у критичних умовах. Так, порівняння вихідного часового ряду з відповідними мірами складності свідчить про очевидне їх реагування на кризові явища. Однак питання використання їх у якості передвісників вимагає додаткових досліджень.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Лабораторна робота № 4</span>"
    ]
  },
  {
    "objectID": "lab_4.html#завдання-для-самостійної-роботи",
    "href": "lab_4.html#завдання-для-самостійної-роботи",
    "title": "4  Лабораторна робота № 4",
    "section": "4.4 Завдання для самостійної роботи",
    "text": "4.4 Завдання для самостійної роботи\n\nДослідіть і порівняйте результати для фінансових рядів, що представляють розвинені компанії (країни, криптовалюти) і такі, що розвиваються. Порівняйте результати. Поясніть, в чому їх схожість та відмінності\nЯким чином поводять себе міри складності у період фінансових шоків і криз?\nНаскільки чутливими є результати розрахунків до вибору ширини вікна та кроку?\n\n\n\n\n\n[1] M. S. Kanwal, J. A. Grochow, and N. Ay, Comparing Information-Theoretic Measures of Complexity in Boltzmann Machines, Entropy 19, (2017).\n\n\n[2] A. N. Kolmogorov, Three Approaches to the Quantitative Definition of Information, International Journal of Computer Mathematics 2, 157 (1968).\n\n\n[3] D. G. Bonchev, Information Theoretic Complexity Measures, in Encyclopedia of Complexity and Systems Science, edited by R. A. Meyers (Springer New York, New York, NY, 2009), pp. 4820–4839.\n\n\n[4] L. T. Lui, G. Terrazas, H. Zenil, C. Alexander, and N. Krasnogor, Complexity Measurement Based on Information Theory and Kolmogorov Complexity, Artificial Life 21, 205 (2015).\n\n\n[5] M. Li and P. Vitányi, Preliminaries, in An Introduction to Kolmogorov Complexity and Its Applications (Springer New York, New York, NY, 2008), pp. 1–99.\n\n\n[6] C. E. Shannon, A Mathematical Theory of Communication, Bell System Technical Journal 27, 379 (1948).\n\n\n[7] A. Lempel and J. Ziv, On the Complexity of Finite Sequences, IEEE Transactions on Information Theory 22, 75 (1976).\n\n\n[8] J.-L. Blanc, L. Pezard, and A. Lesne, Delay Independence of Mutual-Information Rate of Two Symbolic Sequences, Phys. Rev. E 84, 036214 (2011).\n\n\n[9] S. Zozor, P. Ravier, and O. Buttelli, On Lempel–Ziv Complexity for Multidimensional Data Analysis, Physica A: Statistical Mechanics and Its Applications 345, 285 (2005).\n\n\n[10] E. Estevez-Rams, R. Lora Serrano, B. Aragón Fernández, and I. Brito Reyes, On the non-randomness of maximum Lempel Ziv complexity sequences of finite size, Chaos: An Interdisciplinary Journal of Nonlinear Science 23, 023118 (2013).\n\n\n[11] R. Giglio, R. Matsushita, A. Figueiredo, I. Gleria, and S. D. Silva, Algorithmic Complexity Theory and the Relative Efficiency of Financial Markets, Europhysics Letters 84, 48005 (2008).\n\n\n[12] C. Taufemback, R. Giglio, and S. D. Silva, Algorithmic complexity theory detects decreases in the relative efficiency of stock markets in the aftermath of the 2008 financial crisis, Economics Bulletin 31, 1631 (2011).\n\n\n[13] R. Giglio and S. Da Silva, Ranking the Stocks Listed on Bovespa According to Their Relative Efficiency, MPRA Paper, University Library of Munich, Germany, 2009.\n\n\n[14] Y. Bai, Z. Liang, and X. Li, A Permutation Lempel-Ziv Complexity Measure for EEG Analysis, Biomedical Signal Processing and Control 19, 102 (2015).\n\n\n[15] M. Borowska, Multiscale Permutation Lempel–Ziv Complexity Measure for Biomedical Signal Analysis: Interpretation and Application to Focal EEG Signals, Entropy 23, (2021).\n\n\n[16] B. K. Hillen, G. T. Yamaguchi, J. J. Abbas, and R. Jung, Joint-Specific Changes in Locomotor Complexity in the Absence of Muscle Atrophy Following Incomplete Spinal Cord Injury, Journal of NeuroEngineering and Rehabilitation 10, 1 (2013).\n\n\n[17] M. D. Costa, C.-K. Peng, and A. L. Goldberger, Multiscale Analysis of Heart Rate Dynamics: Entropy and Time Irreversibility Measures, Cardiovascular Engineering 8, 88 (2008).\n\n\n[18] R. A. Fisher and E. J. Russell, On the Mathematical Foundations of Theoretical Statistics, Philosophical Transactions of the Royal Society of London. Series A, Containing Papers of a Mathematical or Physical Character 222, 309 (1922).\n\n\n[19] B. Hjorth, EEG Analysis Based on Time Domain Properties, Electroencephalography and Clinical Neurophysiology 29, 306 (1970).\n\n\n[20] F. Mormann, T. Kreuz, C. Rieke, R. G. Andrzejak, A. Kraskov, P. David, C. E. Elger, and K. Lehnertz, On the Predictability of Epileptic Seizures, Clinical Neurophysiology 116, 569 (2005).\n\n\n[21] V. Marmelat, K. Torre, and D. Delignieres, Relative Roughness: An Index for Testing the Suitability of the Monofractal Model, Frontiers in Physiology 3, (2012).\n\n\n[22] T. M. Cover, Elements of Information Theory (John Wiley & Sons, 1999).\n\n\n[23] A. Hacine-Gharbi and P. Ravier, A Binning Formula of Bi-Histogram for Joint Entropy Estimation Using Mean Square Error Minimization, Pattern Recognition Letters 101, 21 (2018).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Лабораторна робота № 4</span>"
    ]
  },
  {
    "objectID": "lab_5.html",
    "href": "lab_5.html",
    "title": "5  Лабораторна робота № 5",
    "section": "",
    "text": "5.1 Теоретичні відомості\nПитання динаміки розвитку і функціонування складних систем може розглядатись у двох варіантах:\nОстанніми роками було використано кілька підходів для ідентифікації механізмів, що лежать в основі еволюції складних. Особливо корисні результати було отримано при їх дослідженні методами теорії випадкових матриць, моно- та мультифрактального аналізу, теорії хаосу з реконструкцією траєкторії системи у фазовому просторі, рекурентного аналізу тощо. Ми розглянули ці методи у попередніх роботах. Однак, застосування деяких із методів висуває вимоги до стаціонарності досліджуваних даних, потребує довгих часових рядів та комплексного обчислення кількох параметрів.\nІншим широко відомим підходом моделювання особливостей складних систем є обчислення характеристик різних видів ентропії.\nКонцепція термодинамічної ентропії як міри хаосу системи добре відома у фізиці, однак, останніми роками поняття ентропії було застосоване до складних систем інших об’єктів (біологічних, економічних, соціальних тощо). Так, один із найбільш часто використовуваних методів визначення ентропії базується на обчисленні спектру потужності Фур’є та застосовується для вивчення часових рядів різної природи. Проте, використання дискретного перетворення Фур’є для аналізу часових рядів має свої недоліки, зокрема, на результати впливає нестаціонарність рядів, варіювання їх довжини від сотень до сотень тисяч, та обмеження самого методу (незмінність частотно-часових характеристик протягом всього часу функціонування системи). Тому виникає питання про розрахунок значень ентропії за допомогою інших методів.\nВведемо поняття ентропії, скориставшись інформацією, яку можна знайти у Вікіпедії.\nТермодинамічна ентропія \\(S\\), часто просто іменована ентропія, в хімії і термодинаміці є мірою кількості енергії у фізичній системі, яка не може бути використана для виконання роботи. Вона також є мірою безладдя, присутнього в системі.\nПоняття ентропії була вперше введено у 1865 році Рудольфом Клаузіусом  [1]. Він визначив зміну ентропії термодинамічної системи при оборотному процесі як відношення зміни загальної кількості тепла \\(\\Delta Q\\) до величини абсолютної температури \\(T\\):\n\\[\n\\Delta S = \\Delta Q / T.\n\\]\nРудольф Клаузіус дав величині \\(S\\) ім’я “ентропія”, що походить від грецького слова τρoπή, “зміна” (зміна, перетворення).\nУ 1877 році, Людвіг Больцман  [2] зрозумів, що ентропія системи може відноситися до кількості можливих “мікростанів” (мікроскопічних станів), що узгоджуються з їх термодинамічними властивостями. Розглянемо, наприклад, ідеальний газ у посудині. Мікростан визначений як позиції і імпульси кожного атома, що становить систему. Зв’язність пред’являє до нас вимоги розглядати тільки ті мікростани, для яких: (i) місце розташування всіх частин обмежене границями судини, (ii) для отримання загальної енергії газу кінетичні енергії атомів підсумовуються. Больцман постулював що\n\\[\nS = k_{B}\\ln{\\Omega},\n\\]\nде константу \\(k_{B} = 1,38 \\cdot 10^{-23} Дж/К\\) ми знаємо тепер як сталу Больцмана, a \\(\\Omega\\) є числом мікростанів, які можливі в наявному макроскопічному стані. Цей постулат, відомий як принцип Больцмана, може бути оцінений як початок статистичної механіки, що описує термодинамічні системи з використанням статистичної поведінки компонентів. Принцип Больцмана зв’язує мікроскопічні властивості системи (\\(\\Omega\\)) з однією з її термодинамічних властивостей (\\(S\\)).\nЗгідно визначенню Больцмана, ентропія є просто функцією стану. Більш того, оскільки (\\(\\Omega\\)) може бути тільки натуральним числом, ентропія повинна бути додатною — виходячи з властивостей логарифма.\nУ випадку дискретних станів квантової механіки кількість станів підраховується звичайним чином. У рамках класичної механіки мікроскопічний стан системи описується координатами \\(q_{i}\\) й імпульсами \\(p_{i}\\) окремих частинок, які пробігають неперервні значення. У такому випадку\n\\[\nS = k_{B}\\ln\\frac{1}{( 2\\pi\\hbar )^{s}} \\int \\prod_{i=1}^{s} dq_{i}dp_{i},\n\\]\nде \\(s\\) — число незалежних координат, \\(\\hbar\\) — приведена стала Планка, а інтегрування проводиться по області фазового простору, який відповідає певному макроскопічному стану.\nКлод Шеннон  [3] запропонував формулу для оцінки невизначеності кодової інформації в каналах зв’язку, звану ентропією Шеннона:\n\\[\nS = -k\\sum_{i=1}^{n}p_{i}\\ln{p_{i}},\n\\]\nде \\(p_{i}\\) — вірогідність того, що символ \\(i\\) зустрічається в коді, який містить \\(N\\) символів, \\(k\\) — розмірний множник.\nЗв’язок між ентропією і інформацією можна прослідкувати на наступному прикладі. Розглянемо тіло при абсолютному нулі температури, і хай ми маємо повну інформацію про координати і імпульси кожної частинки. Для простоти покладемо, що імпульси всіх частинок рівні нулю. В цьому випадку термодинамічна ймовірність рівна одиниці, а ентропія — нулю. При кінцевих температурах ентропія в рівновазі досягає максимуму. Можна зміряти всі макропараметри, що характеризують даний макростан. Проте ми практично нічого не знаємо про мікростан системи. Точніше кажучи, ми знаємо, що даний макростан можна реалізувати за допомогою дуже великого числа мікростанів. Таким чином, нульовій ентропії відповідає повна інформація (ступінь незнання рівний нулю), а максимальної ентропії — повне незнання мікростанів (ступінь незнання максимальний).\nУ теорії інформації ентропія (інформаційна ентропія) визначається як кількість інформації. Нехай \\(P\\) — апріорна вірогідність деякої події (ймовірність до проведення досвіду), а \\(P_{1}\\) — ймовірність цієї події після проведення досвіду. Для простоти вважатимемо, що \\(P_{1} = 1\\). За Шенноном, кількість інформації \\(I\\), яка дає точну відповідь (після проведення експерименту)\n\\[\nI = K \\log{P}.\n\\]\nЦя кількість інформації, за визначенням, дорівнює одному біту.\nФізичний сенс \\(I\\) — це міра нашого незнання. Іншими словами, \\(I\\) — це та інформація, яку ми можемо одержати, вирішивши завдання. У прикладі (тіло при абсолютному нулі температури), що розглядається вище, міра нашого незнання рівна нулю, оскільки \\(P = 1\\). Після проведення досвіду ми одержуємо нульову інформацію \\(I = 0\\), оскільки все було відомо до досвіду. Якщо розглядати тіло при кінцевих температурах, то до проведення досвіду число мікростанів, а отже, і \\(P\\) дуже велике. Після проведення досвіду ми одержуємо велику інформацію, оскільки нам стають відомими координати і імпульси всіх частинок.\nАналогія між кількістю інформації і ентропією \\(S\\), визначуваною з принципу Больцмана, очевидна. Досить покласти множник \\(K\\) рівним постійній Больцмана \\(k_{B}\\) і використовувати натуральний логарифм. Саме з цієї причини величину \\(I\\) називають інформаційною ентропією. Інформаційна ентропія (кількість інформації) була визначена по аналогії із звичайною ентропією, і вона має властивості, характерні для звичайній ентропії: адитивність, екстремальні властивості і т.д. Проте ототожнювати звичайну ентропію з інформаційною не можна, оскільки неясно, яке відношення має другий закон термодинаміки до інформації. Нагадаємо, що екстенсивна величина — ця така характеристика системи, яка росте зі збільшенням розмірів системи, тобто, якщо наша система складається з двох незалежних підсистем \\(А\\) і \\(В\\), то ентропію всієї системи можна одержати складанням ентропій підсистем:\n\\[\nS(A + B) = S(A) + S(B).\n\\]\nСаме ця властивість і означає екстенсивність, або адитивність, ентропії.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Лабораторна робота № 5</span>"
    ]
  },
  {
    "objectID": "lab_5.html#теоретичні-відомості",
    "href": "lab_5.html#теоретичні-відомості",
    "title": "5  Лабораторна робота № 5",
    "section": "",
    "text": "як дослідження шумової активності;\nяк детерміністичного випадку з певним ступенем порядку.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Лабораторна робота № 5</span>"
    ]
  },
  {
    "objectID": "lab_5.html#хід-роботи",
    "href": "lab_5.html#хід-роботи",
    "title": "5  Лабораторна робота № 5",
    "section": "5.2 Хід роботи",
    "text": "5.2 Хід роботи\nРозглянемо, яким чином ми використовувати ентропійні показники в якості індикаторів або індикаторів-передвісників кризових подій. Перш за все імпортуємо та встановимо необхідні модулі для подальшої роботи:\n\n!pip install EntropyHub\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport yfinance as yf\nimport neurokit2 as nk\nimport EntropyHub as eh\nimport warnings\nimport scienceplots\nfrom tqdm import tqdm\n\nwarnings.filterwarnings('ignore')\n\nДалі виконаємо налаштування формату виведення рисунків:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nsize = 16\nparams = {\n    'figure.figsize': (8, 6),            # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': size,                   # розмір фонтів рисунку\n    'lines.linewidth': 2,                # товщина ліній\n    'axes.titlesize': 'small',           # розмір титулки над рисунком\n    'axes.labelsize': size,              # розмір підписів по осям\n    'legend.fontsize': size,             # розмір легенди\n    'xtick.labelsize': size,             # розмір розмітки по осі Ох\n    'ytick.labelsize': size,             # розмір розмітки по осі Ох\n    \"font.family\": \"Serif\",              # сімейство стилів підписів \n    \"font.serif\": [\"Times New Roman\"],   # стиль підпису\n    'savefig.dpi': 300,                  # якість збережених зображень\n    'axes.grid': False                   # побудова сітки на самому рисунку\n}\n\nplt.rcParams.update(params)              # оновлення стилю згідно налаштувань\n\nУ даній роботі виконаємо розрахунки на прикладі одного з найважливіших фондових індексів Японії — Nikkei 225. Індекс обчислюється шляхом визначення простого середнього арифметичного значення цін акцій 225 провідних компаній, які входять до першої секції Токійської фондової біржі. Для отримання значень індексу скористаємось бібліотекою yfinance. Будемо розглядати значення до 1-го грудня 2023 року.\n\nsymbol = '^N225'                      # Символ індексу\n\nend = \"2023-12-01\"                    # Дата закінчення зчитування даних\ndata = yf.download(symbol, end=end)   # вивантажуємо дані\ntime_ser = data['Adj Close'].copy()   # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'                 # підпис по вісі Ох \nylabel = symbol                       # підпис по вісі Оу\n\nnp.savetxt(f'{symbol}_initial_time_series.txt', time_ser.values)\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nУвага\n\n\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance, а із власного файлу. Зрозуміло, що й аналіз результатів, і висновки залежать від того з яким рядом ми працюємо\n\n\n\n\nsymbol = 'sMpa11'                  # Символ індексу\n\npath = \"databases\\sMpa11.txt\"      # шлях по якому здійснюється зчитування файлу\ndata = pd.read_csv(path,           # зчитування даних \n                   names=[symbol])\ntime_ser = data[symbol].copy()     # копіюємо значення кривої \n                                   # \"напруга-видовження\" до окремої змінної\n\nxlabel = r'$\\varepsilon$'          # підпис по вісі Ох \nylabel = symbol                    # підпис по вісі Оу\n\n\nВиводимо досліджуваний ряд:\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\n\n\n\nРис. 5.1: Динаміка щоденних змін фондового індексу N225\n\n\n\n\n\nДля приведення ряду до стандартизованого вигляду або прибутковостей визначимо функцію transformations():\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\nДля побудови пари часових рядів визначимо функцію plot_pair():\n\ndef plot_pair(x_values, \n              y1_values,\n              y2_values,  \n              y1_label, \n              y2_label,\n              x_label, \n              file_name, clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(x_values, \n                  y1_values, \n                  \"b-\", label=fr\"{y1_label}\")\n    p2, = ax2.plot(x_values,\n                   y2_values, \n                   color=clr, \n                   label=y2_label)\n\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(f\"{y1_label}\")\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', rotation=35, **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(file_name + \".jpg\")\n        \n    plt.show();\n\n\n5.2.1 Апроксимаційна ентропія\nЕнтропія подібності (Approximate Entropy, ApEn) є “статистикою регулярності”  [4,5], що визначає можливість передбачувати флуктуації в часових рядах  [6]. Інтуїтивно вона означає, що наявність повторюваних шаблонів (послідовностей певної довжини, побудованих із чисел ряду, що слідують одне за іншим) флуктуацій у часовому ряді призводить до більшої передбачуваності часового ряду порівняно із рядами, де повторюваності шаблонів немає. Порівняно велике значення ApEn показує ймовірність того, що подібні між собою шаблони спостережень не будуть слідувати один за одним. Іншими словами, часовий ряд, що містить велику кількість повторюваних шаблонів, має порівняно мале значення ApEn, а значення ApEn для менш передбачуваного (більш складного) процесу є більшим.\nПри розрахунку ApEn для даного часового ряду \\(S_{N}\\), що складається із \\(N\\) значень \\(t(1),t(2),t(3),...,t(N)\\,\\) вибираються два параметри, \\(m\\) та \\(r\\). Перший з цих параметрів, \\(m\\), вказує на довжину шаблона, а другий — \\(r\\) — визначає критерій подібності. Досліджуються підпослідовності елементів часового ряду \\(S_{N}\\), що складаються з \\(m\\) чисел, взятих, починаючи з номера \\(i\\), і називаються векторами \\(p_{m} (i)\\). Два вектори (шаблони), \\(p_{m}(i)\\) та \\(p_{m}(j)\\), будуть подібними, якщо всі різниці пар їх відповідних координат є меншими за значення \\(r\\), тобто якщо\n\\[\n| t(i+k) - t(j+k) | &lt; r \\quad \\textrm{для} \\quad 0 \\leq k &lt; m.\n\\]\nДля розглядуваної множини \\(P_{m}\\) всіх векторів довжини \\(m\\) часового ряду \\(S_{N}\\) можна розрахувати значення\n\\[\nC_{im}(r) = n_{im}(r) \\Big/ (N-m+1),\n\\]\nде \\(n_{im}(r)\\) — кількість векторів у \\(P_{m}\\), що подібні вектору \\(p_{m}(i)\\) (враховуючи вибраний критерій подібності \\(r\\)). Значення \\(C_{im}(r)\\) є часткою векторів довжини \\(m\\), що мають схожість із вектором такої ж довжини, елементи якого починаються з номера \\(i\\). Для даного часового ряду обраховуються значення \\(C_{im}(r)\\) для кожного вектора у \\(P_{m}\\), після чого знаходиться середнє значення \\(C_{m}(r)\\), що відображає розповсюдженість подібних векторів довжини \\(m\\) у ряду \\(S_{N}\\). Безпосередньо ентропія подібності для часового ряду \\(S_{N}\\) з використанням векторів довжини \\(m\\) та критерію подібності \\(r\\) визначається за формулою:\n\\[\nApEn(S_{N}, m, r) = \\ln{C_{m}(r)} - \\ln{C_{m+1}(r)}.\n\\]\nТобто, як натуральний логарифм відношення повторюваності векторів довжиною \\(m\\) до повторюваності векторів довжиною \\(m+1\\).\nТаким чином, якщо знайдуться подібні вектори у часовому ряді, ApEn оцінить логарифмічну ймовірність того, що наступні інтервали після кожного із векторів будуть відрізнятись. Менші значення ApEn відповідають більшій ймовірності того, що за векторами слідують подібні їм. Якщо часовий ряд дуже нерегулярний — наявність подібних векторів не може бути передбачуваною і значення ApEn є порівняно великим.\nЗауважимо, що ApEn є нестійкою до вхідних даних характеристикою, оскільки досить сильно залежить від параметрів \\(m\\) та \\(r\\).\n\nwindow = 500                   # ширина вікна\ntstep = 5                      # часовий крок\n\nm = 3                          # розмірність вкладень\ntau = 1                        # часова затримка\nr = 0.45                       # параметр подібності\n\nret_type = 4                   # вид ряду: \n                               # 1 - вихідний, \n                               # 2 - детрендований (різниця між теп. значенням та попереднім)\n                               # 3 - прибутковості звичайні, \n                               # 4 - стандартизовані прибутковості, \n                               # 5 - абсолютні значення (волатильності)\n                               # 6 - стандартизований ряд \n\nlength = len(time_ser.values)  # довжина самого ряду\n\nApEn = []                      # масив для зберігання значень ентропії\n\n\nfor i in tqdm(range(0,length-window,tstep)): # фрагменти довжиною window  \n                                             # з кроком tstep\n    \n    # відбираємо фрагмент\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n    \n    # розраховуємо апроксимаційну ентропію\n    Ap, _ = nk.entropy_approximate(signal=fragm, \n                                    dimension=m,\n                                    delay=tau, \n                                    tolerance=r,\n                                    corrected=False)\n    ApEn.append(Ap)\n\n100%|██████████| 2798/2798 [00:15&lt;00:00, 182.49it/s]\n\n\nЗберігаємо значення апроксимаційної ентропії до текстового файлу:\n\nnp.savetxt(f\"ApEn_name={symbol}_window={window}_step={tstep}_\\\n           dim={m}_tau={tau}_radius={r}_sertype={ret_type}.txt\", ApEn)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_apen = fr'$ApEn$'\n\nfile_name_apen = f\"ApEn_name={symbol}_window={window}_step={tstep}_\\\n           dim={m}_tau={tau}_radius={r}_sertype={ret_type}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          ApEn, \n          ylabel, \n          label_apen,\n          xlabel,\n          file_name_apen)\n\n\n\n\n\n\n\nРис. 5.2: Динаміка фондового індексу N225 та апроксимаційної ентропії\n\n\n\n\n\nЯк можна бачити з представленого рисунку (Рис. 5.2), апроксимаційна ентропія падає у кризові та передкризові моменти часу. Це говорить те, що середнє значення кореляційного інтегралу отриманого для фазового простору розмірністю \\(d_{E+1}\\) не сильно відрізняється від того, що було отримано для фазового простору розмірністю \\(d_{E}\\). Тобто, при реконструкції простору в різних вимірах, усі точки просто знаходяться достатньо близько один до одного, не дивлячись на геометричні перетворення атрактора фондового ринку. Це вказує на досить високий ступінь кореляцій цінових флуктуацій індексу N225 і спрямованість трейдерів ринку по одному тренду.\n\n\n5.2.2 Нечітка ентропія\nОднією з модифікацій ентропії Шеннона є нечітка ентропія (Fuzzy entropy, FuzzEn)  [7–9]. Цей підхід виключає самоподібність між досліджуваними векторами, і замість функції Гевісайда, яка видає або 0, або 1 для схожих векторів, використовується нечітка функція належності, яка у випадку FuzzEn буде асоціювати схожість між двома векторами з реальним значенням у діапазоні \\([0, 1]\\). Різницю можна побачити на етапі побудови вектора вкладень, де для реконструйованих векторів ми виконуємо детрендування:\n\\[\n\\vec{X}(i) = \\left\\{ x(i), x(i+1),..., x(i+d_E - 1) - x_{0}(i) \\right\\}, \\,\\, i=1,...,N-d_{E}+1,\n\\]\n\\(x_{0}(i) = (d_{E})^{-1} \\sum_{j=0}^{d_{E}-1}x(i+j)\\). Далі, для послідовних вбудованих векторів знаходиться відстань\n\\[\nd\\left[ \\vec{X}(i), \\vec{X}(j) \\right] = \\max{\\left| \\vec{X}(i) - \\vec{X}(j) \\right|}, i\\geq 1, \\, j \\leq N - d_E + 1.\n\\]\nУ класичній ApEn значення відстаней пропускаються через функцію Гевісайда. Нечітка модифікація використовує функції належності для вимірювання належності однієї траєкторії до іншої:\n\\[\nD_{i, j} = \\mu\\left( d\\left[\\vec{X}(i), \\vec{X}(j) \\right]\\right),\n\\]\n\\(\\mu = \\exp\\left( -x^{r_2} \\big/ r_1 \\right)\\), а \\(r_1\\) і \\(r_2\\) ширина та градієнт експоненціальної функції.\nДалі, обчислюється наступна функція, що подібна до кореляційного інтегралу в класичній ApEn:\n\\[\n\\phi^{d_E} = \\frac{1}{(N-d_{E}+1)}\\sum_{i=1}^{N-d_{E}+1}\\left( \\frac{1}{N-d_{E}}\\sum_{j=1, j \\neq i}^{N-d_{E}} D_{i, j} \\right).\n\\]\nНарешті,\n\\[\nFuzzEn(\\vec{X}, d_{E}) = -\\left[ \\ln{\\phi^{d_{E}+1} - \\ln{\\phi^{d_{E}}}} \\right].\n\\]\n\nwindow = 500                    # ширина вікна\ntstep = 5                       # часовий крок\n\nm = 3                           # розмірність вкладень\ntau = 1                         # часова затримка\n\ncharacteristic_func = \"default\" # вид функції приналежності: \n                                # default, \n                                # sigmoid, \n                                # gudermannian, \n                                # linear\n\nr = (0.4, 2.0)                  # параметри, що подаються до функції приналежності: \n                                # для 'default' та 'sigmoid' - два значення r, \n                                # для gudermannian та linear - 1 значення r, \n  \nret_type = 4                    # вид ряду: \n                                # 1 - вихідний \n                                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                                # 3 - прибутковості звичайні \n                                # 4 - стандартизовані прибутковості \n                                # 5 - абсолютні значення (волатильності)\n                                # 6 - стандартизований ряд \n\nlength = len(time_ser.values)  # довжина самого ряду\n\nFuzzEn = [] #масив для зберігання значень ентропії\n\n\nfor i in tqdm(range(0,length-window,tstep)):  # фрагменти довжиною window  \n                                              # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # обчислення нечіткої ентропії \n    Fuzz, _, _ = eh.FuzzEn(Sig=fragm, m=m, tau=tau, Fx=characteristic_func, r=r)  \n    FuzzEn.append(Fuzz[-1]) # дожаємо розрахованє значення до масиву значень \n\n100%|██████████| 2798/2798 [02:05&lt;00:00, 22.30it/s]\n\n\nЗберігаємо значення нечіткої ентропії до текстового файлу:\n\nnp.savetxt(f\"FuzzEn_name={symbol}_window={window}_step={tstep}_\\\n        dim={m}_tau={tau}_radius={r}_sertype={ret_type}_\\\n        memberfunc={characteristic_func}.txt\", FuzzEn)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_fuzzen = fr'$FuzzEn$'\n\nfile_name_fuzzen = f\"FuzzEn_name={symbol}_window={window}_step={tstep}_\\\n        dim={m}_tau={tau}_radius={r}_sertype={ret_type}_\\\n        memberfunc={characteristic_func}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          FuzzEn, \n          ylabel, \n          label_fuzzen,\n          xlabel,\n          file_name_fuzzen,\n          clr='red')\n\n\n\n\n\n\n\nРис. 5.3: Динаміка фондового індексу N225 та нечіткої ентропії\n\n\n\n\n\nРис. 5.3 демонструє спадку динаміку нечіткої ентропії в кризові та передкризові періоди, яка працює по аналогії до апроксимаційної ентропії. Із цього випливає, що нечітка ентропія також вказує на зростання корельованості системи та зростання її трендостійкості через однонаправленність думок трейдерів щодо подальшої динаміки фондового ринку.\n\n\n5.2.3 Ентропія шаблонів\nПри розрахунку ApEn беруться до уваги подібності певного вектора \\(p_n(i)\\) до самого себе, що використовується для уникнення можливого значення \\(\\ln{0}\\) при відсутності подібних до даного векторів. Однак, вказана особливість призводить до нівелювання двох важливих характеристик у ентропії подібності:\n\nApEn сильно залежить від довжини розглядуваного шаблона (вектора) і є нижчою, ніж очікується, для векторів малої розмірності;\nApEn не враховує відносну щільність даних.\n\nЦе означає, що коли значення ApEn для одного ряду є більшим, ніж для іншого, то воно повинно залишатись таким (проте не є) для будь-яких можливих початкових умов. Такий висновок тим більш важливий, оскільки ApEn рекомендується в якості міри порівняння двох наборів даних різними авторами.\nВраховуючи вказані обмеження, розроблена для розрахунку інша характеристика, — ентропія шаблонів (Sample Entropy, SampEn)  [10].\nПри розрахунку SampEn, на відміну від алгоритму ApEn, додаються дві умови:\n\nне враховується подібність вектора самому собі;\nпри розрахунку значень умовних ймовірностей SampEn не використовується довжина векторів.\n\nНа основі аналізу вищезазначеного можна зробити висновок про те, що SampEn:\n\nбільше, ніж ApEn, відповідає теорії випадкових чисел для ряду із відомою функцією щільності розподілу;\nзберігає відносну щільність, в той час як ApEn втрачає дану характеристику;\nдодає значно меншу помилку до розрахованого значення у випадку використання векторів малої розмірності.\n\n\nwindow = 500    # ширина вікна\ntstep = 5       # часовий крок\n\nm = 3           # розмірність вкладень\ntau = 1         # часова затримка\nr = 0.4         # параметр подібності\n\nret_type = 4    # вид ряду: \n                # 1 - вихідний\n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні \n                # 4 - стандартизовані прибутковості \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд  \n\nlength = len(time_ser.values)  # довжина самого ряду\n\nSampEn = []     # масив для зберігання значень ентропії шаблонів\n\n\nfor i in tqdm(range(0,length-window,tstep)):  # фрагменти довжиною window  \n                                              # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n    \n    # обчислення ентропії шаблонів\n    Samp, _ = nk.entropy_sample(signal=fragm, \n                                dimension=m, \n                                delay=tau, \n                                tolerance=r)\n    SampEn.append(Samp)\n\n100%|██████████| 2798/2798 [00:14&lt;00:00, 188.79it/s]\n\n\nЗберігаємо значення ентропії шаблонів до текстового файлу:\n\nnp.savetxt(f\"SampEn_name={symbol}_window={window}_step={tstep}_\\\n        dim={m}_tau={tau}_radius={r}_sertype={ret_type}.txt\", SampEn)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_sampen = fr'$SampEn$'\n\nfile_name_sampen = f\"SampEn_name={symbol}_window={window}_step={tstep}_\\\n        dim={m}_tau={tau}_radius={r}_sertype={ret_type}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          SampEn, \n          ylabel, \n          label_sampen,\n          xlabel,\n          file_name_sampen,\n          clr='darkgreen')\n\n\n\n\n\n\n\nРис. 5.4: Динаміка фондового індексу N225 та ентропії шаблонів\n\n\n\n\n\nРис. 5.4 демонструє, що ентропія шаблонів спадає в передкризові періоди фондового ринку, що вказує на зростання корельованості траєкторій реконструйованого фазового простору індексу N225. Це говорить про те, що ринок у передкрахові періоди стає більш упорядкованим і трендостійким.\n\n\n5.2.4 Ентропія перестановок\nЕнтропія перестановок (Permutation entropy, PEn) — це міра з теорії хаосу, запропонована Бандтом і Помпе  [11], і характеризується концептуальною простотою і швидкістю обчислень. Ідея PEn базується на звичайній ентропії Шеннона, але використовує патерни перестановок — порядкові відношення між значеннями системи. Порівняно з іншими мірами складності має певні переваги, такі як стійкість до шуму та інваріантність до нелінійних монотонних перетворень  [12].\nЯк і в попередніх типах ентропії, ми реконструюємо часовий ряд із \\(N\\) значень з фіксованою розмірністю вбудовування \\(d_E\\) та часовою затримкою \\(\\tau\\), за вкладеною матрицею формуємо часові векторні послідовності\n\\[\n\\vec{X}(i) = \\left\\{ x(i), x(i+1),..., x(i+[d_E - 1]\\tau) \\right\\},\n\\]\nі в результаті отримується \\(N - [d_E - 1]\\tau\\) векторів.\nКожен елемент \\(\\vec{X}(i)\\) перетворюється в числові ранги відповідно до їх порядку. Наприклад, для \\(d_E = 2\\) і \\(\\tau = 1\\) та часового ряду \\(\\vec{X}(i) = (-0.1, 0.4, 3.2, 12.0, 6.5)\\), вбудована матриця матиме такі пари: \\(\\vec{X}(1) = {-0.1, 0.4}\\), \\(\\vec{X}(2) = {0.4, 3.2}\\), \\(\\vec{X}(3) = {3.2, 12.0}\\), \\(\\vec{X}(4) = {12.0, 6.5}\\).\nДалі ми формуємо порядкові послідовності відповідно до їх числового порядку. Такі вектори як \\(\\vec{X}(1), \\vec{X}(2), \\vec{X}(3)\\) задовольняють умові \\(x(i) &lt; x(i+1)\\) і один вектор \\(\\vec{X}(4)\\) задовольняє умові \\(x(i) &gt; x(i+1)\\). Можна розглянути \\(d_E!\\) можливих перестановок порядку \\(d_E\\). У нашому прикладі є лише \\(2!\\) шаблони: \\(\\pi_1 = {0, 1}, \\pi_2 = {1, 0}\\).\nДля кожного шаблону ми визначаємо його відносну частоту:\n\\[\np(\\pi) = \\#\\left\\{\\vec{X}(i) \\,\\, \\text{має шаблон} \\,\\, \\pi \\right\\} \\Big/ (N - [d_E - 1]\\tau).\n\\]\nІмовірність знаходження вектора по шаблону \\(\\pi_1\\) дорівнює \\(3/4\\) і по шаблону \\(\\pi_2\\) дорівнює \\(1/4\\), тобто, ми формуємо розподіл імовірностей \\(P = \\left\\{ p(\\pi_{1}),...,p(\\pi_{d_E}) \\right\\}\\). Нарешті, даний вид ентропії може бути розрахований у той самий спосіб, що й ентрпія Шеннона:\n\\[\nPEn(\\vec{X}, d_E) = -\\sum_{i=1}^{d_E}p(\\pi_i)\\ln{p(\\pi_i)}.\n\\]\nДля зручності PEn нормалізується згідно наступного рівняння  [13]:\n\\[\n\\overline{PEn(\\vec{X}, d_E)} = PEn(\\vec{X}, d_E) \\Big/ PEn_{max},\n\\]\nде \\(PEn_{max} = \\ln{D!}\\), а нормалізована ентропія перестановок знаходиться в діапазоні \\(0 \\leq \\overline{PEn(\\vec{X}, d_E)} \\leq 1\\).\n\nwindow = 500      # ширина вікна\ntstep = 5         # часовий крок\n\nm = 4             # розмірність вкладень\ntau = 3           # часова затримка\n\nType = 'none'     # none - класична \n                  # finegrain - Дрібнозерниста ентропія перестановок \n                  # modified - Модифікована ентропія перестановок \n                  # weighted - Зважена ентропія перестановок \n                  # ampaware - Амплітудно-орієнтована ентропія перестановок \n                  # edge - Ентропія перестановки граней\n                  # uniquant - Рівномірна ентропія перестановок на основі квантування \n            \ntpx = -1          # finegrain tpx - параметр α, додатний скаляр (за замовчуванням: 1)\n                  # ampaware tpx - параметр A, значення в діапазоні [0, 1] (за замовчуванням: 0.5)\n                  # edge tpx - параметр чутливості r, скаляр &gt; 0 (за замовчуванням: 1)\n                  # uniquant tpx - параметр L, ціле число &gt; 1 (за замовчуванням: 4)\n\nlog = np.exp(1)   # основа логарифма\nnorm = True       # нормування ентропії \n\nret_type = 1      # вид ряду: \n                  # 1 - вихідний\n                  # 2 - детрендований (різниця між теп. значенням та попереднім)\n                  # 3 - прибутковості звичайні \n                  # 4 - стандартизовані прибутковості \n                  # 5 - абсолютні значення (волатильності)\n                  # 6 - стандартизований ряд   \n\nlength = len(time_ser.values)  # довжина самого ряду\n\nPEn = []        # масив для зберігання значень нормалізованої перм. ентропії\n\n\nfor i in tqdm(range(0,length-window,tstep)):  # фрагменти довжиною window  \n                                              # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n    \n    # обчислюємо ентропію перестановок \n    _, Pnorm, cPE = eh.PermEn(fragm, \n                              m=m, \n                              tau=tau, \n                              Typex=Type, \n                              tpx=tpx, \n                              Logx=log, \n                              Norm=norm)\n    \n    PEn.append(Pnorm[-1])\n\n100%|██████████| 2798/2798 [00:07&lt;00:00, 366.68it/s]\n\n\nЗберігаємо значення пермутаційної ентропії до текстового файлу:\n\nnp.savetxt(f\"PEn_name={symbol}_window={window}_step={tstep}_\\\n        dim={m}_tau={tau}_sertype={ret_type}_type={Type}_param={tpx}.txt\", PEn)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_permen = fr'$PEn$'\n\nfile_name_perm = f\"PEn_name={symbol}_window={window}_step={tstep}_\\\n        dim={m}_tau={tau}_sertype={ret_type}_type={Type}_param={tpx}\"\n\nВізуалізуємо результат для ентропії перестановок:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          PEn, \n          ylabel, \n          label_permen,\n          xlabel,\n          file_name_perm,\n          clr='indigo')\n\n\n\n\n\n\n\nРис. 5.5: Динаміка фондового індексу N225 та ентропії перестановок\n\n\n\n\n\nНа представленому рисунку (Рис. 5.5) видно, що \\(PEn\\) спадає в кризові та передкризові періоди на фондовому ринку. Це вказує на зростання ймовірності появи одного конкретного патерна для подальшої динаміки ринку, а отже й кількості очікуваної інформації при аналізі флуктуацій індексу N225.\n\n\n5.2.5 Eнтропія сингулярного розкладу\nЕнтропію сингулярного розкладу (Singlular value decomposition entropy, SVDEn)  [14] можна інтуїтивно розглядати як показник того, скільки власних векторів потрібно для адекватного пояснення набору даних. Іншими словами, вона вимірює багатство ознак: чим вища SVDEn, тим більше ортогональних векторів потрібно для адекватного пояснення стану простору. Подібно до інформаційого показника Фішера, SVDEn базується на розкладанні сингулярного значення реконструйованого методом часових затримок сигналу.\n\nwindow = 500      # ширина вікна\ntstep = 5         # часовий крок\n\nm = 3             # розмірність вкладень\ntau = 1           # часова затримка\n\nret_type = 6      # вид ряду: \n                  # 1 - вихідний\n                  # 2 - детрендований (різниця між теп. значенням та попереднім)\n                  # 3 - прибутковості звичайні \n                  # 4 - стандартизовані прибутковості \n                  # 5 - абсолютні значення (волатильності)\n                  # 6 - стандартизований ряд   \n\nlength = len(time_ser.values)  # довжина самого ряду\n\nSVDEn = [] # масив для зберігання значень розподільної ентропії \n\n\nfor i in tqdm(range(0,length-window,tstep)):  # фрагменти довжиною window  \n                                              # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n    \n    # обчислення розподільної ентропії\n    svden, _ = nk.entropy_svd(signal=fragm, \n                            dimension=m, \n                            delay=tau)\n\n    SVDEn.append(svden)\n\n100%|██████████| 2798/2798 [00:01&lt;00:00, 2679.47it/s]\n\n\nЗберігаємо значення розподільної ентропії до текстового файлу:\n\nnp.savetxt(f\"SVDEn_name={symbol}_window={window}_step={tstep}_\\\n        dim={m}_tau={tau}_sertype={ret_type}.txt\", SVDEn)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_svden = fr'$SVDEn$'\n\nfile_name_svden = f\"SVDEn_name={symbol}_window={window}_step={tstep}_\\\n        dim={m}_tau={tau}_sertype={ret_type}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          SVDEn, \n          ylabel, \n          label_svden,\n          xlabel,\n          file_name_svden,\n          clr='darkorange')\n\n\n\n\n\n\n\nРис. 5.6: Динаміка фондового індексу N225 та ентропії сингулярного розкладу\n\n\n\n\n\nРис. 5.6 показує, що ентропія сингулярного розкладу спадає у (перед)кризові періоди, що говорить про зростання кореляцій на фондовому ринку. Оскільки \\(SVDEn\\) базується на розподілі власних векторів, можна зробити припущення, що в передкризові моменти часу динамікою ринку керують один або декілька власних векторів, які і є рушійною складновою досліджуваного індексу.\n\n\n5.2.6 Дисперсійна ентропія\nДля заданого одновимірного сигналу довжини \\(N\\): \\(x=\\{x_1, x_2,...,x_N\\}\\), алгоритм дисперсійної ентропії (Dispersion entropy, DispEn) включає 4 основних кроки  [15]:\n\nСпочатку \\(x_j \\, (j=1,2,...N)\\) відображаються на \\(c\\) класів, позначених від \\(1\\) до \\(c\\). Для цього існує ряд лінійних та нелінійних підходів. Хоча лінійний алгоритм відображення є найшвидшим, коли максимальні та/або мінімальні значення часового ряду набагато більші або менші за середнє/медіанне значення сигналу, більшість значень \\(x_i\\) віднесено лише до кількох класів. Таким чином, ми спочатку використовуємо нормальну кумулятивну функцію розподілу (NCDF) для відображення \\(x\\) в \\(y=\\{y_1,y_2,...,y_N\\}\\) від \\(0\\) до \\(1\\). Далі виконується лінійний алгоритм присвоєння кожному \\(y_j\\) цілого числа від \\(1\\) до \\(c\\). Для цього для кожного члена відображеного сигналу ми використовуємо \\(z_{j}^{c} = \\text{round}(y_{j} + 0.5)\\), де \\(z_{j}^{c}\\) показує \\(j\\)-й член класифікованого часового ряду, а округлення передбачає або збільшення, або зменшення числа до наступної цифри. Варто зазначити, що цей крок можна виконати за допомогою інших лінійних та нелінійних методів відображення.\nКожен вектор \\(\\mathbf{z}_{i}^{d_E, c}\\) з розмірністю \\(d_E\\) та часовою затримкою \\(\\tau\\) має вид \\(\\mathbf{z}_{i}^{d_E, c} = \\{ z_{i}^{c}, z_{i+\\tau}^{c},...,z_{i+(d_E-1)\\tau}^{c} \\}\\), \\(i=1,...,N-(d_E-1)\\tau\\) і проектується на дисперсійний шаблон \\(\\pi_{v_0, v_1,...,v_{d_E-1}}\\), де \\(z_{i}^{c}=v_{0}, z_{i+\\tau}^{c}=v_{1},...,z_{i+(d_{E}-1)\\tau}^{c}=v_{d_{E}-1}\\). Кількість можливих дисперсійних шаблонів, що може бути присвоєна кожному вектору \\(\\mathbf{z}_{i}^{d_E, c}\\), дорівнює \\(c^{m}\\), оскільки сигнал має \\(d_E\\) елементів і кожному елементу може бути присвоєно ціле значення від \\(1\\) до \\(c\\).\nДля всіх потенційних \\(c^{m}\\) дисперсійних шаблонів розраховується відносна частота:\n\\[\n   p(\\pi_{v_0, v_1,...,v_{d_{E}-1}}) = \\frac{\\#\\left\\{i | i \\leq N-(d_E-1)\\tau, \\mathbf{z}_{i}^{d_E, c} \\,\\, \\text{має шаблон} \\,\\, \\pi_{v_0, v_1,...,v_{d_{E}-1}} \\right\\} }{N-(d_{E}-1)\\tau}.\n\\]\nНарешті, опираючись на формулу ентропії Шеннона, DispEn розраховується як\n\\[\n   DispEn(\\mathbf{x}, d_E, c, \\tau) = -\\sum_{\\pi = 1}^{c^{d_E}}p(\\pi_{v_0, v_1,...,v_{d_{E}-1}})\\ln{(\\pi_{v_0, v_1,...,v_{d_{E}-1}})}.\n\\]\n\nКоли всі можливі дисперсійні шаблони мають однакову ймовірність, отримуємо найбільше значення DispEn, яке має величину \\(\\ln{c^{d_E}}\\). І навпаки, якщо тільки одне \\(p(\\pi_{v_0, v_1,...,v_{d_{E}-1}})\\) відрізняється від нуля (абсолютно регулярний/передбачуваний сигнал), отримуємо найменше значення DispEn.\n\nwindow = 500      # ширина вікна\ntstep = 5         # часовий крок\n\nfluct = False     # флуктуаційно-дисперсійна ентропія\nm = 3             # розмірність вкладень\ntau = 1           # часова затримка\nrho = 1           # параметр для Type=\"finesort\", позитивний скаляр (за замовчуванням 1)\nclasses = 6       # кількість символів, що задіяні при перетворені\n\ntype = 'ncdf'     # тип символьного перетворення ряду:\n                  # \"ncdf\" - Нормалізована кумулятивна функція розподілу\n                  # \"kmeans\" - Алгоритм кластеризації K-середніх\n                  # \"linear\" - Лінійна сегментація діапазону сигналу\n                  # \"finesort\" - Ентропія дрібнодисперсного розсіювання\n\nret_type = 4      # вид ряду: \n                  # 1 - вихідний\n                  # 2 - детрендований (різниця між теп. значенням та попереднім)\n                  # 3 - прибутковості звичайні \n                  # 4 - стандартизовані прибутковості \n                  # 5 - абсолютні значення (волатильності)\n                  # 6 - стандартизований ряд   \n\nlength = len(time_ser.values)  # довжина самого ряду\n\nDispEn = [] # масив значень для зберігання дисперсійної ентропії \n\n\nfor i in tqdm(range(0,length-window,tstep)):  # фрагменти довжиною window  \n                                              # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n    \n    # обчислюємо дисперсійну ентропію\n    Disp, _ = nk.entropy_dispersion(signal=fragm, \n                                    dimension=m, \n                                    delay=tau, \n                                    c=classes, \n                                    symbolize=type, \n                                    fluctuation=fluct, \n                                    rho=rho)\n    DispEn.append(Disp)\n\n100%|██████████| 2798/2798 [00:03&lt;00:00, 757.89it/s]\n\n\nЗберігаємо значення дисперсійної ентропії до текстового файлу:\n\nnp.savetxt(f\"DispEn_symbol={symbol}_window={window}_step={tstep}_d_e={m}_tau={tau}_\\\n           series_type={ret_type}_fluct={fluct}_rho={rho}_\\\n           classes={classes}_type={Type}.txt\", DispEn)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_dispen = fr'$DispEn$'\n\nfile_name_dispen = f\"DispEn_symbol={symbol}_window={window}_step={tstep}_d_e={m}_tau={tau}_\\\n           series_type={ret_type}_fluct={fluct}_rho={rho}_\\\n           classes={classes}_type={Type}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          DispEn, \n          ylabel, \n          label_dispen,\n          xlabel,\n          file_name_dispen,\n          clr='coral')\n\n\n\n\n\n\n\nРис. 5.7: Динаміка фондового індексу N225 та дисперсійної ентропії\n\n\n\n\n\nРис. 5.7 демонструє, що дисперсійна ентропія падає напередодні крахових періодів. Особливо помітним це предстає для крахів 1970, 1990, 2010 та 2020. Це говорить про те, що розподіл дисперсійних шаблонів стає зміщенним, що й відзеркалюється у спаді ентропії. Це також вказує на періодизацію ринку. Для періодів, коли дисперсійна ентропія максимальна, очікування трейдерів також залишаються різносторонніми, що робить ринок більш непередбачуваним.\n\n\n5.2.7 Спектральна ентропія\nСпектральна ентропія (Spectral entropy, SE або SpEn)  [16] розглядає нормовану щільність спектра потужності (power spectral density, PSD) сигналу в частотній області як розподіл імовірностей і обчислює його ентропію Шеннона:\n\\[\nSpEn = -\\sum P(f)\\log_{2}[P(f)].\n\\]\nСигнал з однією частотною складовою (наприклад, чиста синусоїда) має найменшу ентропію. З іншого боку, сигнал з усіма частотними компонентами однакової потужності (білий шум) дає найбільшу ентропію.\n\nwindow = 500      # ширина вікна\ntstep = 5         # часовий крок\n\nnum_bins = 30     # якщо передано ціле число, розділить PSD \n                  # на декілька частотних діапазонів\n\nmethod = 'fft'    # метод для розрахунку щільності спектра потужності:\n                  # welch\n                  # fft\n                  # multitapers\n                  # lombscargle\n                  # burg\n\nret_type = 4      # вид ряду: \n                  # 1 - вихідний\n                  # 2 - детрендований (різниця між теп. значенням та попереднім)\n                  # 3 - прибутковості звичайні \n                  # 4 - стандартизовані прибутковості \n                  # 5 - абсолютні значення (волатильності)\n                  # 6 - стандартизований ряд   \n\nlength = len(time_ser.values)  # довжина самого ряду\n\nSpEn = [] # масив значень для зберігання спектральної ентропії \n\n\nfor i in tqdm(range(0,length-window,tstep)):  # фрагменти довжиною window  \n                                              # з кроком tstep\n\n    # відбираємо фрагмент\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n    \n    # обчислюємо спектральну ентропію\n    spec, _ = nk.entropy_spectral(signal=fragm, \n                                  bins=num_bins,\n                                  method=method)\n\n    SpEn.append(spec)\n\n100%|██████████| 2798/2798 [00:13&lt;00:00, 209.38it/s]\n\n\nЗберігаємо значення спектральної ентропії до текстового файлу:\n\nnp.savetxt(f\"SpEn_symbol={symbol}_window={window}_step={tstep}_\\\n           series_type={ret_type}_bins={num_bins}_psd={method}.txt\", SpEn)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_spen = fr'$SpEn$'\n\nfile_name_spen = f\"SpEn_symbol={symbol}_window={window}_step={tstep}_\\\n                   series_type={ret_type}_bins={num_bins}_psd={method}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          SpEn, \n          ylabel, \n          label_spen,\n          xlabel,\n          file_name_spen,\n          clr='deeppink')\n\n\n\n\n\n\n\nРис. 5.8: Динаміка фондового індексу N225 та спектральної ентропії\n\n\n\n\n\nЗ Рис. 5.8 видно, що спектральна ентропія починає падати в передкризовий період, що говорить про зміщення спектра потужності в конкретну область частот. Це вказує на періодизацію динаміки ринку, що у свою чергу вказує на зростання кореляцій і трендостійкості ринку.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Лабораторна робота № 5</span>"
    ]
  },
  {
    "objectID": "lab_5.html#висновок",
    "href": "lab_5.html#висновок",
    "title": "5  Лабораторна робота № 5",
    "section": "5.3 Висновок",
    "text": "5.3 Висновок\nНа прикладі ентропійних мір складності перевірено гіпотезу про зв’язок мір складності та кризових явищ, висунуту на основі теорії складних систем. У рамках алгоритму ковзного вікна за набором ентропійних показників було показано, що фінансові крахи характеризуються зміною складності: у передкризовий період, як правило, ми можемо спостерігати упорядкування системи, а в кризовий та післякризовий періоди зростання хаотичності. Порівняння ентропійних характеристик відкриває можливість передчасної ідентифікації та попередження ня кризових явищ у системах різної природи та складності.\nТаким чином, представлені індикатори-передвісники кризових явищ, теоретично, дозволяють обійти потребу в значних обчислювальних ресурсах і досить дискусійних методів прогнозування цінових коливань і їх трендів.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Лабораторна робота № 5</span>"
    ]
  },
  {
    "objectID": "lab_5.html#завдання-для-самостійної-роботи",
    "href": "lab_5.html#завдання-для-самостійної-роботи",
    "title": "5  Лабораторна робота № 5",
    "section": "5.4 Завдання для самостійної роботи",
    "text": "5.4 Завдання для самостійної роботи\n\nПроведіть порівняльний аналіз ентропійних показників для тестових часових рядів\nПобудуйте часові ряди, які у визначених точках характеризуються кризами і покажіть, чи є (і які саме) ентропійні міри індикаторами, або передвісниками кризових явищ\nЗробіть загальні висновки і оформіть звіт\n\n\n\n\n\n[1] R. Clausius, T. A. Hirst, and J. Tyndall, The Mechanical Theory of Heat: With Its Applications to the Steam-Engine and to the Physical Properties of Bodies (J. Van Voorst, 1867).\n\n\n[2] L. Boltzmann, Weitere Studien über Das wärmegleichgewicht Unter Gasmolekülen, in Kinetische Theorie II: Irreversible Prozesse Einführung Und Originaltexte (Vieweg+Teubner Verlag, Wiesbaden, 1970), pp. 115–225.\n\n\n[3] C. E. Shannon, A Mathematical Theory of Communication, Bell System Technical Journal 27, 379 (1948).\n\n\n[4] S. M. Pincus, I. M. Gladstone, and R. A. Ehrenkranz, A Regularity Statistic for Medical Data Analysis, Journal of Clinical Monitoring 7, 335 (1991).\n\n\n[5] S. M. Pincus, Approximate Entropy as a Measure of System Complexity, Proceedings of the National Academy of Sciences 88, 2297 (1991).\n\n\n[6] V. N. Soloviev, A. O. Bielinskyi, and N. A. Kharadzjan, Coverage of the Coronavirus Pandemic Through Entropy Measures, in 3rd Workshop for Young Scientists in Computer Science and Software Engineering (CS and SE and SW 2020), Kryvyi Rih, Ukraine, November 27, 2020, edited by K. A. E., S. S. O., S. V. N., and S. A. M., Vol. 2832 (CEUR-WS.org, 2021), pp. 24–42.\n\n\n[7] W. Chen, Z. Wang, H. Xie, and W. Yu, Characterization of Surface EMG Signal Based on Fuzzy Entropy, IEEE Transactions on Neural Systems and Rehabilitation Engineering 15, 266 (2007).\n\n\n[8] H.-B. Xie, W.-X. He, and H. Liu, Measuring Time Series Regularity Using Nonlinear Similarity-Based Sample Entropy, Physics Letters A 372, 7140 (2008).\n\n\n[9] A. O. Bielinskyi, V. N. Soloviev, S. O. Semerikov, and V. V. Solovieva, IDENTIFYING STOCK MARKET CRASHES BY FUZZY MEASURES OF COMPLEXITY, Neuro-Fuzzy Modeling Techniques in Economics 10, 3 (2021).\n\n\n[10] J. S. Richman and J. R. Moorman, Physiological Time-Series Analysis Using Approximate Entropy and Sample Entropy, American Journal of Physiology-Heart and Circulatory Physiology 278, H2039 (2000).\n\n\n[11] C. Bandt and B. Pompe, Permutation Entropy: A Natural Complexity Measure for Time Series, Phys. Rev. Lett. 88, 174102 (2002).\n\n\n[12] H. Kantz and T. Schreiber, Nonlinear Time Series Analysis (Cambridge University Press, 2004).\n\n\n[13] V. N. Soloviev, A. Bielinskyi, and V. Solovieva, Entropy Analysis of Crisis Phenomena for DJIA Index, in Proceedings of the 15th International Conference on ICT in Education, Research and Industrial Applications. Integration, Harmonization and Knowledge Transfer. Volume II: Workshops, Kherson, Ukraine, June 12-15, 2019, edited by V. Ermolayev, F. Mallet, V. Yakovyna, V. S. Kharchenko, V. Kobets, A. Kornilowicz, H. Kravtsov, M. S. Nikitchenko, S. Semerikov, and A. Spivakovsky, Vol. 2393 (CEUR-WS.org, 2019), pp. 434–449.\n\n\n[14] S. J. Roberts, W. Penny, and I. Rezek, Temporal and Spatial Complexity Measures for Electroencephalogram Based Brain-Computer Interfacing, Medical & Biological Engineering & Computing 37, 93 (1999).\n\n\n[15] M. Rostaghi and H. Azami, Dispersion Entropy: A Measure for Time-Series Analysis, IEEE Signal Processing Letters 23, 610 (2016).\n\n\n[16] J. C. Crepeau and L. K. Isaacson, Journal of Non-Equilibrium Thermodynamics 16, 137 (1991).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Лабораторна робота № 5</span>"
    ]
  },
  {
    "objectID": "lab_6.html",
    "href": "lab_6.html",
    "title": "6  Лабораторна робота № 6",
    "section": "",
    "text": "6.1 Теоретичні відомості",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Лабораторна робота № 6</span>"
    ]
  },
  {
    "objectID": "lab_6.html#теоретичні-відомості",
    "href": "lab_6.html#теоретичні-відомості",
    "title": "6  Лабораторна робота № 6",
    "section": "",
    "text": "6.1.1 Визначення фрактала\nФракталами називають геометричні об’єкти: лінії, поверхні, просторові тіла, що мають сильно шорстку поверхню або форму і характеризуються властивістю самоподібності  [1–3]. Слово фрактал походить від латинського слова fractus і перекладається як дробовий, ламаний. Самоподібність як основна характеристика фрактала означає, що він більш-менш однорідно змінюється в широкому діапазоні масштабів. Так, при збільшенні маленькі фрагменти фрактала стають дуже схожими на великі. В ідеальному випадку така самопподібність призводить до того, що фрактальний об’єкт виявляється інваріантним щодо розтягувань, тобто йому, як кажуть, притаманна дилатаційна симетрія. Вона передбачає незмінність основних геометричних особливостей фрактала при зміні масштабу.\nОчевидно, що фрактальні об’єкти реального світу не є нескінченно самоподібними й існує мінімальний масштаб \\(l_{min}\\), такий, що на масштабі \\(l \\approx l_{min}\\) властивість самоподібності зникає. Окрім цього, на достатньо великих масштабах довжин \\(l &gt; l_{max}\\), де \\(l_{max}\\) — характерний геометричний розмір об’єктів, ця властивість самоподібності також порушується. Тому властивості природніх фракталів розглядаються лише на масштабах \\(l\\), що задовільняють відношення \\(l_{min} \\ll l \\ll l_{max}\\). Такі обмеження природні, оскільки, коли ми приводимо в якості прикладу фракталу — ламану, негладку траєкторію броунівської частинки, то ми розуміємо, що цей образ представляє очевидну ідеалізацію. Справа в тому, що на малих масштабах приховується граничність маси і розмірів броунівської частинки, а також скінченність часу зіткнення. При врахуванні цих обставин траєкторія броунівської частинки починає представляти гладку криву.\nВарто зазначити, що властивість точної самоподібності характерна лише для регулярних фракталів. Якщо замість детермінованого способу побудови включити в алгоритм їхнього створення деякий елемент випадковості (як це буває, наприклад, у багатьох процесах диференційованого зростання кластерів, електричному пробої тощо), то виникають так звані випадкові фрактали. Основна їхня відмінність від регулярних полягає в тому, що властивості самоподібності є справедливими тільки після відповідного усереднення за всіма статистично незалежними реалізаціями об’єкта. При цьому збільшена частина фрактала не точно ідентична вихідному фрагменту, проте їхні статистичні характеристики збігаються.\n\n\n6.1.2 Довжина берегової лінії\nПершочергово поняття фрактала у фізиці виникло у зв’язку із завданням про визначення довжини берегової лінії. Під час її вимірювання за наявною картою місцевості з’ясувалася цікава деталь — чим більш великомасштабна карта береться, тим довшою виявляється ця берегова лінія  [4–7]. Нехай, наприклад, відстань по прямій між розсташованими на береговій лінії точками А та В дорівнює R (див. Рис. 6.1).\n\n\n\n\n\n\nРис. 6.1: Визначення довжини берегової лінії між точками А та В  [8]\n\n\n\nТоді, щоб виміряти довжину берегової лінії між цима точками, ми розташуємо по берегу жорстко пов’язані один з одним вузол так, що відстань між сусідніми вузлами дорівнювала б, наприклад, \\(l=10\\) км. Довжину берегової лінії в кілометрах між точками А і В ми приймемо тоді рівною числу вузлів мінус 1, помноженому на 10. Наступне вимірювання цієї довжини ми зробимо так само, але відстань між сусідніми вузлами зробимо вже рівною \\(l=1\\) км.\nВиявляється, що результат цих вимірювань буде різним. При зменшенні масштабу \\(l\\) ми отримуватимемо все більші й більші значення довжини. На відміну від гладкої кривої, лінія морського узбережжя виявляється найчастіше настільки порізаною (аж до найменших масштабів), що зі зменшенням довжини ланки \\(l\\) величина \\(L\\) — довжина берегової лінії — не прагне до кінцевого межі, а збільшується за степеневим законом\n\\[\nL \\approx l\\left( R \\big/ l \\right)^{D},\n\\tag{6.1}\\]\nа \\(D &gt; 1\\) — деякий степеневий показник, котрий іменується фрактальною розмірністю берегової лінії  [7]. Чим більше значення \\(D\\), тим більш ломаною або деталізованою представляється ця берегова лінія. Походження залежності (6.1) має бути інтуїтивно зрозумілим: чим менший масштаб ми використовуємо, тим меньші деталі узбережжя будуть враховані і тим менший вклад вони внесуть у вимірювану довжину. Навпаки, збільшуючи масштаб, ми “розгортаємо” узбережжя, зменшуючи довжину \\(L\\).\nТаким чином, ми бачимо, що для визначення довжини берегової лінії \\(L\\) за допомогою жорсткого масштабу \\(l\\), необхідно зробити \\(N=L/l\\) кроків, причому величина \\(L\\) змінюється з \\(l\\) так, що \\(N\\) залежить від \\(l\\) за законом \\(N \\approx (R/l)^{D}\\). У результаті зі зменшенням масштабу довжина берегової лінії необмежено зростає. Ця обставина різко відрізняє фрактальну криву від звичайної гладкої кривої (типу кола, еліпса), для якої межа довжини апроксимованої ламаної \\(L\\), яка апроксимує, за наближення до нуля довжини її ланки \\(l\\) є скінченною. У результаті для гладкої кривої її фрактальна розмірність \\(D = 1\\), тобто збігається з топологічною.\n\n\n6.1.3 Фрактальна розмірність множин\nВище було введено поняття про фрактальну розмірність берегової лінії. Дамо тепер загальне визначення цієї величини. Нехай \\(d\\) — звичайна Евклідова розмірність простору, в якому розташований наш фрактальний об’єкт (\\(d=1\\) — лінія, \\(d=2\\) — площина, \\(d=3\\) — звичайний тривимірний простір). Покриємо тепер цей об’єкт цілком \\(d\\)-мірними “кулями” радіуса 1. Припустимо, що нам потребувалося для цього не менше, ніж \\(N(l)\\) куль. Тоді, якщо за досить малих \\(l\\) величина \\(N(l)\\) змінюється з \\(l\\) за степеневим законом\n\\[\nN(l) \\sim 1 \\big/ l^D ,\n\\tag{6.2}\\]\nтоді \\(D\\) — називається хаусдорфовою або фрактальною розмірністю цього об’єкта  [9]. Очевидно, що ця формула еквівалентна відношеню \\(N \\approx \\left( R/l \\right)^{D}\\), що використовувалось вище для визначення довжини берегової лінії. Рівняння 6.3 можна переписати у вигляді\n\\[\nD = -\\lim_{l \\to 0} \\ln{N(l)} \\big/ \\ln{l}.\n\\tag{6.3}\\]\nЦе відношення й слугує загальним визначенням фрактальної розмірності \\(D\\). У відповідності до нього величина \\(D\\) представляє локальну характеристику досліджуваного об’єкта.\n\n\n6.1.4 Процедури обчислення монофрактальних розмірностей\nНаразі існує багато визначень та методів вимірювання фрактальної розмірності. Найпоширенішими одновимірними фрактальними розмірностями є розмірність Хаусдорфа, розмірність Хігучі, розмірність Петросяна та Мінковського  [10]. Розмірність Хаусдорфа є найпростішою фрактальною розмірністю. Але її обчислювальна складність є високою, що ускладнює її практичне застосування. Розмірність Мінковського є відносно простою, і фрактальну розмірність сигналу можна отримати, регулюючи розмір довжини сторони клітини в межах якої визначається “шорсткість” поверхні сигналу. Тому вона є широко впізнаваємою та застосовуваною. Який з показників фрактальної розмірності найточніше описує складність сигналу та здатний ідентифікувати кризові явища і представляє ключовий момент цієї лабораторної роботи.\n\n6.1.4.1 R/S-аналіз\nМетод R/S-аналізу, розроблений Мандельбротом та Уоллесом  [11], базується на попередньо створеному методі гідрологічного аналізу Херста  [12,13], і дозволяє обчислювати параметр самоподібності \\(H\\), який вимірює інтенсивність довготривалих залежностей у часовому ряді. Коефіцієнт \\(H\\), який називають коефіцієнтом Херста, містить мінімальні прогнози стосовно природи системи, що вивчається, і може класифікувати часові ряди. За допомогою цього показника розрізняють випадкові (гаусові) та невипадкові ряди; окрім того, він пов’язаний із фрактальною розмірністю, що, у свою чергу, характеризує ступінь згладженості графіка, побудованого на основі часового ряду. Методом R/S-аналізу можливо також виявити максимальну довжину інтервалу (цикл), на якому значення зберігають інформацію про початкові дані системи (довготривала пам’ять).\nАналіз починається з побудови ряду логарифмічних прибутковостей, \\(G(t) \\equiv \\ln{x(t + \\Delta t)} - \\ln{x(t)}\\), де \\(x(t)\\) — значення вихідного часового ряду в момент \\(t\\), \\(\\Delta t\\) — часовий крок. Отримана послідовність \\(G(t)\\) розбивається на \\(d\\) підпослідовностей довжини \\(n\\).\nДля кожної підпослідовності \\(m=1,...,d\\):\n\nШукається середнє значення \\(\\mu_m\\) та стандартне відхилення \\(S_m\\).\nДані нормалізуються шляхом віднімання середнього значення послідовності \\(X_{i,m}=G_{i,m}-\\mu_m\\), \\(i=1,...,n\\).\nЗнаходиться кумулятивна сума послідовності \\(X\\)ів: \\(Y_{i,m}=\\sum_{j=1}^{i}X_{j,m}\\), \\(i=1,...,n\\).\nУ межах кожної підпослідовності знаходиться розмах між максимальним та мінімальним значеннями: \\(R_m = \\max\\{Y_{1,m},...,Y_{n,m}\\}-\\min\\{Y_{1,m},...,Y_{n,m}\\}\\), який стандартизується середнім квадратичним відхиленням \\(R_{m}/S_{m}\\).\nОбчислюється середнє \\((R/S)_n\\) нормованих значень розмаху для всіх підпослідовностей довжини \\(n\\).\n\nR/S-статистика, обрахована таким чином, відповідає співвідношенню \\((R/S)_{n} \\cong cn^{H}\\), де значення \\(H\\) може бути отримане шляхом обчислення \\((R/S)_n\\) для послідовностей інтервалів зі збільшенням часового горизонту:\n\\[\n\\log{(R/S)}_{n} = \\log{c} + H\\log{n}.\n\\tag{6.4}\\]\nЗнайти коефіцієнт Херста можна, побудувавши залежність \\((R/S)_n\\) vs. \\(n\\) у подвійному логарифмічному масштабі і взявши коефіцієнт нахилу прямої, яка інтерполює точки отриманого графіка. Якщо значення \\(H=0.5\\), говорять про послідовність, що представляє собою білий шум; \\(0.5 &lt; H \\leq 1\\) свідчить про персистентний ряд, коли існує тенденція слідування великих значень ряду за великими і навпаки; \\(H&lt;0.5\\) вказує на антиперсистентний ряд.\nПри збільшенні часового горизонту коефіцієнт нахилу інтерполюючої прямої повинен прямувати до значення \\(H=0.5\\); сам процес переходу свідчить про втрату впливу початкових умов на поточні значення, і, таким чином, можна говорити про горизонт довгої пам’яті — це точка, до якої коефіцієнт нахилу інтерполюючої прямої відмінний від 0.5, а після — близько 0.5.\n\n\n\n\n\n\nПримітка до R/S-аналізу\n\n\n\nМіж фрактальною розмірністю та показником Херста також існує зв’язок\n\\[\nD_f = 2-H.\n\\]\nЯкщо для берегової лінії ми визначали масштабування її довжини \\(L\\) в залежності від зміни \\(l\\), то у випадку з R/S-аналізом ми визначаємо зміну нормованого розмаху значень ряду в межах масштабу \\(n\\)\n\n\n\n\n6.1.4.2 Аналіз детрендованих флуктуацій\nАналіз детрендований флуктуацій (Detrended fluctuation analysis, DFA)  [14] базується на гіпотезі про те, що корельований часовий ряд може бути відображений на самоподібний процес шляхом інтегрування. Таким чином, вимірювання властивостей самоподібності може непрямо свідчити про кореляційні властивості ряду. Переваги DFA порівняно з іншими методами (спектральний аналіз, R/S-аналіз) полягають в тому, що він виявляє довгочасові кореляції нестаціонарних часових рядів, а також дозволяє ігнорувати очевидні випадкові кореляції, що є наслідком нестаціонарності  [15].\nІснують DFA різних порядків, що відрізняються трендами, які вилучаються з даних.\nРозглянемо DFA найнижчого порядку.\n\nДля часового ряду довжини \\(N\\) знаходиться кумулятивна сума, \\(y(k)=\\sum_{i=1}^{k}\\left( x_i - \\bar{x} \\right)\\), де \\(x_i\\) — це \\(i\\)-те значення часового ряду, \\(\\bar{x}\\) — його середнє значення, \\(k=1,...,N\\).\nОтриманий ряд \\(y(k)\\) розбивається на \\(m\\) підпослідовностей (вікон) однакової ширини \\(n\\) і для кожної підпослідовності (у кожному вікні) виконується наступне:\n\nза допомогою методу найменших квадратів знаходиться локальний лінійний тренд \\(y_{t}(k)\\);\n\nпідпослідовність детрендується шляхом віднімання значення локального тренду \\(y_{t}(k)\\) від значень ряду \\(y(k)\\), що належать послідовності \\(t\\);\nзнаходиться середнє \\(\\bar{y_t}\\) детрендований значень.\n\n\nДля отриманих таким чином значень на всіх підпослідовностях знаходиться:\n\\[\nF_n = \\sqrt{\\bar{y_t} \\cdot m^{-1}},\n\\]\nде \\(n\\) — кількість точок у підпослідовності (ширина вікна), \\(m\\) — кількість підпослідовностей, \\(\\bar{y_t}\\) — середнє детрендованих значень для підпослідовності \\(t\\).\nВказана процедура повторюється для різних значень \\(n\\), внаслідок чого ми отримує набір залежностей \\(F_n\\) від \\(n\\). Побудова залежності \\(\\log{F(n)}\\) від \\(\\log{n}\\) та інтерполяція отриманих значень лінією регресії дає змогу обчислити показник скейлінга \\(\\alpha\\), що є коефіцієнтом кута нахилу інтерполяційної прямої і характеризує зміну кореляцій флуктуацій часового ряду \\(F_n\\) при збільшенні часового інтервалу \\(n\\).\nПорівняно із R/S-аналізом, DFA дає більші можливості інтерпретації скейлінгового показника \\(\\alpha\\):\n\nдля випадкового ряду (перемішаного чи “сурогатного”) \\(\\alpha = 0.5\\);\nпри наявності лише короткочасових кореляцій \\(\\alpha\\) може відрізнятись від 0.5, проте має тенденцію прямувати до 0.5 при збільшенні розміру вікна;\nЗначення \\(0.5 &lt; \\alpha \\leq 1.0\\) показує персистентні довгочасові кореляції, що відповідають степеневому закону;\n\\(0 &lt; \\alpha &lt; 0.5\\) означає антиперсистентний ряд;\ncпеціальний випадок, коли \\(\\alpha = 1\\), означає наявність \\(1/f\\) шуму.\nдля випадків, коли \\(\\alpha \\geq 1\\), кореляції існують, проте перестають відображувати степеневу залежність;\nвипадок \\(\\alpha = 1.5\\) свідчить про броунівський шум, який представляє інтегрований білий шум.\n\nУ випадку степеневої залежності функції автокореляцій спостерігається спад автокореляції з показником \\(\\gamma\\):\n\\[\nC(L) \\sim L^{-\\gamma}.\n\\]\nНа додачу до цього, спектральна густина також спадає за степеневим законом:\n\\[\nP(f) \\sim f^{-\\beta}.\n\\tag{6.5}\\]\nВідповідні показники виражаються через наступні відношення:\n\n\\(\\gamma=2-2\\alpha\\);\n\\(\\beta=2\\alpha-1\\).\n\nУ DFA другого порядку (DFA2) обчислюються відхилення \\(F^2(v,s)\\) профілю від інтерполяційного многочлена другого порядку. Таким чином, вилучаються впливи можливих лінійних та параболічних трендів для масштабів, більших за розглядувані. Взагалі, у DFA порядку \\(n\\) обчислюються відхилення профілю від інтерполяційного многочлена \\(n\\)-го порядку, що вилучає вплив всіх можливих трендів порядків до (\\(n-1\\)) для масштабів, більших від розміру вікна.\nПотім обчислюється найближчий поліном \\(y_{ν}(s)\\) для профілю на кожному із \\(2N_s\\) сегментів \\(v\\) і визначається відхилення\n\\[\nF^2(v,s) \\equiv \\frac{1}{s}\\sum_{i=1}^{s}\\left( x_{(v-1)s+i} - y_{i}(i) \\right)^{2}.\n\\tag{6.6}\\]\nДалі знаходиться середнє значення флуктуацій всіх детрендованих профілів:\n\\[\nF_2(s) \\equiv \\sqrt{\\left( \\frac{1}{2N_s} \\sum_{v=1}^{2N_s}F^{2}(v,s) \\right)}.\n\\tag{6.7}\\]\nЗначення (6.7) можна трактувати як середньоквадратичний зсув (переміщення) точки випадкових блукань у ланцюжку після \\(s\\) кроків.\n\n\n6.1.4.3 Фрактальна розмірність Хігучі\nФрактальна розмірність Хігучі  [16,17] — це один з різновидів монофрактальної розмірності, яка визначається наступним чином:\nПрипустимо, що у нас є часовий ряд \\(x(1), x(2),...,x(N)\\) і реконструйований часовий ряд \\(x_{m}^{k} = \\{ x(m), x(m+k), x(m+2k),..., x\\left( m+\\left[ (N-m) \\big/ k \\right] \\cdot k \\right) \\}\\) для \\(m=1,2,...,k\\), де \\(m\\) представляє початковий час; \\(k=2,...,k_{max}\\) представляють ступінь часового зміщення. Позначення \\([\\cdot]\\) представляє цілу частину \\(x\\). Для кожного реконструйованого часового ряду \\(x_{m}^{k}\\) розраховується середня довжина часової послідовності \\(L_{m}(k)\\):\n\\[\nL_{m}(k) = \\frac{\\sum_{i=1}^{\\left[ (N-m)/k \\right]} | x(m+ik) - x(m+(i-1)\\cdot k) | \\cdot (N-1)}{\\left[ (N-m) \\big/ k \\right] \\cdot k}.\n\\]\nДалі, для всіх середніх довжин \\(L_{m}(k)\\), знаходиться загальне середнє \\(L(k) = (k)^{-1}\\sum_{m=1}^{k}L_{m}(k)\\). Згідно методу Хігучі узагальне середнє значення \\(L(k)\\) пропорційне масштабу \\(k\\), тобто \\(L(k) \\propto k^{-D}\\). Далі логарифмуємо обидві сторони й отримуємо рівність \\(\\ln{L(k)} \\propto D \\cdot \\ln{\\left( 1 \\big/ k \\right)}\\). Інтерполювавши лінію регресії через залежність \\(\\ln{L(k)}\\) від \\(\\ln{\\left( 1 \\big/ k \\right)}\\), ми можемо отримати показник фрактальності \\(D\\) як кут нахилу цієї лінії. Показник \\(D\\) і представлятиме фрактальну розмірність Хігучі.\n\n\n6.1.4.4 Фрактальна розмірність Петросяна\nСпочатку для часового ряду \\(\\{ x_1, x_2,...,y_{N} \\}\\) створюємо його дискретизовану (бінарну) версію, \\(z_i\\):\n\\[\nz_i =\n\\begin{cases}\n    1, & x_i &gt; \\langle x \\rangle, \\\\\n    -1, & x_i \\leq \\langle x \\rangle.\n\\end{cases}\n\\]\nФрактальну розмірність Петросяна  [18–20] можна визначити як\n\\[\nD = \\log_{10}{N} \\big/ \\left( \\log_{10} + \\log_{10}{\\left[ N \\big/ (N+0.4N_{\\Delta}) \\right]} \\right),\n\\]\nде \\(N_{\\Delta}\\) — кількість загальних змін знаку величини \\(z_i\\): \\(N_{\\Delta} = \\sum_{i=1}^{N-2} \\left| (z_{i+1}-z_i) \\big/ 2 \\right|\\).\n\n\n6.1.4.5 Фрактальна розмірність Каца\nПредставимо, що сигнал складається з пари точок \\(\\left( x_i, y_i \\right)\\). Тоді, фрактальна розмірність Каца  [21] визначається як\n\\[\nD = \\log{N}/\\left(\\log{N} + \\log{d/L}\\right),\n\\]\nде \\(L = \\sum_{i=0}^{N-2}\\sqrt{\\left( y_{i+1}-y_{i} \\right)^{2} + \\left( x_{i+1}-x_{i} \\right)^{2}}\\), а значення \\(d = \\max{\\left( \\sqrt{\\left( x_i - x_1 \\right)^{2} - \\left( y_i - y_1 \\right)^{2}} \\right)}\\).\n\n\n6.1.4.6 Фрактальна розмірність Шевчика\nСпочатку, для множини значень \\(\\left( x_i, y_i \\right)\\) виконується нормалізація: \\(x_{i}^{*} = \\left( x_i-x_{min} \\right) \\big/ \\left( x_{max}-x_{min} \\right)\\) і \\(y_{i}^{*} = \\left( y_i - y_{min} \\right) \\big/ \\left( y_{max}-y_{min} \\right)\\).\nФрактальна розмірність Шевчика  [22] може бути визначена як\n\\[\nD = 1 + \\left. \\ln{L} \\big/ \\ln{\\left( 2 \\cdot  [N-1] \\right)} \\right.,\n\\]\n\\(L\\) — це довжина сигналу, що може бути розрахована за формулою \\(L = \\sum_{i=0}^{N-2}\\sqrt{\\left( y_{i+1}^{*}-y_{i}^{*} \\right)^{2} + \\left( x_{i+1}^{*}-x_{i}^{*} \\right)^{2}}\\).\n\n\n6.1.4.7 Фрактальна розмірність через нормалізовану щільність довжини\nДаний показник розраховується в наступний спосіб  [23]:\n\nДля часового ряду \\(\\{ x_1, x_2,...,x_n \\}\\) виконується стандартизація: \\(y_i = (x_i - \\mu)/\\sigma\\), де \\(\\mu\\) — це середнє значення ряду, \\(\\sigma\\) — це стандартне відхилення.\nРозраховується нормалізована щільність довжини \\(NLD = \\frac{1}{N}\\sum_{i=2}^{N}\\left| y_i - y_{i-1} \\right|\\). Фактичний розрахунок фрактальної розмірності сигналу базується на побудові монотонної калібрувальної кривої \\(D = f(NLD)\\) за набором функцій Вейєрштрасса, для яких значення \\(D\\) задаються теоретично.\nДля обчислювальних цілей створено дві моделі цієї залежності:\n\nлогарифмічну модель: \\(D = a \\cdot \\log{\\left(NLD - NLD_{0} \\right)} + C\\);\nстепеневу модель: \\(D = a \\cdot \\left(NLD - NLD_{0} \\right)^{k}\\). Бібліотека neurokit2 використовує саме степеневу модель. Параметр \\(a=1.9079\\), \\(k=0.18383\\) і \\(NLD_{0}=0.097178\\), згідно  [24].\n\n\n\n\n6.1.4.8 Фрактальна розмірність і спектральна щільність потужності\nФрактальну розмірність можна обчислити на основі аналізу нахилу спектральної щільності потужності (power spectral density slope, PSD)  [25,26] в сигналах, що характеризуються степеневою частотною залежністю.\nСпочатку виконується перетворення часового ряду до частотної області і далі сигнал розбивається на гармонійні коливання певної амплітуди, які разом “складаються”, щоб представити вихідний сигнал. Якщо існує систематичний зв’язок між частотами в сигналі і потужністю цих частот, то в логарифмічних координатах це проявляється через лінійну залежність. Кут нахилу лінії регресії приймається як оцінка фрактальної розмірності.\nНахил 0 відповідає білому шуму, а нахил менше 0, але більше -1, відповідає рожевому шуму, тобто шуму \\(1/f\\). Спектральні нахили крутіші за -2 вказують на дробовий броунівський рух, що є проявом процесів випадкового блукання.\n\n\n6.1.4.9 Кореляційна розмірність\nКореляційна розмірність (\\(D_2\\)) — це похідна величина від кореляційного інтеграла (кореляційної суми) і може бути подана у вигляді  [27–29]:\n\\[\nC(\\varepsilon) = \\frac{1}{N^{2}}\\sum_{\\substack{i,j=1 \\\\ i\\neq j}}^{N}\\Theta \\left( \\varepsilon - \\| \\vec{x}(i) - \\vec{x}(j) \\| \\right), \\; \\vec{x}(i) \\in \\Re^{m}.\n\\]\nКореляційна розмірність може бути виведена з наступної степеневої залежності:\n\\[\nC(\\varepsilon) \\sim \\varepsilon^{\\nu},\n\\]\nабо\n\\[\nD_2 = \\lim_{M\\to\\infty}\\lim_{\\varepsilon\\to 0} \\log{\\left( g_{\\varepsilon} \\big/ N^2 \\right)} \\big/ \\log{\\varepsilon},\n\\]\nде \\(g_{\\varepsilon}\\) — це сумарна кількість пар точок, відстань між якими менша за радіус \\(\\varepsilon\\).\nУ першому випадку ми відбираємо \\(i\\)-ту траєкторію та всі інші \\(j\\)-ті траєкторії, і дивимося, чи потрапляють \\(j\\)-ті траєкторії в \\(\\varepsilon\\)-окіл \\(i\\)-ої траєкторії. Якщо відстань між ними не перевищує окіл радіусом \\(\\varepsilon\\), ми ставимо 1. Але якщо відстань між траєкторіями більша за \\(\\varepsilon\\), тоді ставимо 0. Далі все це підсумовується, ділиться на загальну кількість траєкторій. По суті кореляційний інтеграл це середня ймовірність того, що дві розглянуті траєкторії фазового простору, будуть знаходитися досить близько одна від одної. Чим тісніше розташовані точки фазового простору, тим більше значення кореляційного інтеграла. Чим більш рівновіддаленими видаються траєкторії одна від одної, тим ближче значення кореляційного інтеграла до нуля.\nЗначення кореляційної розмірності ми можемо відшукати аналогічно попередним фрактальним показникам: ми шукаємо залежність кореляційного інтеграла від значення \\(\\varepsilon\\). Ця залежність будується в логарифмічному масштабі.\nОсь деякі цікаві приклади.\nЕлектрокардіограма (ЕКГ): ЕКГ-сигнали відображають електричну активність серця. Складність ЕКГ-сигналу може бути оцінена за допомогою кореляційної розмірності. Очікується, що кореляційна розмірність ЕКГ здорового серця буде вищою через наявність складних патернів і варіабельності. З іншого боку, аномальні ЕКГ-сигнали, наприклад, від пацієнтів з аритміями або серцевими захворюваннями, можуть мати нижчу кореляційну розмірність через втрату складності сигналу.\nЕлектроенцефалограма (ЕЕГ): Сигнали ЕЕГ реєструють електричну активність мозку. Кореляційна розмірність може використовуватися для аналізу складності мозкової активності, яка може змінюватися залежно від різних когнітивних станів, стадій сну або неврологічних розладів. У здорових людей сигнали ЕЕГ у стані бадьорості та уваги можуть мати вищу кореляційну розмірність порівняно з сигналами у стадії сну, коли активність мозку є більш регулярною і синхронізованою.\nДихальні сигнали: Дихальні сигнали, такі як частота дихання або повітряний потік, також можуть бути проаналізовані за допомогою кореляційної розмірності. Складність цих сигналів може змінюватися залежно від таких факторів, як стрес, фізичне навантаження або наявність респіраторних захворювань. За нормального дихання може спостерігатися вища кореляційна розмірність, тоді як порушення в дихальних сигналах, наприклад, за обструктивного апное уві сні або дихальних розладів, можуть призвести до зниження кореляційної розмірності.\nАналіз ходи: Кореляційна розмірність може бути використана для аналізу моделей ходи. Вона допомагає зрозуміти складність рухів людини під час ходьби чи бігу. Зміни в кореляційній розмірності сигналів ходи можуть свідчити про зміну стабільності ходи або про наявність відхилень у ході, викликаних неврологічними або опорно-руховими захворюваннями.\nВаріативність динаміки серцевого ритму (ВСР): ВСР являє собою зміну часових інтервалів між послідовними ударами серця. Вона перебуває під впливом вегетативної нервової системи і відображає адаптивність і складність серцево-судинної системи. Вищий рівень ВСР, що відповідає вищій кореляційній розмірності, зазвичай асоціюється з кращим станом серцево-судинної системи та її адаптивністю до фізіологічних змін і змін навколишнього середовища. Її падіння може асоціюватися з аномальною динамікою серця.\nПослідовності ДНК: Кореляційна розмірність може бути використана і при аналізі послідовностей ДНК. Вона допомагає виявити самоподібні або фрактальні патерни всередині послідовностей, що може мати значення для розуміння генетичної складності, еволюційних зв’язків і регуляції генів. Висока кореляційна розмірність — висока складність ланцюжка ДНК. Мала кореляційна розмірність — спрощений ланцюжок ДНК.\nФінансові ринки: Вища кореляційна розмірність у даних часових рядів фінансового ринку свідчить про більшу складність та існування в їхній основі самоподібних моделей або фрактальних структур. Хаотична поведінка цін на акції може бути пов’язана з періодами високої волатильності та непередбачуваності. З іншого боку, нижча величина кореляційної розмірності може свідчити про більш передбачувані та менш складні рухи цін, що відповідає періодам стабільності або менш волатильним ринковим умовам.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Лабораторна робота № 6</span>"
    ]
  },
  {
    "objectID": "lab_6.html#хід-роботи",
    "href": "lab_6.html#хід-роботи",
    "title": "6  Лабораторна робота № 6",
    "section": "6.2 Хід роботи",
    "text": "6.2 Хід роботи\nРозглянемо як можна застосовувати зазначені показники в якості індикаторів кризових станів.\nСпочатку імпортуємо необхідні бібліотеки:\n\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport neurokit2 as nk\nimport yfinance as yf\nimport pandas as pd\nimport scienceplots\nfrom tqdm import tqdm\n\n%matplotlib inline\n\nДалі виконаємо налаштування формату виведення рисунків:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nsize = 16\nparams = {\n    'figure.figsize': (8, 6),            # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': size,                   # розмір фонтів рисунку\n    'lines.linewidth': 2,                # товщина ліній\n    'axes.titlesize': 'small',           # розмір титулки над рисунком\n    'axes.labelsize': size,              # розмір підписів по осям\n    'legend.fontsize': size,             # розмір легенди\n    'xtick.labelsize': size,             # розмір розмітки по осі Ох\n    'ytick.labelsize': size,             # розмір розмітки по осі Ох\n    \"font.family\": \"Serif\",              # сімейство стилів підписів \n    \"font.serif\": [\"Times New Roman\"],   # стиль підпису\n    'savefig.dpi': 300,                  # якість збережених зображень\n    'axes.grid': False                   # побудова сітки на самому рисунку\n}\n\nplt.rcParams.update(params)              # оновлення стилю згідно налаштувань\n\nНа цьому етапі скористаємось монофрактальними показниками для ідентифікації кризових явищ на ринку золота. Розглянемо ціну золота, наприклад, за період з 1 грудня 2001 року по 1 листопада 2023 року:\n\nsymbol = 'GC=F'          # Символ індексу\nstart = \"2001-01-01\"     # Дата початку зчитування даних\nend = \"2023-11-01\"       # Дата закінчення зчитування даних\n\ndata = yf.download(symbol)            # вивантажуємо дані\ntime_ser = data['Adj Close'].copy()   # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'                 # підпис по вісі Ох \nylabel = symbol                       # підпис по вісі Оу\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nУвага\n\n\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance, а із власного файлу. Зрозуміло, що й аналіз результатів, і висновки залежать від того з яким рядом ми працюємо\n\n\n\n\nsymbol = 'sMpa11'                  # Символ індексу\n\npath = \"databases\\sMpa11.txt\"      # шлях по якому здійснюється зчитування файлу\ndata = pd.read_csv(path,           # зчитування даних \n                   names=[symbol])\ntime_ser = data[symbol].copy()     # копіюємо значення кривої \n                                   # \"напруга-видовження\" до окремої змінної\n\nxlabel = r'$\\varepsilon$'          # підпис по вісі Ох \nylabel = symbol                    # підпис по вісі Оу\n\n\nВиводимо досліджуваний ряд:\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\n\n\n\nРис. 6.2: Динаміка щоденних змін ціни золота\n\n\n\n\n\nВизначимо функцію transformation() для стандартизації ряду:\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\n\n6.2.1 Обчислення показника Херста із використанням R/S-аналізу\nДля подальших розрахунків використовуватимемо бібліотеку neurokit2 та fathon. Другу з них можна встановити в наступний спосіб:\n\n!pip install fathon\n\nДалі імпортуємо саму бібліотеку та дотичні до неї модулі:\n\nimport fathon\nfrom fathon import fathonUtils as fu\n\nБібліотека neurokit містить необхідний метод для R/S-аналізу — fractal_hurst. Його синтаксис:\nfractal_hurst(signal, scale='default', corrected=True, show=False)\nПараметри:\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень або датафрейму бібліотеки pandas;\nscale (list) — список, що містить довжини вікон (кількість точок даних у кожній підмножині ряду), на які розбито сигнал;\ncorrected (bool) — якщо значення True, до вихідних даних буде застосовано поправочний коефіцієнт Аніса-Ллойда-Пітерса  [30] відповідно до очікуваного значення для окремих значень (R/S);\nshow (bool) — якщо значення True, виводить залежність \\((R/S)_n\\) від \\(n\\) (scale) у подвійному логарифмічному масштабі.\n\nПовертає:\n\nh (float) — показник Херста;\nkwargs — словник, що містить інформацію відносно використовуваних у процедурі параметрів.\n\nРозглянемо ступінь трендостійкості в динаміці ціни золота, використовуючи весь часовий ряд. Далі знайдемо значення показника Херста в рамках віконної процедури.\n\n6.2.1.1 Увесь часовий ряд\nПершочергово знайдемо значення прибутковостей для нашого ряду та стандартизуємо їх. Після цього виконаємо обчислення.\n\nsignal = time_ser.copy()\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nfor_rs = transformation(signal, ret_type) \n\nВиконуємо R/S-аналіз:\n\nh, info = nk.fractal_hurst(for_rs, corrected=False, show=True)\n\n\n\n\n\n\n\nРис. 6.3: Залежність значень R/S від скейлінгу\n\n\n\n\n\nЯк ми можемо бачити з Рис. 6.3, значення \\(h=0.53\\), що свідчить про подібність цінової динаміки золота до випадкового блукання. Але оскільки закони, що регулюють ринок, змінюються з часом, мають змінюватись і кореляції всередині системи, а тому коефіцієнт Херста також може змінюватись.\n\n\n6.2.1.2 Віконна процедура\nВизначимо функцію для побудови парних графіків:\n\ndef plot_pair(x_values, \n              y1_values,\n              y2_values,  \n              y1_label, \n              y2_label,\n              x_label, \n              file_name, clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(x_values, \n                  y1_values, \n                  \"b-\", label=fr\"{y1_label}\")\n    p2, = ax2.plot(x_values,\n                   y2_values, \n                   color=clr, \n                   label=y2_label)\n\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(f\"{y1_label}\")\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(file_name + \".jpg\")\n        \n    plt.show();\n\nПриступимо до віконної процедури:\n\n# встановлюємо параметри\nret_type = 4                   # вид ряду\nwindow = 250                   # ширина вікна\ntstep = 1                      # часовий крок вікна \nlength = len(time_ser.values)  # довжина самого ряду\ncorr = False                   # поправочний коефіцієнт Аніса-Ллойда-Пітерса\n\nH = []                         # масив для віконного Херсту\n\n\nfor i in tqdm(range(0,length-window,tstep)): # фрагменти довжиною window  \n                                             # з кроком tstep\n\n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо взаємну інформацію \n    h, _ = nk.fractal_hurst(fragm, corrected=corr, show=False)\n    \n    # та додаємо результат до масиву значень\n    H.append(h)\n\n  7%|▋         | 416/5663 [00:00&lt;00:10, 524.31it/s]100%|██████████| 5663/5663 [00:10&lt;00:00, 515.78it/s]\n\n\n\nnp.savetxt(f\"rs_hurst_name={symbol}_window={window}_step={tstep}_ \\\n           rettype={ret_type}_corrected={corr}.txt\" , H)\n\nВізуалізуємо результат:\n\nmeasure_label = r'$H$'\nfile_name = f\"rs_hurst_name={symbol}_window={window}_step={tstep}_ \\\n           rettype={ret_type}_corrected={corr}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          H, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name)\n\n\n\n\n\n\n\nРис. 6.4: Динаміка ціни золота та показника Херста\n\n\n\n\n\nНа Рис. 6.4 можемо бачити, що показник Херста зростає в передкризовий період та спадає під час кризи. Перед кризою динаміка ринку характеризується зростанням трендостійкості (персистентності), що відзеркалює зростання скорельованості дій трейдерів.\n\n\n\n6.2.2 Обчислення на основі DFA\nБібліотека fathon представляє інструментарій як для виконання класичного аналізу детрендованих флуктуацій, так і для його мультифрактального аналогу, мова про який піде в наступній лабораторній роботі.\n\n6.2.2.1 Для всього ряду\nСпочатку представимо значення \\(\\alpha\\) для всього ряду. Процедура розрахунків на основі бібліотеки fathon виглядатиме наступним чином:\nЗнаходимо стандартизовані прибутковості ряду:\n\nsignal = time_ser.copy()\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nfor_dfa = transformation(signal, ret_type) \n\n\ncumulat = fu.toAggregated(for_dfa) # знаходимо кумулятивні накопичення\n\nrev = True # чи повторювати розрахунок ф-ції флуктуацій з кінця\norder = 2  # порядок локального лінійного тренду \n\npydfa = fathon.DFA(cumulat) # ініціалізація об'єкту DFA\n                            # для виконання подальших обчислень\n\nwin_beg = 100               # початкова ширина сегментів\nwin_end = 2000              # кінцева ширина сегментів\n\nwins = fu.linRangeByStep(win_beg, win_end) # генеруємо масив \n                                           # лінійно розділених \n                                           # елементів.\n\nn, F = pydfa.computeFlucVec(wins, \n                            polOrd=order, \n                            revSeg=rev)    # знаходимо функцію флуктуацій\n\nH, H_intercept = pydfa.fitFlucVec()        # знаходимо показник альфа\n\nВиводимо залежність функції флуктуацій від масштабу:\n\npolyfit = np.polyfit(np.log(n), np.log(F), 1)\nfluctfit = np.exp(1) ** np.polyval(polyfit, np.log(n))\n\nБудуємо залежність функції флуктуацій від масштабу в подвійному логарифмічному масштабі:\n\nfig, ax = plt.subplots()\nfig.suptitle(\"Показник Херста на основі DFA\")\n\nax.scatter(\n        np.log(n),\n        np.log(F),\n        marker=\"o\",\n        zorder=1,\n        label=\"_no_legend_\",\n    )\n\nlabel = fr\"$\\alpha$ = {H:.2f}\"\nax.plot(np.log(n), np.log(fluctfit), \n        color=\"#E91E63\", zorder=2, \n        linewidth=3, label=label)\n\nax.set_ylabel(r'$\\ln{F_{2}(n)}$')\nax.set_xlabel(r'$\\ln{n}$')\n\nax.legend(loc=\"lower right\")\n\nplt.show()\n\n\n\n\n\n\n\nРис. 6.5: Логарифмічна залежність значень функції флуктуацій від скейлінгу\n\n\n\n\n\nПроцедура DFA показує, що значення ціни золота представляються скоріше антиперсистентними, але представлений результат доволі близький до того, що був отриманий за допомогою R/S-аналізу. Розглянемо значення \\(\\alpha\\) в рамках алгоритму ковзного вікна.\n\n\n6.2.2.2 Віконна процедура\nВизначимо наступні параметри:\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням \n                                                # та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nrev = True      # чи повторювати розрахунок ф-ції флуктуацій з кінця\norder = 2       # порядок поліноміального тренду\n\nperiods = 1\n\nwin_beg = 10             # початковий масштаб сегментів\nwin_end = window-1       # кінцевий масштаб сегментів\n\n\n\nlength = len(time_ser.values) # довжина ряду\n\nalpha = []               # масив показників альфа (Херста)\nD_f = []                 # фрактальна розмірність\nbeta = []                # показник спектральної щільності\ngamma = []               # показник автокореляції\n\nЗнайдемо показник Херста (\\(\\alpha\\)), фрактальну розмірність (\\(D_f\\)), показник спектральної щільності (\\(\\beta\\)) та показник автокореляції (\\(\\gamma\\)):\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # знаходимо кумулятивні накопичення\n    cumulat_wind = fu.toAggregated(fragm) \n\n    # ініціалізація об'єкту DFA\n    pydfa = fathon.DFA(cumulat_wind) \n\n    # генеруємо масив лінійно розділених елементів\n    wins = fu.linRangeByStep(win_beg, win_end) \n\n    # знаходимо функцію флуктуацій\n    n, F_wind = pydfa.computeFlucVec(wins, polOrd=order, revSeg=rev)    \n\n    # знаходимо показник альфа\n    H_wind, _ = pydfa.fitFlucVec()\n\n    # знаходимо фрактальну розмірність        \n    D = 2. - H_wind\n\n    # показник спектральної щільності\n    bi = 2. * H_wind - 1 \n\n    # показник автокореляції\n    gi = 2. - 2. * H_wind\n\n    alpha.append(H_wind)\n    D_f.append(D)\n    beta.append(bi)\n    gamma.append(gi)\n\n100%|██████████| 5663/5663 [00:55&lt;00:00, 102.04it/s]\n\n\nЗберігаємо абсолютні значення показників до текстових файлів:\n\nnp.savetxt(f\"alpha_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}.txt\", alpha)\nnp.savetxt(f\"D_f_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}.txt\", D_f)\nnp.savetxt(f\"beta_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}.txt\", beta)\nnp.savetxt(f\"gamma_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}.txt\", gamma)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_alpha = fr'$\\alpha$'\nlabel_d = fr'$D_f$'\nlabel_beta = fr'$\\beta$'\nlabel_gamma = fr'$\\gamma$'\n\nfile_name_alpha = f\"alpha_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}\"\nfile_name_d = f\"D_f_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}\"\nfile_name_beta = f\"beta_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}\"\nfile_name_gamma = f\"gamma_{symbol}_{window}_{tstep}_ \\\n            {ret_type}_{order}_{win_beg}_{win_end}\"\n\nВиводимо результати:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          alpha, \n          ylabel, \n          label_alpha,\n          xlabel,\n          file_name_alpha)\n\n\n\n\n\n\n\nРис. 6.6: Флуктуації ціни золота та показника альфа\n\n\n\n\n\nЯкщо порівнювати з R/S-аналізом, Рис. 6.6 демонструє, що DFA динаміка узагальненого показника Херста є набагато стабільнішою. Тепер ми здатні диференціювати значну частку крахових подій, що мали місце на ринку золота. Узагальнений Херст показує, що передкризові явища характеризуються зростанням трендостійкості ринку, підвищенням ступеня самоорганізації системи.\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          D_f, \n          ylabel, \n          label_d,\n          xlabel,\n          file_name_d)\n\n\n\n\n\n\n\nРис. 6.7: Коливання ціни золота та фрактальної розмірності\n\n\n\n\n\nРис. 6.7 показує, що \\(D_f\\) характеризується спадом при кризових станах. Це є індикатором того, що вищий ступень організованості ринку відзеркалюється в більш згладженій або менш волатильних флуктуаціях досліджуваного сигналу.\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          beta, \n          ylabel, \n          label_beta,\n          xlabel,\n          file_name_beta)\n\n\n\n\n\n\n\nРис. 6.8: Динаміка ціни золота та показника спектральної щільності\n\n\n\n\n\nСпектральна густина потужності \\(\\beta\\) Рівняння 6.5 зростає в кризові періоди, що свідчить про спад потужності сигналу на одиничному інтервалі частоти. Це також є свідченням зростання кореляційних властивостей системи.\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          gamma, \n          ylabel, \n          label_gamma,\n          xlabel,\n          file_name_gamma)\n\n\n\n\n\n\n\nРис. 6.9: Динаміка індексу золота та показника автокореляції\n\n\n\n\n\nЗ Рис. 6.9 видно, що показник \\(\\gamma\\) спадає в кризові та передкризові періоди. Це є показником сповільнення спаду функції автокореляції, що в свою чергу також вказує на зростання корельованності динаміки системи.\n\n\n\n6.2.3 Обчислення фрактальної розмірності Хігучі\nЯк уже зазначалося, фрактальна розмірність Хігучі є одним із різновидів фрактальної розмірності для часових рядів. Вона обчислюється шляхом реконструкції \\(k_{max}\\) кількості нових наборів даних. Для кожного відновленого набору даних обчислюється довжина кривої і відкладається проти відповідного \\(k_{max}\\)-значення в логарифмічній шкалі. HFD відповідає нахилу лінійного тренду за методом найменших квадратів.\nРозрахуємо оптимальне значення \\(k\\) для всього часового ряду. Бібліотека neurokit2 представляє готову процедуру для автоматизованого підбору даного параметру. Оптимальний \\(k_{max}\\) розраховується на основі точки, в якій значення фрактальної розмірності досягає плато для діапазону значень \\(k_{max}\\)  [17].\nСинтаксис даної функції виглядає наступним чином:\ncomplexity_k(signal, k_max='max', show=False)\nПараметри:\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень;\n\\(k_{max}\\) (Union[int, str, list], optional) — максимальна кількість інтервалів (має бути більше або дорівнювати 3), які потрібно перевірити. Якщо \\(k_{max}\\) = default, тоді вибирається максимально можливе значення, що відповідає половині довжини сигналу;\nshow (bool) — візуалізовується нахил кривої для обраного значення \\(k_{max}\\).\n\nПовертає:\n\nk (float) — оптимальний \\(k_{max}\\) часового ряду;\ninfo (dict) — словник, що містить додаткову інформацію про параметри, які використовуються для обчислення оптимального \\(k_{max}\\).\n\n\n6.2.3.1 Для всього ряду\nДля подальших розрахунків спочатку виконаємо перетворення ряду. Будемо використовувати вихідний часовий ряд для подальших розрахунків:\n\nsignal = time_ser.copy()\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nfor_higuchi = transformation(signal, ret_type) \n\nІ тепер отримаємо оптимальне значення \\(k_{max}\\) згідно зазначеної процедури:\n\nk_max, info = nk.complexity_k(for_higuchi, k_max=100, show=True)\n\n\n\n\n\n\n\nРис. 6.10: Залежність розмірності Хігучі від діапазону значень kmax\n\n\n\n\n\nТепер побудуємо залежність довжини сигналу від часового зміщення в логарифмічному масштабі. Для фрактального сигналу має зберігатися лінійна залежність. Бібліотека neurokit2 містить метод для розрахунку даної фрактальної розмірності. Синтаксис цієї процедури виглядає наступним чином:\nfractal_higuchi(signal, k_max='default', show=False, **kwargs)\nПараметри:\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень;\n\\(k_{max}\\) (Union[int, str, list], optional) — максимальна кількість інтервалів (має бути більше або дорівнювати 3), які потрібно перевірити;\nshow (bool) — візуалізовується нахил кривої для обраного значення \\(k_{max}\\).\n\nПовертає:\n\nHFD (float) — фрактальна розмірність Хігучі для досліджуваного часового ряду;\ninfo (dict) — словник, що містить додаткову інформацію про параметри, які використовуються для обчислення фрактальної розмірності Хігучі.\n\n\nhfd, info = nk.fractal_higuchi(for_higuchi, k_max=k_max, show=True)\n\n\n\n\n\n\n\nРис. 6.11: Залежність довжини сигналу від часового зміщення\n\n\n\n\n\nУ подальшому будемо послуговуватись отриманим оптимальним значенням для розрахунку розмірності Хігучі в рамках алгоритму ковзного вікна.\n\n\n6.2.3.2 Віконна процедура\nСкористаємось наступними параметрами:\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням \n                                                # та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nk_max_wind = 30                    # максимальне часове зміщення\n\nlength = len(time_ser.values)      # довжина ряду\n\nhfd_wind = []                      # масив показників Хігучі\n\nРозпочинаємо віконну процедуру:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо фрактальну розмірність Хігучі\n    higuchi, _ = nk.fractal_higuchi(fragm, \n                                    k_max=k_max_wind, \n                                    show=False)\n\n    # зберігаємо результат до масиву значень\n    hfd_wind.append(higuchi)\n\n100%|██████████| 5663/5663 [00:13&lt;00:00, 414.26it/s]\n\n\nЗберігаємо вихідні значення до текстового документа:\n\nnp.savetxt(f\"fd_higuchi_name={symbol}_kmax={k_max_wind}_\\\n           wind={window}_step={tstep}.txt\", hfd_wind)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_higuchi = fr'$HFD$'\n\nfile_name_higuchi = f\"fd_higuchi_name={symbol}_kmax={k_max_wind}_\\\n           wind={window}_step={tstep}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          hfd_wind, \n          ylabel, \n          label_higuchi,\n          xlabel,\n          file_name_higuchi)\n\n\n\n\n\n\n\nРис. 6.12: Динаміка індексу золота та фрактальної розмірності Хігучі\n\n\n\n\n\nЯк можна бачити з представленого рисунку, фрактальна розмірність Хігучі може працювати як індикатор або індикатор-передвісник кризових явищ. Видно, що даний показник починає спадати у передкризові періоди чи у сам момент кризи, вказуючи на зростання згладженності динаміки системи, ступеня кореляцій та трендостійкості динаміки ринку.\n\n\n\n6.2.4 Обчислення фрактальної розмірності Петросяна\nПетросян  [18] запропонував швидкий метод оцінки фрактальної розмірності шляхом перетворення сигналу в двійкову послідовність, з якої оцінюється фрактальна розмірність. Існує кілька варіацій алгоритму (neurokit2, наприклад, пропонує варіанти \"A\", \"B\", \"C\" або \"D\"), що відрізняються насамперед способом створення дискретної (символьної) послідовності (див. complexity_symbolize() для деталей). Найпоширеніший метод (\"C\", за замовчуванням) бінаризує сигнал за знаком послідовних різниць.\nБільшість з цих методів дискретизації припускають, що сигнал є періодичним (без лінійного тренду). Для усунення лінійних трендів може бути корисним лінійне детрендування.\nСинтаксис даної процедури має наступний вигляд:\nfractal_petrosian(signal, symbolize='C', show=False)\nПараметри:\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень;\nsymbolize (str) - метод перетворення неперервного вхідного сигналу в символьний (дискретний) сигнал. За замовчуванням присвоює 0 та 1 значенням нижче та вище середнього. Може мати значення None, що припускає дискретність вхідного сигналу;\nshow (bool) — виводить дискретизацію сигналу.\n\nПовертає:\n\nPFD (float) — фрактальна розмірність Петросяна для досліджуваного часового ряду;\ninfo (dict) — словник, що містить додаткову інформацію про параметри, які використовуються для обчислення фрактальної розмірності Петросяна.\n\nМи не розглядатимемо детально синтаксис функції complexity_symbolize(). Опишемо лише ті методи дискретизації, що дотичні до фрактальної розмірності Петросяна:\n\nметод A бінаризує сигнал за більшими та меншими значеннями порівняно із середнім значенням сигналу. Еквівалентом є method=\"mean\" (method=\"median\" також є допустимим);\nметод B використовує значення, що знаходяться в діапазоні \\(\\pm 1\\sigma\\), проти значень, що виходять за межі цього діапазону;\nметод C обчислює різницю між послідовними вибірками та бінаризує їх залежно від знаку;\nметод D відокремлює послідовні відліки, що перевищують \\(1\\sigma\\) сигналу, від інших менших змін.\n\nТепер розглянемо віконну динаміку показника.\n\n6.2.4.1 Віконна процедура\nОскільки більшість з даних методів дискретизації вимагають детрендування ряду, будемо виконувати розрахунки для прибутковостей ціни золота. Скористаємось наступними параметрами:\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням \n                                                # та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nsymb = \"B\"                    # тип дискретизації ряду\n\nlength = len(time_ser.values) # довжина ряду\n\npetr_wind = []                 # масив показників Петросяна\n\nРозпочинаємо віконну процедуру:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо фрактальну розмірність Петросяна\n    petrocian, _ = nk.fractal_petrosian(fragm, \n                                        symbolize=symb, \n                                        show=False)\n\n    # зберігаємо результат до масиву значень\n    petr_wind.append(petrocian)\n\n100%|██████████| 5663/5663 [00:04&lt;00:00, 1240.52it/s]\n\n\nЗберігаємо вихідні значення до текстового документа:\n\nnp.savetxt(f\"fd_petrosian_name={symbol}_method={symb}_\\\n           wind={window}_step={tstep}.txt\", petr_wind)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_petrocian = fr'$PFD$'\n\nfile_name_petrocian = f\"fd_petrosian_name={symbol}_method={symb}_\\\n           wind={window}_step={tstep}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          petr_wind, \n          ylabel, \n          label_petrocian,\n          xlabel,\n          file_name_petrocian)\n\n\n\n\n\n\n\nРис. 6.13: Динаміка ціни золота та фрактальної розмірності Петросяна\n\n\n\n\n\nРис. 6.13 показує, що показник Петросяна також спадає під час кризових подій і вказує на зростання періодизації ринку та синхронізації активності трейдерів у відповідні моменти часу.\n\n\n\n6.2.5 Обчислення фрактальної розмірності Каца\nОбчислимо фрактальну розмірність Каца. Евклідові відстані між послідовними точками сигналу підсумовуються і усереднюються, а також визначається максимальна відстань між початковою точкою і будь-якою іншою точкою у вибірці.\nФрактальна розмірність варіюється від 1.0 для прямих ліній, приблизно до 1.15 для випадкових блукань і наближається до 1.5 для найбільш “дивних” форм сигналу.\nСинтаксис процедури для розрахунку даної розмірності виглядає наступним чином:\nfractal_katz(signal)\nПараметри:\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\n\nПовертає:\n\nKFD (float) — фрактальна розмірність Каца для досліджуваного часового ряду;\ninfo (dict) — словник, що містить додаткову інформацію (наразі порожній, але повертається для узгодженості з іншими функціями).\n\n\n6.2.5.1 Віконна процедура\nОскільки даний показник є параметронезалежним, нам достатньо буде лише розміру часового вікна, кроку та типу ряду:\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням \n                                                # та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nlength = len(time_ser.values)      # довжина ряду\n\nkz_wind = []                      # масив показників Каца\n\nРозпочинаємо віконну процедуру:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо фрактальну розмірність Хігучі\n    katz, _ = nk.fractal_katz(fragm)\n\n    # зберігаємо результат до масиву значень\n    kz_wind.append(katz)\n\n 78%|███████▊  | 4423/5663 [00:00&lt;00:00, 5510.57it/s]100%|██████████| 5663/5663 [00:01&lt;00:00, 5507.52it/s]\n\n\nЗберігаємо вихідні значення до текстового документа:\n\nnp.savetxt(f\"fd_katz_name={symbol}_wind={window}_step={tstep}.txt\", kz_wind)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_katz = fr'$KFD$'\n\nfile_name_katz = f\"fd_katz_name={symbol}_wind={window}_step={tstep}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          kz_wind, \n          ylabel, \n          label_katz,\n          xlabel,\n          file_name_katz)\n\n\n\n\n\n\n\nРис. 6.14: Динаміка ціни золота та фрактальної розмірності Каца\n\n\n\n\n\nЗ рисунку видно, що фрактальна розмірність Каца також спадає у кризові та передкризові періоди і також є індикатором зростання ступеня корельованості системи в дані періоди.\n\n\n\n6.2.6 Обчислення фрактальної розмірності Шевчика\nАлгоритм цієї фрактальної розмірності був запропонований для обчислення фрактальної розмірності сигналів Шевчиком  [22]. Цей метод можна використовувати для швидкого вимірювання складності сигналу.\nСинтаксис методу:\nfractal_sevcik(signal)\nПараметри\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень.\n\nПовертає\n\nSFD (float) — фрактальна розмірність Севчика для досліджуваного часового ряду;\ninfo (dict) — словник, що містить додаткову інформацію (наразі порожній, але повертається для узгодженості з іншими функціями).\n\n\n6.2.6.1 Віконна процедура\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням \n                                                # та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nlength = len(time_ser.values)      # довжина ряду\n\nsfd_wind = []                      # масив показників Севчика\n\nРозпочинаємо віконну процедуру:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо фрактальну розмірність Севчика\n    sevcik, _ = nk.fractal_sevcik(fragm)\n\n    # зберігаємо результат до масиву значень\n    sfd_wind.append(sevcik)\n\n 56%|█████▌    | 3182/5663 [00:00&lt;00:00, 4571.63it/s]100%|██████████| 5663/5663 [00:01&lt;00:00, 4536.64it/s]\n\n\nЗберігаємо вихідні значення до текстового документа:\n\nnp.savetxt(f\"fd_cevcik_name={symbol}_wind={window}_step={tstep}.txt\", sfd_wind)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_sevcik = fr'$SFD$'\n\nfile_name_sevcik = f\"fd_cevcik_name={symbol}_wind={window}_step={tstep}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          sfd_wind, \n          ylabel, \n          label_sevcik,\n          xlabel,\n          file_name_sevcik)\n\n\n\n\n\n\n\nРис. 6.15: Динаміка ціни золота та фрактальної розмірності Шевчика\n\n\n\n\n\nБачимо, що фрактальна розмірність Шевчика реагує спадом на крахові події на ринку золота. Особливо характерними є спади під час криз 2008, 2011, 2015 та 2020 років. Цінові флуктуації золота під час зазначених кризових подій також характеризувалися зростанням персистентності (кореляцій).\n\n\n\n6.2.7 Обчислення фрактальної розмірності через нормалізовану щільність довжини\nЦе доволі простий показник, що відповідає середнім абсолютним послідовним різницям (стандартизованого) сигналу (np.mean(np.abs(np.diff(std_signal)))). Метод було розроблено для вимірювання складності сигналів дуже короткої тривалості (&lt; 30 відліків), і його можна використовувати, наприклад, коли цікавлять неперервні зміни фрактальної розмірності сигналу при обчисленні в межах ковзних вікон.\nСинтаксис процедури:\nfractal_nld(signal, corrected=False)\nПараметри:\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень;\ncorrected (bool) — якщо значення True, змінює масштаб вихідного значення фрактальної розмірності відповідно до степеневої моделі, щоб зробити його більш порівнянним з “істинним” значенням: FD = 1.9079*((NLD-0.097178)^0.18383). Зауважте, що це може призвести до np.nan, якщо результат різниці буде від’ємним.\n\nПовертає:\n\nNLDFD (float) — фрактальна розмірність;\ninfo (dict) — словник із додатковою інформацією по розрахункам.\n\n\n6.2.7.1 Віконна процедура\nДля цього показника нам також не потребується нічого зайвого:\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням \n                                                # та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nnld_corrected = True               # нормалізація фрактальної розмірності\n\nlength = len(time_ser.values)      # довжина ряду\n\nnldfd_wind = []                    # масив показників \n\nРозпочинаємо віконну процедуру:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо фрактальну розмірність \n    nld, _ = nk.fractal_nld(fragm, \n                            corrected=nld_corrected)\n\n    # зберігаємо результат до масиву значень\n    nldfd_wind.append(nld)\n\n100%|██████████| 5663/5663 [00:04&lt;00:00, 1286.46it/s]\n\n\nЗберігаємо вихідні значення до текстового документа:\n\nnp.savetxt(f\"fd_nld_name={symbol}_wind={window}_\\\n           step={tstep}_corrected={nld_corrected}.txt\", nldfd_wind)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_nld = fr'$NLDFD$'\n\nfile_name_nld = f\"fd_nld_name={symbol}_wind={window}_\\\n                step={tstep}_corrected={nld_corrected}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          nldfd_wind, \n          ylabel, \n          label_nld,\n          xlabel,\n          file_name_nld)\n\n\n\n\n\n\n\nРис. 6.16: Динаміка ціни золота та фрактальної розмірності через нормалізовану щільність довжини\n\n\n\n\n\nРис. 6.16 показує, що \\(NLDFD\\) спадає під час кризових та передкризових подій, що свідчить про зростання кореляцій у дані періоди.\n\n\n\n6.2.8 Обчислення фрактальної розмірності через нахил спектральної щільності потужності\nСкористаємось наступним методом бібліотеки neurokit2:\nfractal_psdslope(signal, method='voss1988', show=False, **kwargs)\nПараметри:\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень;\nmethod (str) — метод оцінки фрактальної розмірності за нахилом, може бути \"voss1988\" (за замовчуванням) або \"hasselman2013\";\nshow (bool) — якщо значення True, повертає графік залежності спектральної щільності потужності від частоти в логарифмічному масштабі;\nkwargs — інші аргументи, які слід передати до signal_psd().\n\nПовертає:\n\nslope (float) — оцінка фрактальної розмірності, отримана в результаті аналізу нахилу спектральної щільності потужності;\ninfo (dict) — словник, що містить додаткову інформацію про параметри, які використовуються для аналізу нахилу спектральної щільності потужності.\n\n\n6.2.8.1 Для всього ряду\nДля подальших розрахунків спочатку виконаємо перетворення ряду. Будемо використовувати вихідний часовий ряд для подальших розрахунків:\n\nsignal = time_ser.copy()\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nfor_psd = transformation(signal, ret_type) \n\nІ тепер виведемо графік залежності спектральної щільності потужності від частоти в логарифмічному масштабі:\n\npsdslope, info = nk.fractal_psdslope(for_psd,\n                                     method=\"voss1988\",\n                                     show=True)\n\n\n\n\n\n\n\nРис. 6.17: Залежність спектральної щільності потужності від частоти в логарифмічному масштабі\n\n\n\n\n\nОчевидно, нахил спектральної щільності потужності на різних частотах має лінійну залежність, а кут нахилу прямої побудованої за спектром, близький до -2, що вказує на те, що динаміка індексу золота близька до дробового броунівського руху.\nТепер розглянемо варіацію кута нахилу спектра в рамках алгоритму ковзного вікна.\n\n\n6.2.8.2 Віконна процедура\nВстановимо наступні параметри:\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням \n                                                # та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nmethod_psd = \"voss1988\"        # метод для розрахунку\n                               # спектральної щільності\n\nlength = len(time_ser.values)  # довжина ряду\n\npsd_wind = []                  # масив показників \n\nРозпочинаємо віконну процедуру:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо фрактальну розмірність \n    psd, _ = nk.fractal_psdslope(fragm, method=method_psd)\n\n    # зберігаємо результат до масиву значень\n    psd_wind.append(psd)\n\n  6%|▌         | 334/5663 [00:00&lt;00:13, 405.54it/s]100%|██████████| 5663/5663 [00:13&lt;00:00, 416.58it/s]\n\n\nЗберігаємо вихідні значення до текстового документа:\n\nnp.savetxt(f\"fd_psd_name={symbol}_method{method_psd}_\\\n           wind={window}_step={tstep}.txt\", psd_wind)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_psd = fr'$PSDFD$'\n\nfile_name_psd = f\"fd_psd_name={symbol}_method{method_psd}_\\\n                wind={window}_step={tstep}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          psd_wind, \n          ylabel, \n          label_psd,\n          xlabel,\n          file_name_psd)\n\n\n\n\n\n\n\nРис. 6.18: Флуктуації ціни золота та фрактальної розмірності за нахилом спектральної щільності потужності\n\n\n\n\n\nРисунок вище показує, що даний показник також реагує спадом на кризові та передкризові події, вказуючи на зростання автокореляції часового ряду. Також зрозуміло, що має місце варіації нахилу спектра щільності потужності. В одні моменти часу динаміка сигналу може бути подібна до броунівського руху, а в інші до білого шуму.\n\n\n\n6.2.9 Обчислення кореляційної розмірності\nКореляційна розмірність (також позначається як \\(D_2\\)) є нижньою границею оцінки фрактальної розмірності досліджуваного фазового простору.\nСпочатку здійснюється реконструкція фазового простору сигналу згідно методу часової затримки, і далі обчислюються відстані між усіма точками траєкторії. Потім обчислюється “кореляційна сума”, яка є часткою пар точок, відстань між якими менша за заданий радіус. Остаточна кореляційна розмірність апроксимується графіком залежності кореляційної суми від радіусу багатовимірного околу досліджуваних траєкторій у логарифмічному масштабі.\nЦю розмірність можна викликати через fractal_correlation(). Її синтаксис виглядає наступним чином:\nfractal_correlation(signal, delay=1, dimension=2, radius=64, show=False, **kwargs)\nПараметри:\n\nsignal (Union[list, np.array, pd.Series]) — сигнал (тобто часовий ряд) у вигляді вектора значень;\ndelay (int) — часова затримка (\\(\\tau\\));\ndimension (int) — розмірність вкладень (\\(m\\));\nradius (Union[str, int, list]) — послідовність радіусів для перевірки. Якщо передано ціле число, буде отримано експоненціальну послідовність довжиною заданим скалярним значенням radius у межах від 2.5% до 50% від діапазону відстані. Методи, реалізовані в інших пакетах, можна використовувати, вказуючи \"nolds\", \"Corr_Dim\" або \"boon2008\";\nshow (bool) — графік кореляційної розмірності, якщо True. За замовчуванням — False;\nkwargs — інші аргументи для передачі (наразі не використовуються).\n\nПовертає:\n\ncd (float) — кореляційна розмірність часового ряду;\ninfo (dict) — словник, що містить додаткову інформацію про параметри, які використовуються для обчислення кореляційної розмірності.\n\n\n6.2.9.1 Для всього часового ряду\nРозглянемо залежність кореляційної суми від радіусу для всього часового ряду. Перш за все виконаємо перетворення ряду:\n\nsignal = time_ser.copy()\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nfor_corr = transformation(signal, ret_type) \n\nТепер розрахуємо кореляційну розмірність, побудувавши залежність кореляційної суми від радіусу в логарифмічному масштабі:\n\ncd, info = nk.fractal_correlation(for_corr,\n                                  delay=1, \n                                  dimension=1,\n                                  radius=\"nolds\", \n                                  show=True)\n\n\n\n\n\n\n\nРис. 6.19: Залежність кореляційної суми від радіусу багатовимірного околу досліджуваних траєкторій\n\n\n\n\n\nЯк ми можемо бачити, кореляційна сума дійсно має лінійну залежність для різних значень радіусу околу певної траєкторії, що вказує на фрактальність системи. Тепер подивимось як варіюється значення кореляційної розмірності в періоди турбулентності.\n\n\n6.2.9.2 Віконна процедура\nДля цього показника визначимо наступні параметри:\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням \n                                                # та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nd_wind = 2          # розмірність вкладень\ntau_wind = 1        # часова затримка\nrad_wind = \"nolds\"  # метод для визначення масиву радіусів\n\nlength = len(time_ser.values)      # довжина ряду\n\ncorr_wind = []                     # масив показників \n\nРозпочинаємо віконну процедуру:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # розраховуємо фрактальну розмірність \n    cd_wind, _ = nk.fractal_correlation(fragm,\n                                        delay=tau_wind, \n                                        dimension=d_wind,\n                                        radius=rad_wind)\n\n    # зберігаємо результат до масиву значень\n    corr_wind.append(cd_wind)\n\n100%|██████████| 5663/5663 [00:18&lt;00:00, 308.32it/s]\n\n\nЗберігаємо вихідні значення до текстового документа:\n\nnp.savetxt(f\"fd_correlation_name={symbol}_wind={window}_\\\n                step={tstep}_dim={d_wind}_tau={tau_wind}_\\\n                radius={rad_wind}.txt\", corr_wind)\n\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nlabel_cd = fr'$CD$'\n\nfile_name_cd = f\"fd_correlation_name={symbol}_wind={window}_\\\n                step={tstep}_dim={d_wind}_tau={tau_wind}_\\\n                radius={rad_wind}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          corr_wind, \n          ylabel, \n          label_cd,\n          xlabel,\n          file_name_cd)\n\n\n\n\n\n\n\nРис. 6.20: Динаміка ціни золота та кореляційної фрактальної розмірності\n\n\n\n\n\nРис. 6.20 демонструє, що кореляційна розмірність для індексу золота також спадає у кризові та передкризові періоди, вказуючи на зростання корельованості теперішніх цін на золото з попередніми. Можна сказати й по іншому: у період криз трейдери починають самоорганізовуватись та колективно скупати або продавати відповідний актив; іншими словами, їх динаміка стає більш синхронною. Оскільки кореляційна розмірність вимірюється для траєкторій фазового простору, спад цього показника свідчить про зростання щільності досліджуваних траєкторій. Тобто, фазовий простір стає більш розрідженим, а всі його траєкторії концентрованими лише в одній конкретній області, що є індикатором згуртованості прихованих змінних досліджуваної системи.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Лабораторна робота № 6</span>"
    ]
  },
  {
    "objectID": "lab_6.html#завдання-для-виконання",
    "href": "lab_6.html#завдання-для-виконання",
    "title": "6  Лабораторна робота № 6",
    "section": "6.3 Завдання для виконання",
    "text": "6.3 Завдання для виконання\n\nВибрати із запропонованої бази даних варіант завдання\nВиконати дослідження фрактальних характеристик заданих часових рядів. Зберегти результати в окремому файлі\nПорівняти значення коефіцієнтів Херста, одержаних методом R/S-аналізу та DFA\nПровести повний аналіз різних методів розрахунку монофрактальних розмірностей\nДати інтерпретацію отриманим результатам\n\n\n\n\n\n[1] B. B. Mandelbrot and B. B. Mandelbrot, The Fractal Geometry of Nature, Vol. 1 (WH freeman New York, 1982).\n\n\n[2] B. B. Mandelbrot, C. J. G. Evertsz, and Y. Hayakawa, Exactly Self-Similar Left-Sided Multifractal Measures, Phys. Rev. A 42, 4528 (1990).\n\n\n[3] H. F. Jelinek, N. Elston, and B. Zietsch, Fractal Analysis: Pitfalls and Revelations in Neuroscience, in Fractals in Biology and Medicine, edited by G. A. Losa, D. Merlini, T. F. Nonnenmacher, and E. R. Weibel (Birkhäuser Basel, Basel, 2005), pp. 85–94.\n\n\n[4] H. Steinhaus, Length, Shape and Area, in Colloquium Mathematicum, Vol. 3 (Polska Akademia Nauk. Instytut Matematyczny PAN, 1954), pp. 1–13.\n\n\n[5] A. Vulpiani, Lewis Fry Richardson: Scientist, Visionary and Pacifist, Lettera Matematica 2, 121 (2014).\n\n\n[6] B. Hayes, Computing Science: Statistics of Deadly Quarrels, American Scientist 90, 10 (2002).\n\n\n[7] B. Mandelbrot, How Long Is the Coast of Britain? Statistical Self-Similarity and Fractional Dimension, Science 156, 636 (1967).\n\n\n[8] S. V. Bozhokin and D. A. Parshin, Fractals and Multifractals: Textbook (Scientific; Publishing Center \"Regular; Chaotic Dynamics\", 2001).\n\n\n[9] T. Gneiting, H. Ševčíková, and D. B. Percival, Estimators of Fractal Dimension: Assessing the Roughness of Time Series and Spatial Data, Statistical Science 27, 247 (2012).\n\n\n[10] K. Falconer, Fractal Geometry: Mathematical Foundations and Applications (John Wiley & Sons, 2003).\n\n\n[11] B. B. Mandelbrot and J. A. Wheeler, The Fractal Geometry of Nature, American Journal of Physics 51, 286 (1983).\n\n\n[12] H. E. Hurst, Long-Term Storage Capacity of Reservoirs, Transactions of the American Society of Civil Engineers 116, 770 (1951).\n\n\n[13] H. E. Hurst, A Suggested Statistical Model of Some Time Series Which Occur in Nature, Nature 180, 494 (1957).\n\n\n[14] C.-K. Peng, S. V. Buldyrev, S. Havlin, M. Simons, H. E. Stanley, and A. L. Goldberger, Mosaic Organization of DNA Nucleotides, Phys. Rev. E 49, 1685 (1994).\n\n\n[15] Z.-Q. Jiang, W.-J. Xie, and W.-X. Zhou, Testing the Weak-Form Efficiency of the WTI Crude Oil Futures Market, Physica A: Statistical Mechanics and Its Applications 405, 235 (2014).\n\n\n[16] T. Higuchi, Approach to an Irregular Time Series on the Basis of the Fractal Theory, Physica D: Nonlinear Phenomena 31, 277 (1988).\n\n\n[17] C. F. Vega and J. Noel, Parameters Analyzed of Higuchi’s Fractal Dimension for EEG Brain Signals, in 2015 Signal Processing Symposium (SPSympo) (2015), pp. 1–5.\n\n\n[18] A. Petrosian, Kolmogorov Complexity of Finite Sequences and Recognition of Different Preictal EEG Patterns, in Proceedings Eighth IEEE Symposium on Computer-Based Medical Systems (1995), pp. 212–217.\n\n\n[19] R. Esteller, G. Vachtsevanos, J. Echauz, and B. Litt, A Comparison of Waveform Fractal Dimension Algorithms, IEEE Transactions on Circuits and Systems I: Fundamental Theory and Applications 48, 177 (2001).\n\n\n[20] C. Goh, B. Hamadicharef, G. T. Henderson, and E. C. Ifeachor, Comparison of Fractal Dimension Algorithms for the Computation of EEG Biomarkers for Dementia, in 2nd International Conference on Computational Intelligence in Medicine and Healthcare (CIMED2005) (Professor José Manuel Fonseca, UNINOVA, Portugal, Lisbon, Portugal, 2005).\n\n\n[21] M. J. Katz, Fractals and the Analysis of Waveforms, Computers in Biology and Medicine 18, 145 (1988).\n\n\n[22] C. Sevcik, A Procedure to Estimate the Fractal Dimension of Waveforms, (2010).\n\n\n[23] A. Kalauzi, T. Bojić, and L. Rakić, Extracting Complexity Waveforms from One-Dimensional Signals, Nonlinear Biomedical Physics 3, 1 (2009).\n\n\n[24] A. Kalauzi, T. Bojić, and L. Rakić, Extracting Complexity Waveforms from One-Dimensional Signals, Nonlinear Biomedical Physics 3, (2009).\n\n\n[25] F. Hasselman, When the Blind Curve Is Finite: Dimension Estimation and Model Inference Based on Empirical Waveforms, Frontiers in Physiology 4, (2013).\n\n\n[26] R. F. Voss, Fractals in Nature: From Characterization to Simulation, in The Science of Fractal Images, edited by H.-O. Peitgen and D. Saupe (Springer New York, New York, NY, 1988), pp. 21–70.\n\n\n[27] P. Grassberger and I. Procaccia, Measuring the Strangeness of Strange Attractors, Physica D: Nonlinear Phenomena 9, 189 (1983).\n\n\n[28] P. Grassberger and I. Procaccia, Characterization of Strange Attractors, Phys. Rev. Lett. 50, 346 (1983).\n\n\n[29] P. Grassberger, Generalized Dimensions of Strange Attractors, Physics Letters A 97, 227 (1983).\n\n\n[30] A. A. Anis and E. H. Lloyd, The Expected Value of the Adjusted Rescaled Hurst Range of Independent Normal Summands, Biometrika 63, 111 (1976).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Лабораторна робота № 6</span>"
    ]
  },
  {
    "objectID": "lab_7.html",
    "href": "lab_7.html",
    "title": "7  Лабораторна робота № 7",
    "section": "",
    "text": "7.1 Теоретичні відомості",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Лабораторна робота № 7</span>"
    ]
  },
  {
    "objectID": "lab_7.html#теоретичні-відомості",
    "href": "lab_7.html#теоретичні-відомості",
    "title": "7  Лабораторна робота № 7",
    "section": "",
    "text": "7.1.1 Визначення мультифракталів\nУ цій лабораторній ми викладемо основи теорії мультифракталів — неоднорідних фрактальних об’єктів, для повного опису яких, на відміну від регулярних фракталів, недостатньо введення лише однієї величини, його фрактальної розмірності \\(D\\), а потрібен цілий спектр таких розмірностей, кількість яких, взагалі кажучи, нескінченна. Причина цього полягає в тому, що поряд із суто геометричними характеристиками, які визначаються величиною \\(D\\), такі фрактали характеризуються й деякими специфічними статистичними властивостями.\nПростіше всього пояснити, що розуміється під “неоднорідним фракталом” на прикладі трикутника Серпинського, отриманого за допомогою методу випадкових ітерацій (Рис. 7.1).\n\n\n\n\n\n\nРис. 7.1: Трикутник Серпинського, області якого згенеровані з різними ймовірностями\n\n\n\nПрипустимо, що в методі випадкових ітерацій ми тепер із якоїсь причини віддали перевагу одній із вершин трикутника, наприклад, вершині А, і стали вибирати її з імовірністю 90%. Дві ж інші вершини В і С для нас рівноцінні, але на їхню частку тепер припадає всього лише по 5%. Результат такої несиметричної гри” зображено нижче на рисунку вище.\nВидно, що точки всередині трикутника АВС розподілені тепер вкрай нерівномірно. Більша їх частина перебуває біля вершини А та її прообразів. Водночас у вершин В і С (і їхніх прообразів) їх є вкрай мало. Проте, за звичайною термінологією, ця множина точок (за умови прагнення числа ітерацій до нескінченності) є фракталом, тому що збереглася основна властивість фрактала — самоподібність. Дійсно, трикутник DFC, хоча в ньому у 20 разів менше точок, за своїми статистичними і геометричними властивостями повністю подібний до великого трикутника АВС. Так само, як і у великому трикутнику, точки в ньому концентруються здебільшого поблизу вершини D — аналогу вершини А.\nРис. 7.2 більш детально демонструє результуючий розподіл точок по трикутнику Серпинського. Цифри в кожному з маленьких трикутників показують його відносну заселеність точками множини.\n\n\n\n\n\n\nРис. 7.2: Розподіл точок по трикутнику Серпинського, представленого на попередньому рисунку\n\n\n\nОднак, не дивлячись на нерівномірність розподілу точок фрактала, фрактальна розмірність залишилась при цьому такою ж, \\(D=\\ln{3}/\\ln{2}\\). Покриття цієї множини все меншими трикутниками можна здійснити по тому ж алгоритму, що й раніше. Таке співпадіння змушує замислитись над пошуком нових кількісних характеристик, котрі могли б відрізнити нерівномірний розподіл точок від рівномірного.\nІнший, складніший приклад неоднорідного фрактала, який ми б хотіли ще навести, показано на наступному рисунку. Ліворуч продемонстровано великий квадрат зі стороною, що дорівнює одиниці, який на цьому (нульовому) етапі повністю покриває собою деяку фрактальну множину точок \\(M\\). На наступному (першому) етапі, у центрі рисунка, показано, як ту саму множину можна покрити трьома меншими квадратами зі сторонами \\(l_1=1/2, \\ l_2=l_3=5/16\\), у яких, відповідно, міститься частка \\(p_1=1/2, \\ p_2=1/3\\) та \\(p_3=1/6\\) усіх точок.\n\n\n\n\n\n\nРис. 7.3: Приклад мультифрактала, що підкоряється ренормалізаційній схемі\n\n\n\nНаступний етап покриття (зображений на рисунку праворуч) містить уже 9 квадратів зі сторонами \\(l_{1}^{2}=1/4, \\ l_{1}l_{2}=l_{1}l_{3}=5/32\\) (у нижньому правому куті) і \\(l_{2}l_{1}=5/32, \\ l_{2}^{2}=l_{2}l_{3}=25/256\\) (угорі праворуч і ліворуч). Відносна заселеність цих квадратів точками множини показана на рисунку. Вона відповідає добутку чинників заселеності (імовірностей): \\(p_{1}^{2}=1/4, \\ p_{1}p_{2}=1/6, \\ p_{1}p_{3}=1/12\\) — для нижньої правої групи, \\(p_{2}p_{1}=1/6, \\ p_{2}^{2}=1/9, \\ p_{2}p_{3} = 1/18\\) — для верхньої лівої та \\(p_{3}p_{1}=1/12, \\ p_{3}p_{2}=1/18, \\ p_{3}^{2}=1/36\\) — для верхньої правої групи. Зазначимо, що є чітка відповідність між заселеністю квадрата \\(p_{j}p_{i}\\) і його розмірами \\(l_{i}l_{i}\\).\nПодальший процес розбиття і покриття множини \\(M\\) здійснюється згідно із цією ренормалізаційною схемою. Кожен квадрат, що має на \\(n\\)-му кроці розмір \\(l\\) і заселеність \\(р\\), замінюється на \\(n+1\\) кроці трьома квадратами з розмірами \\(ll_{1}, \\ ll_{2}, \\ ll_{3}\\) і заселеностями \\(pp_{1}, \\ pp_{2}, \\ pp_{3}\\) відповідно, розміщеними таким самим чином відносно один одного, як показано на Рис. 7.3.\nДвоє з розглянутих вище випадки являють собою приклади неоднорідних фракталів. Під словом “неоднорідний” ми тут розуміємо нерівномірний розподіл точок множини по фракталу або нерівномірний розподіл малих та великих флуктуацій у часовому ряді. Причина неоднорідності в попередніх випадках одна й та сама — різні ймовірності заповнення геометрично однакових елементів фрактала, або в загальному випадку невідповідність імовірностей заповнення геометричним розмірам відповідних областей. Такі неоднорідні фрактальні об’єкти в літературі називають мультифракталами, і їх вивченням ми й займемося надалі.\n\n\n7.1.2 Узагальнені фрактальні розмірності \\(D_{q}\\)\nДамо загальне визначення мультифрактала. Розглянемо фрактальний об’єкт, що займає якусь обмежену ділянку \\(\\Omega\\) розміру \\(L\\) у Евклідовому просторі з розмірністю \\(d\\). Нехай на якомусь етапі його побудови він являє собою множину з \\(N \\gg 1\\) точок, якось розподілених у цій області. Ми будемо припускати, що врешті-решт \\(N \\to \\infty\\). Прикладом такої множини може слугувати трикутник Серпінського, побудований методом випадкових ітерацій. Кожен крок ітераційної процедури додає до цієї множини одну нову точку.\nРозіб’ємо всю область \\(\\Omega\\) на кубічні клітинки зі стороною \\(\\varepsilon \\ll L\\) та об’ємом \\(\\varepsilon^{d}\\). Далі нас будуть цікавити тільки зайняті клітинки, у яких міститься хоча б одна точка. Нехай номер зайнятих комірок \\(i\\) змінюється в межах \\(і=1, 2,..., N(\\varepsilon)\\), де \\(N(\\varepsilon)\\) — сумарна кількість зайнятих клітинок, яка, звісно, залежить від розміру клітинки \\(\\varepsilon\\).\nНехай \\(n_{i}(\\varepsilon)\\) представляє собою кількість точок у клітинці з номером \\(i\\), тоді величина \\(p_{i}(\\varepsilon) = \\lim_{N\\to\\infty}n_{i}(\\varepsilon)/{N}\\) представляє собою ймовірність того, що навмання взята точка з нашої множини знаходиться в комірці \\(i\\). Інакше кажучи, ймовірності \\(р_{i}\\) характеризують відносну заселеність комірок. З умови нормування ймовірності випливає, що\n\\[\n\\sum_{i=1}^{N(\\varepsilon)}p_{i}(\\varepsilon)=1.\n\\]\nУведемо тепер у розгляд узагальнену статистичну суму \\(Z(q,\\varepsilon)\\), що характеризується показником ступеня \\(q\\), який може набувати будь-яких значень в інтервалі \\(-\\infty&lt;q&lt;+\\infty\\)\n\\[\nZ(q,\\varepsilon)=\\sum_{i=1}^{N(\\varepsilon)}p_{i}^{q}(\\varepsilon).\n\\]\nСпектр узагальнених фрактальних розмірностей \\(D_{q}\\), що характеризує даний розподіл точок в області \\(\\Omega\\), визначається за допомогою співвідношення \\(D_{q} = \\tau(q)\\big/(q-1)\\), де функція \\(\\tau(q)\\) має вид\n\\[\n\\tau(q)=\\lim_{\\varepsilon\\to 0} \\ln{Z(q,\\varepsilon)} \\big/ \\ln{\\varepsilon}.\n\\]\nЯк ми покажемо нижче, якщо \\(D_{q}=D=\\text{const}\\), тобто не залежить від \\(q\\), то дана множина точок являє собою звичайний, регулярний фрактал, який характеризується лише однією величиною — фрактальною розмірністю \\(D\\). Навпаки, якщо функція \\(D_{q}\\) якось змінюється з \\(q\\), то розглянута множина точок представляє мультифрактал.\nТаким чином, мультифрактал у загальному випадку характеризується деякою нелінійною функцією \\(\\tau(q)\\), що визначає поведінку статистичної суми \\(Z(q,\\varepsilon)\\) при \\(\\varepsilon\\to 0\\)\n\\[\nZ(q,\\varepsilon)=\\sum_{i=1}^{N(\\varepsilon)}p_{i}^{q}(\\varepsilon) \\approx \\varepsilon^{\\tau(q)}.\n\\tag{7.1}\\]\nСлід мати на увазі, що в реальній ситуації ми завжди маємо скінченне, хоча й дуже велике число дискретних точок \\(N\\), тому при комп’ютерному моделюванні конкретної множини граничний перехід \\(\\varepsilon\\to 0\\) треба виконувати з обережністю, пам’ятаючи, що йому завжди передує ліміт \\(N \\to 0\\).\nПокажемо тепер, як поводиться узагальнена статистична сума у випадку звичайного регулярного фрактала з фрактальною розмірністю \\(D\\). У цьому випадку в усіх зайнятих комірках міститься однакова кількість точок, \\(n_{i}(\\varepsilon) = N \\big/ N(\\varepsilon)\\), тобто фрактал представляється однорідним. Тоді очевидно, що відносні населеності клітинок, \\(p_{i}(\\varepsilon)=1/N(\\varepsilon)\\), також однакові, і узагальнена статистична сума набуває вигляду\n\\[\nZ(q,\\varepsilon) = N^{1-q}(\\varepsilon).\n\\tag{7.2}\\]\nВрахуємо тепер, що, згідно визначеню фрактальної розмірності \\(D\\), кількість зайнятих клітинок при достатньо малому \\(\\varepsilon\\) поводить себе наступним чином:\n\\[\nN(\\varepsilon) \\approx \\varepsilon^{-D}.\n\\tag{7.3}\\]\nПідставляючи (7.3) у (7.2), і порівнюючи з (7.1), отримуємо\n\\[\n\\varepsilon^{\\tau(q)} = \\varepsilon^{-D(1-q)} \\to \\tau(q)=(q-1)D.\n\\tag{7.4}\\]\nМи приходимо до висновку, що у випадку звичайного фрактала функція (7.4) є лінійною. Тоді всі \\(D_{q}\\) дійсно не залежать від \\(q\\). Фрактал у якого всі узагальнені фрактальні розмірності \\(D_{q}\\) співпадають називається монофракталом.\nЯкщо розподіл точок по клітинкам неоднаковий, тоді фрактал називається неоднорідним, тобто представляє із себе мультифрактал, і для його характеристики необхідний цілий спектр узагальнених фрактальних розмірностей \\(D_{q}\\), кількість котрих, у загальному випадку, нескінченна.\nТак, наприклад, при \\(q \\to +\\infty\\) основний внесок в узагальнену статистичну суму (7.1) вносять комірки, що містять найбільшу кількість частинок \\(n_{i}\\) у них і, відповідно, що характеризуються найбільшою ймовірністю їх заповнення \\(p_{i}\\). Навпаки, при \\(q \\to -\\infty\\) основний внесок в узагальнену статистичну суму вносять найбільш розрідженні комірки з найменшою ймовірністю їх заповнення \\(p_{i}\\). Таким чином, функція \\(D_{q}\\) показує, наскільки неоднорідним представляється досліджувана множина точок \\(\\Omega\\).\nУ подальшому для характеристики розподілу точок необхідно знати не тільки функцію \\(\\tau(q)\\), але і її похідну:\n\\[\n\\frac{d\\tau(q)}{dq} = \\lim_{\\varepsilon\\to 0} \\sum_{i=1}^{N(\\varepsilon)}p_{i}^{q}\\ln{p_{i}} \\Bigg/ \\left( \\sum_{i=1}^{N(\\varepsilon)}p_{i}^{q} \\right)\\ln{\\varepsilon}.\n\\]\nЦя похідна має важливий фізичний зміст, який буде продемонстровано пізніше. Зараз знову зазначимо, що для мультифрактальної системи вона не залишається константною і змінюється з \\(q\\).\n\n\n7.1.3 Функція мультифрактального спектра \\(f(\\alpha)\\)\n\n7.1.3.1 Спектр фрактальних розмірностей\nУ попередньому пункті ми ввели поняття мультифрактала — об’єкта, що представляє собою неоднорідний фрактал. Для його опису ми ввели множину узагальнених фрактальних розмірностей \\(D_{q}\\), де \\(q\\) приймає будь-які значення в інтервалі \\(-\\infty&lt;q&lt;+\\infty\\). Однак величини \\(D_{q}\\) не є, строго кажучи, фрактальними розмірностями в загальному розумінні цього слова.\nТому часто поряд із ними для характеристики мультифрактальної множини використовують так звану функцію мультифрактального спектра \\(f(\\alpha)\\) (спектр сингулярностей мультифрактала), до якої, як ми побачимо надалі, більше підходить термін фрактальна розмірність. Ми покажемо, що величина \\(f(\\alpha)\\) фактично дорівнює хаусдорфовій розмірності якоїсь однорідної фрактальної підмножини із вихідної множини \\(\\Omega\\), що дає домінантний внесок у статистичну суму при заданій величині \\(q\\).\nОднією з основних характеристик мультифрактала є набір імовірностей \\(р_{i}\\), що показують відносну заселеність клітинок \\(\\varepsilon\\), якими ми покриваємо цю множину. Чим менший розмір клітинки, тим менша величина її заселеності. Для самоподібних множин залежність \\(p_{i}\\) від розміру клітинки \\(\\varepsilon\\) має степеневий характер:\n\\[\np_{i}(\\varepsilon) \\approx \\varepsilon^{\\alpha_{i}},\n\\]\nде \\(\\alpha_{i}\\) являє собою деякий показник ступеня (різний для різнок клітинок \\(i\\)).\n\n\n\n\n\n\nДодатково по \\(\\alpha\\)\n\n\n\nСпрямовуючи значення \\(\\varepsilon\\) до нуля, фрактальність можна розглядати локально для кожної точки (елемента) досліджуваної системи, і таким чином показник \\(\\alpha\\) є локальною фрактальною розмірністю. Його ще називають показником Гьолдера або силою сингулярності\nМожемо спостерігати саме степеневу залежність, оскільки, вочевидь, розподіл маси (флуктуацій) концентрується з різною “силою” \\(\\alpha\\), тож і ймовірнісна міра змінюється пропорційно розмірам вікон \\(\\varepsilon\\)\n\n\n\n\n\n\n\n\nРис. 7.4: Схематичне представлення залежності сили сингулярності та густини порівняно з околицями\n\n\n\nСірий масштаб являє собою ймовірнісну міру для кожної локації, як показано на кожній панелі. На рисунку (a) тільки \\(i\\)-та локація має ненульову щільність, інші місця порожні. Імовірнісна міра на комірці залишається \\(\\rho\\), навіть коли розмір клітинки \\(\\varepsilon\\) збільшується. \\(\\varepsilon\\) збільшується, що підкреслюється жирною лінією. Проте, через те, що далі ми не спостерігаємо зростання щільності, показник \\(\\alpha\\) залишається нульовим. На Рис. 7.22 (b) усі комірки мають однакову щільність. Імовірнісні міри комірок дорівнюють \\(\\rho\\), \\(9\\rho\\) і \\(25\\rho\\) для найменшої, другої найменшої та найбільшої комірки (виділено жирною лінією). Таким чином, сила сингулярності \\(i\\)-го осередку дорівнює 2. На Рис. 7.22 (c) \\(i\\)-й осередок є розрідженим порівняно з навколишніми осередками. Імовірнісна міра осередків дорівнює \\(\\rho\\), \\(27\\rho\\) і \\(125\\rho\\) для найменшого, другого найменшого і найбільшого осередку (виділено жирною лінією). Таким чином, сила сингулярності \\(i\\)-ої комірки дорівнює 3.\n\n\n\n\n\n\nДодатково по \\(\\alpha\\)\n\n\n\nМожна сказати, що чим більш гладкою видається поверхня системи, чим менше елементів задіяно в її розвитку, тим меншим є показник сингулярності. Що більше елементів системи вступають у взаємозв’язок один з одним, що більше процесів протікає під час еволюції системи, то більшим є показник сингулярності\n\n\nВідомо, що для регулярного (однорідного) фрактала всі показники ступеня \\(\\alpha_{i}\\) однакові й рівні фрактальній розмірності \\(D\\):\n\\[\np_{i} = 1 \\big/ N(\\varepsilon) \\approx \\varepsilon^{D}.\n\\]\nУ даному випадку статистична сума (7.1) приймає наступний вигляд:\n\\[\nZ(q,\\varepsilon) = \\sum_{i=1}^{N(\\varepsilon)}p_{i}^{q}(\\varepsilon) = N(\\varepsilon)\\varepsilon^{Dq}=\\varepsilon^{-D}\\varepsilon^{Dq} \\approx \\varepsilon^{D(q-1)}.\n\\]\nТому \\(\\tau(q)=D(q-1)\\) і всі узагальнені фрактальні розмірності \\(D_{q}=D\\) у цьому випадку співпадають та не залежать від \\(q\\).\nОднак, для такого складного об’єкта, як мультифрактал, унаслідок його неоднорідності, ймовірності заповнення клітинок \\(p_{i}\\) у загальному випадку різняться, і показник ступеня \\(\\alpha_{i}\\) для різних клітинок може приймати різні значення. Достатньо типовою є ситуація, коли ці значення неперервно заповнюють деякий закритий інтервал \\(\\left( \\alpha_{min}, \\alpha_{max} \\right)\\), причому \\(p_{min} \\approx \\varepsilon^{\\alpha_{max}}\\), a \\(p_{max} \\approx \\varepsilon^{\\alpha_{min}}\\).\nТепер перейдемо до питання розподілу ймовірностей різних значень \\(\\alpha_{i}\\). Нехай \\(n(\\alpha)d\\alpha\\) є ймовірністю того, що \\(\\alpha_{i}\\) знаходиться в інтервалі від \\(\\alpha\\) до \\(\\alpha+d\\alpha\\). Іншими словами, \\(n(\\alpha)d\\alpha\\) представляє собою відносну кількість клітинок \\(i\\), що характеризуються однією і тією самою мірою \\(p_{i}\\) з \\(\\alpha_{i}\\), що лежать у цьому інтервалі. У випадку монофрактала, для котрого всі \\(\\alpha_{i}\\) однакові (і рівні фрактальній розмірності \\(D\\)), це число, очевидно, пропорційно повній кількості клітинок \\(N(\\varepsilon) \\approx \\varepsilon^{-D}\\), степеневим чином залежних від розміру клітинки \\(\\varepsilon\\). Показник степеня в цьому співвідношені визначається фрактальною розмірністю множини \\(D\\).\nДля мультифрактала, однак, це не так, і різні значення \\(\\alpha_{i}\\) зустрічаються з імовірністю, що характеризується не однією і тією ж величиною \\(D\\), а різними (в залежності від \\(\\alpha\\)) значеннями показниками степеня \\(f(\\alpha)\\),\n\\[\nn(\\alpha) \\approx \\varepsilon^{-f(\\alpha)}.\n\\tag{7.5}\\]\nТаким чином, фізичний сенс функції \\(f(\\alpha)\\) полягає в тому, що вона представляє собою розмірність Хаусдорфа деякої однорідної підмножини \\(\\Omega_{\\alpha}\\) із вихідної множини \\(\\Omega\\), що характеризується однаковими ймовірностями заповнення клітинок \\(p_{i} \\approx \\varepsilon^{\\alpha}\\). Оскільки фрактальна розмірність підмножини очевидно завжди менша або рівна фрактальній розмірності вихідної множини \\(D_{0}\\), має місце важлива нерівність для функції \\(f(\\alpha)\\):\n\\[\nf(\\alpha) \\leq D_{0}.\n\\]\nУ результаті можна зробити висновок, що множина різних значень функції \\(f(\\alpha)\\) (при різних \\(\\alpha\\)) представляє собою спектр фрактальних розмірностей  [1,2] однорідних підмножин \\(\\Omega_{\\alpha}\\), на які можна розбити вихідну множину \\(\\Omega\\), кожна з яких характеризується власним значенням фрактальної розмірності \\(f(\\alpha)\\).\n\n\n7.1.3.2 Перетворення Лежандра\nВстановимо зв’язок функції \\(f(\\alpha)\\) із введенною раніше функцією \\(\\tau(q)\\). Обчислимо для цього статистичну суму \\(Z(q,\\varepsilon)\\). Підставляючи у \\(Z(q,\\varepsilon)\\) ймовірності \\(p_i \\approx \\varepsilon^{\\alpha_i}\\), та переходячи від підсумовування по \\(i\\) до інтегрування по \\(\\alpha\\) з щільністю ймовірностей (7.5), отримаємо\n\\[\nZ(q,\\varepsilon) = \\sum_{i=1}^{N(\\varepsilon)} p_{i}^{q}(\\varepsilon) \\approx \\int d\\alpha n(\\alpha)\\varepsilon^{q\\alpha} \\approx \\int d\\alpha\\varepsilon^{q\\alpha-f(\\alpha)}.\n\\tag{7.6}\\]\nТак як величина \\(\\varepsilon\\) дуже мала, основний внесок у цей інтеграл вноситимуть ті значення \\(\\alpha(q)\\), при яких показник степеня \\(q\\alpha-f(\\alpha)\\) виявляється мінімальним (а підінтегральна функція — максимальною). Цей вклад буде пропорційним значенню підінтегральної функції у точці максимума. Саме ж значення \\(\\alpha(q)\\) визначається при цьому з наступної умови:\n\\[\n\\left. \\frac{d}{d\\alpha}[ q\\alpha-f(\\alpha) ] \\right \\vert_{\\alpha=\\alpha(q)} = 0.\n\\]\nТакож, з умови мінімуму ми маємо\n\\[\n\\left. \\frac{d^{2}}{d\\alpha^{2}}[ q\\alpha-f(\\alpha) ] \\right \\vert_{\\alpha=\\alpha(q)} &gt; 0.\n\\]\nУ результаті отримуємо, що залежність \\(\\alpha(q)\\) неявним чином визначається з \\(q = df(\\alpha) \\big/ d\\alpha\\), і що функція \\(f(\\alpha)\\) усюди є випуклою:\n\\[\nf^{''}(\\alpha)&gt;0.\n\\]\nЦе значить, що величина \\(f(\\alpha(q))\\) дійсно є фрактальною розмірністю підмножини \\(\\Omega_{\\alpha(q)}\\), що має найбільший домінуючий внесок у статистичну суму (7.6) при заданій величині показника ступеня \\(q\\).\nОскільки \\(Z(q,\\varepsilon)=\\tau(q)\\), приходимо до висновку, що\n\\[\n\\tau(q) = q\\alpha(q) - f(\\alpha(q)).\n\\tag{7.7}\\]\nПам’ятаючи, що \\(\\tau(q)=D(q-1)\\), можемо віднайти функцію \\(D_{q}\\):\n\\[\nD_{q} = \\frac{1}{q-1}[ q\\alpha(q)-f(\\alpha(q)) ].\n\\tag{7.8}\\]\nТаким чином, якщо ми знаємо функцію мультифрактального спектра \\(f(\\alpha)\\), то за домомогою співвідношень (7.8) та (7.9) ми можемо знайти функцію \\(D_{q}\\). Навпаки, знаючи \\(D_{q}\\), ми можемо відтворити залежність \\(\\alpha(q)\\) за допомогою рівняння\n\\[\n\\alpha(q) = \\frac{d}{dq}[(q-1)D_{q}]\n\\tag{7.9}\\]\nі після цього знайти із (7.9) \\(f(\\alpha(q))\\). Ці два рівняння і визначають функцію \\(f(\\alpha)\\).\n\\[\n\\frac{d\\tau}{dq}\\frac{dq}{d\\alpha} = q + \\alpha\\frac{dq}{d\\alpha} - \\frac{df}{d\\alpha}.\n\\]\nПриймаючи до уваги, що \\(q=df/d\\alpha\\), і скорочуючи це рівняння на \\(dq/d\\alpha\\), приходимо до співвідношення \\(\\alpha = d\\tau(q)/dq\\), яке еквівалентне (7.9).\nВирази для \\(\\tau(q)\\) та \\(\\alpha(q)\\) задають перетворення Лежандра  [3,4] від змінних \\(\\{ q, \\tau(q) \\}\\) до змінних \\(\\{\\alpha, f(\\alpha)\\}\\): \\(\\alpha = d\\tau \\big/ dq\\) та \\(f(\\alpha) = q\\left( d\\tau \\big/ dq \\right) - \\tau(q)\\). Як відомо, для однорідного фрактала \\(D_{q}=D=\\text{const}\\). Тому \\(\\alpha=d\\tau/dq=D\\) і \\(f(\\alpha)=q\\alpha-\\tau(q)=qD-D(q-1)=D\\). У цьому випадку “графік” функції \\(f(\\alpha)\\) на площині \\(\\left( \\alpha, f(\\alpha) \\right)\\) складається лише з однієї точки \\(\\left( D, D \\right)\\).\n\n\n\n7.1.4 Мультифрактальний аналіз детрендованих флуктуацій\nМонофрактальні та мультифрактальні структури фінансових сигналів є особливим різновидом масштабно-інваріантних структур. Найчастіше монофрактальна структура фінансових часових рядів визначається одним степеневим показником і передбачає, що масштабо-інваріантність не залежить від часу і простору. Однак, часто ми маємо змогу спостерігати просторово-часову варіацію масштабно-інваріантної структури досліджуваної складної системи. Ці просторово-часові варіації вказують на мультифрактальність фінансового сигналу, яка визначається мультифрактальним спектром. Мультифрактальний спектр може допомогти кількісно визначити асиметрію підйомів та спадів на фондовому чи криптовалютному ринках, передбачити фінансову кризу, що поступово наближується, і, таким чином, сприяти успішності подальших торгівельних рішень. Основна мета цього розділу — представити одну з найточніших процедур для визначення множини фрактальних показників — мультифрактальний аналіз детрендованих флуктуацій (multifractal detrended fluctuation analysis, MFDFA)  [5–7], котрий і досі залишається одним із найпотужніших методів для аналізу систем різної природи та складності  [8–21].\nПобудова MFDFA складається з 9 кроків:\n\n“Шум і випадкові блукання у часовому ряді” представляє метод приведення часового ряду до такого, що подібний до випадкового блукання.\n“Обчислення середньоквадратичної варіації часового ряду” представляє середньоквадратичну варіацію, яка є основною процедурою для подальших обчислень в MFDFA і типовим способом обчислення середньої варіації часових рядів різної природи.\n“Локальна середньоквадратична варіація часового ряду” представляє обчислення локальної варіації часового ряду як середньоквадратичного відхилення часового ряду в межах сегментів, що можуть як перекриватися, так і не перекриватися.\n“Локальне детрендування часового ряду” представляє обчислення такого ж локального середньоквадратичного відхилення навколо трендів, які часто зустрічаються у фінансових часових рядах.\n“Монофрактальний аналіз детрендованих флуктуацій”: амплітуди локальних середніх квадратичних відхилень підсумовуються в узагальнене середнє квадратичне відхилення. У сумарному середньоквадратичному відхиленні для сегментів з малими розмірами вибірки переважають швидкі флуктуації часового ряду. На противагу цьому, у сумарному середньоквадратичному відхиленні для сегментів з великими розмірами вибірки переважають повільні коливання. Степенева залежність між загальним середнім квадратичним відхиленням для декількох розмірів вибірки (тобто масштабів) визначається за допомогою монофрактального аналізу дентрендованих флуктуацій (monofractal detrended fluctuation analysis, DFA) і називається показником Херста (Hurst exponent, \\(H\\)).\n“Мультифрактальний аналіз детрендованих флуктуацій”: MFDFA отримують шляхом розширення на \\(q\\)-й порядок узагальненого середньоквадратичного відхилення. Середньоквадратичне відхилення \\(q\\)-го порядку може розрізняти сегменти з малими та великими флуктуаціями. Степенева залежність між середньоквадратичним відхиленням \\(q\\)-го порядку чисельно визначається як узагальнейний показник Херста \\(q\\)-го порядку.\n“Мультифрактальний спектр часових рядів”: на основі показника Херста \\(q\\)-го порядку обчислено декілька мультифрактальних спектрів.\n“Узагальнені фрактальні розмірності” представляє більш детальний опис показників \\(D_{q}\\), що будуть описані в подальшому.\n“Аналогії мультифракталів із термодинамікою” показує, що отримані кількісні мультифрактальні показники мають зв’язок із термодинамічними показниками, що дозволило нам вивести мультифрактальну “теплоємність”.\n\nДля подальшої візуалізації кожного кроку процедури MFDFA імпортуємо наступні модулі:\n\nimport matplotlib.pyplot as plt \nimport matplotlib.gridspec as gridspec\nimport numpy as np\nimport neurokit2 as nk\nimport yfinance as yf\nimport pandas as pd\nimport scienceplots\nfrom scipy.integrate import cumulative_trapezoid\nfrom tqdm import tqdm\n\n%matplotlib inline\n\nІ виконаємо налаштування рисунків для виведення:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nsize = 16\nparams = {\n    'figure.figsize': (8, 6),            # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': size,                   # розмір фонтів рисунку\n    'lines.linewidth': 2,                # товщина ліній\n    'axes.titlesize': 'small',           # розмір титулки над рисунком\n    'axes.labelsize': size,              # розмір підписів по осям\n    'legend.fontsize': size,             # розмір легенди\n    'xtick.labelsize': size,             # розмір розмітки по осі Ох\n    'ytick.labelsize': size,             # розмір розмітки по осі Ох\n    \"font.family\": \"Serif\",              # сімейство стилів підписів \n    \"font.serif\": [\"Times New Roman\"],   # стиль підпису\n    'savefig.dpi': 300,                  # якість збережених зображень\n    'axes.grid': False                   # побудова сітки на самому рисунку\n}\n\nplt.rcParams.update(params)              # оновлення стилю згідно налаштувань\n\nЦього разу розглянему можливість побудови індикаторів або індикаторів-передвісників на прикладі індексу сирої нафти West Texas Intermediate (WTI). При описі процедури MFDFA порівнюватимемо мультифрактальність даного ряду зі штучно згенерованими монофрактальними рядами, складність яких, очевидно, має бути меншою.\nСам сайт Yahoo! Finance містить досить коротку історію щодених цін на нафту даної марки. Цього разу, в якості альтернативного прикладу, будемо послуговуватись альтернативним джерелом фінансових даних — федеральним резервом економічних даних федерального резервного банку Сент-Луїса (Federal Reserve Economic Data of the Federal Reserve Bank of St. Louis, FRED). На Python було створено бібліотеку для вивантаження даних із даного джерела — pandas-datareader. Для її встановлення достатньо прописати наступну команду:\n\n!pip install pandas-datareader\n\nТепер імпортуємо відповідну бібліотеку:\n\nimport pandas_datareader.data as web\n\nта виконаємо зчитування індексу з FRED, використовуючи функціонал даної бібліотеки. Завантажимо дані за весь період, що нам буде доступний:\n\nsymbol = 'DCOILWTICO'    # cимвол індексу, як указано на сайті FRED\nstart = \"1986-01-01\"     # Дата початку зчитування даних\nend = \"2023-01-21\"       # Дата закінчення зчитування даних\n\nwti = web.DataReader(symbol, 'fred', start, end) # зчитуємо значення ряду \ntime_ser = wti[symbol].copy()                    # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'    # підпис по вісі Ох \nylabel = symbol          # підпис по вісі Оу\n\nВиведемо досліджуваний ряд:\n\nfig, ax = plt.subplots(1, 1)               # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\n\n\n\nРис. 7.5: Динаміка щоденних змін індексу сирої нафти WTI\n\n\n\n\n\nРозглянемо опис нашого масиву даних:\n\ntime_ser.describe()\n\ncount    9336.000000\nmean       46.078743\nstd        29.597897\nmin       -36.980000\n25%        19.990000\n50%        35.955000\n75%        67.242500\nmax       145.310000\nName: DCOILWTICO, dtype: float64\n\n\nНа представленому рисунку видно, що в значеннях досліджуваного індексу існують пропуски та негативні значення на ціну нафти. Для того, щоб позбутися від’ємного значення, можна виконати заміну значення на таке, що близьке до нуля. Для видалення значень NaN достатньо скористатися методом dropna() бібліотеки pandas.\n\ntime_ser = time_ser.dropna()    # видаляємо значення NaN\ntime_ser[time_ser.values&lt;0] = 5 # замінюємо від'ємне значення на 5\n\nЗнову візуалізуємо результат:\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}_filtered.jpg')      # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\n\n\n\nРис. 7.6: Динаміка щоденних змін індексу сирої нафти WTI (із видаленими NaN та заміненим від’ємним значенням)\n\n\n\n\n\nОстаннє, що нам залишається, це приведення вихідного ряду до прибутковостей. Для цього визначимо функцію transformation() та знайдемо з її допомогою прибутковості:\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\n\nsignal = time_ser.copy()\nret_type = 4    # вид ряду: 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nwti_ret = transformation(signal, ret_type) # знаходимо прибутковості \nwti_length = len(wti_ret)                  # визначаємо довжину прибутковостей\n\nЯк вже зазначалося, при описі процедури MFDFA, ми будемо послуговуватись для порівняння і монофрактальними сигналами. Для подальших розрахунків згенеруємо сигнал білого та рожевого шумів. У цьому нам може допомогти функція signal_noise() бібліотеки neurokit2. Ця функція генерує чистий гаусовий (1/f)**beta шум. Спектр потужності згенерованого шуму пропорційний S(f) = (1/f)**beta. Було описано наступні категорії шуму:\n\nфіолетовий шум: beta = -2;\nсиній шум: beta = -1;\nбілий шум: beta = 0;\nфлікер/рожевий шум: beta = 1;\nкоричневий шум: beta = 2.\n\nЇї синтаксис:\nsignal_noise(duration=10, sampling_rate=1000, beta=1, random_state=None)\nПараметри:\n\nduration (float) — бажана тривалість (у секундах);\nsampling_rate (int) — бажана частота дискретизації (у Гц, тобто відліків на секунду);\nbeta (float) — експонента шуму;\nrandom_state (None, int, numpy.random.RandomState або numpy.random.Generator) — початкове значення (зерно) для генератора випадкових чисел.\n\nПовертає:\n\nnoise (array) — сигнал чистого шуму.\n\nТепер можемо згенерувати 2 види шумів:\n\nwhite_noise = nk.signal_noise(duration=wti_length, # генеруємо білий шум \n                              sampling_rate=1, \n                              beta=0, \n                              random_state=123)\n\npink_noise = nk.signal_noise(duration=wti_length,  # генеруємо рожевий шум \n                              sampling_rate=1, \n                              beta=1, \n                              random_state=123)\n\n\n7.1.4.1 Шум і випадкові блукання у часовому ряді\nМультифрактальний аналіз детрендованих флуктуацій базується на класичному аналізі детрендованих флуктуацій (DFA). Класичний DFA застосовується до часових рядів зі структурою, подібною до випадкових блукань  [22]. Однак більшість фінансових часових рядів мають коливання, які більш схожі на прирости випадкових блукань. Якщо фінансовий часовий ряд має структуру, подібну до шуму, як у прибутковостей, його слід перетворити на випадково-блукаючий часовий ряд перед застосуванням DFA. Шуми можна перетворити на випадкові блукання шляхом віднімання середнього значення та інтегрування часового ряду (знаходження його кумулятивної суми). Часовий ряд з білим шумом, монофрактал (рожевий шум) і мультифрактал є шумовими часовими рядами і перетворюються на випадкові блукання за допомогою коду, наведеного на 7.7:\n\nRW1 = np.cumsum(white_noise-np.mean(white_noise)) # випадкове блукання білого шуму\nRW2 = np.cumsum(pink_noise-np.mean(pink_noise))   # випадкове блукання монофракталу\nRW3 = np.cumsum(wti_ret-np.mean(wti_ret))         # випадкове блукання для нафти\n\n\nfig, ax = plt.subplots(3, 1, sharex=True)\n\nax[0].plot(time_ser.index[1:], wti_ret)\nax[0].plot(time_ser.index[1:], RW3, 'r')\nax[0].margins(x=0)\nax[0].set_title('Мультифрактальний часовий ряд', fontsize=16)\n\nax[1].plot(time_ser.index[1:], pink_noise, label='Шумоподібний часовий ряд')\nax[1].plot(time_ser.index[1:], RW2, 'r', label='Випадкове блукання')\nax[1].margins(x=0)\nax[1].set_title('Монофрактальний часовий ряд', fontsize=16)\nax[1].legend()\n\nax[2].plot(time_ser.index[1:], white_noise)\nax[2].plot(time_ser.index[1:], RW1, 'r')\nax[2].margins(x=0)\nax[2].set_title('Білий шум', fontsize=16)\n\nplt.show();\n\n\n\n\n\n\n\nРис. 7.7: Мультифрактальний (верхня панель), монофрактальний (середня панель) та подібний до білого шуму (нижня панель) часові ряди\n\n\n\n\n\n\n\n7.1.4.2 Обчислення середньоквадратичної варіації часового ряду\nТрадиційний аналіз варіації часових рядів полягає в обчисленні середнього значення варіації як середньоквадратичного відхилення. Читач може скористатися наведеним нижче кодом для обчислення середньоквадратичного відхилення для часових рядів з білим шумом, монофрактальних і мультифрактальних даних:\n\nRMS_ordinary = np.sqrt(np.mean(white_noise**2))    # середньоквадратична варіація білого шуму\nRMS_monofractal = np.sqrt(np.mean(pink_noise**2))  # середньоквадратична варіація монофрактала\nRMS_multifractal = np.sqrt(np.mean(wti_ret**2))    # середньоквадратична варіація мультифрактала\n\n\nfig, ax = plt.subplots(3, 1, sharex=True)\n\nax[0].plot(time_ser.index[1:], wti_ret, label='Шумоподібний часовий ряд')\nax[0].axhline(y=np.mean(wti_ret), c='r', linestyle='--', label='Середнє')\nax[0].axhline(y=np.mean(wti_ret)+RMS_multifractal, c='r', linestyle='-', label='+/- 1 RMS')\nax[0].axhline(y=np.mean(wti_ret)-RMS_multifractal, c='r', linestyle='-')\nax[0].set_ylim(-20, 20)\nax[0].margins(x=0)\nax[0].set_title('Мультифрактальний часовий ряд', fontsize=16)\n\nax[1].plot(time_ser.index[1:], pink_noise)\nax[1].axhline(y=np.mean(pink_noise), c='r', linestyle='--')\nax[1].axhline(y=np.mean(pink_noise)+RMS_monofractal, c='r', linestyle='-')\nax[1].axhline(y=np.mean(pink_noise)-RMS_monofractal, c='r', linestyle='-')\nax[1].margins(x=0)\nax[1].set_title('Монофрактальний часовий ряд', fontsize=16)\n\nax[2].plot(time_ser.index[1:], white_noise)\nax[2].axhline(y=np.mean(white_noise), c='r', linestyle='--')\nax[2].axhline(y=np.mean(white_noise)+RMS_ordinary, c='r', linestyle='-')\nax[2].axhline(y=np.mean(white_noise)-RMS_ordinary, c='r', linestyle='-')\nax[2].margins(x=0)\nax[2].set_title('Білий шум', fontsize=16)\n\nhandles, labels = ax[0].get_legend_handles_labels()\nfig.legend(handles, labels, loc='lower center')\n\nplt.show();\n\n\n\n\n\n\n\nРис. 7.8: Мультифрактальний (верхня панель), монофрактальний (середня панель) та подібний до білого шуму (нижня панель) часові ряди з нульовим середнім значенням (червона штрихова лінія) і \\(\\pm\\) RMS (червона суцільна лінія)\n\n\n\n\n\nРис. 7.8 ілюструє, що середня амплітуда варіації (тобто середньоквадратичне відхилення) є однаковою для всіх трьох часових рядів, навіть якщо вони мають досить різну структуру. MFDFA може розрізняти ці структури.\n\n\n7.1.4.3 Локальна середньоквадратична варіація часового ряду\nМультифрактальні часові ряди на верхній панелі мають локальні флуктуації різної величини. Середньоквадратичне відхилення (RMS) у попередньому коді можна обчислити для сегментів часового ряду, щоб розрізнити величини локальних флуктуацій. Проста процедура полягає в тому, щоб розділити часовий ряд на однакові за розміром сегменти, що не перекриваються, і обчислити локальне середнє квадратичне відхилення для кожного сегмента. Це можна зробити за допомогою коду наведеного нижче:\n\ndef calc_rms(arr, scale=1335, m=1):\n    \n    # симулюємо випадкове блукання (X)\n    X = np.cumsum(arr - np.mean(arr))\n\n    # транспонуємо значення X \n    X = X.T                           \n\n    # визначаємо довжини сегментів \n    scale = scale\n    \n    # визначаємо порядок полінома\n    m = m\n\n    # визначаємо кількість сегментів\n    segments = np.floor(len(X) / scale).astype(int)\n\n    Index = {}  # словник індексів значень\n    fit = {}    # словник для збереження отриманих поліноміальних кривих\n                # для кожного сегмента\n    RMS = []    # список середньоквадратичних відхилень\n\n    for v in range(segments+1):       # проходимо по кожному сегменту\n        Idx_start = v * scale         # визначаємо початкове значення сегмента                                           \n        Idx_stop = (v + 1) * scale    # визначаємо кінцеве значення\n        \n        # формуємо масив індексів значень досліджуваного сегмента\n        Index[v] = np.arange(Idx_start, min(Idx_stop, len(X)))  \n\n        # вилучаємо значення по індексам\n        X_Idx = X[Index[v]]                       \n\n        # визначаємо поліноміальні коефіцієнти порядку m\n        C = np.polyfit(Index[v], X_Idx, m) \n\n        # будуємо поліноміальну криву по визначеним коефіцієнтам\n        fit[v] = np.polyval(C, Index[v])         \n                 \n        # визначаємо варіацію ряду навколо поліноміального тренда\n        RMS.append(np.sqrt(np.mean((X_Idx - fit[v]) ** 2)))  \n\n    return fit, RMS, Index, X\n\nПерший рядок коду функції calc_rms() перетворює зашумлений часовий ряд, мультифрактал, на часовий ряд випадкового блукання \\(X\\). Третій рядок коду задає масштаб параметра, який визначає розмір вибірки сегментів, що не перетинаються, для яких обчислюється локальне середнє квадратичне відхилення, RMS. П’ятий рядок — це кількість сегментів, на які можна розбити часовий ряд \\(X\\), де len(X) — розмір вибірки часового ряду \\(X\\). Таким чином, segments = 6 при len(X) = 9335 і scale = 1335. З дев’ятого по шістнадцятий рядки — це цикл, що обчислює локальне середньоквадратичне значення навколо тренду fit[v] для кожного сегмента, оновлюючи часовий індекс. У першому циклі v = 0, Index[0] переходить від 0 до 1335 значення сегмента (не включно). У другому циклі \\(v = 1\\), Index[1] переходить від 1335 до 2670 значення другого сегмента. В останньому циклі \\(v = 6\\), Index[6] переходить від 8010-го до 9345-го значення (не включно).\n\n\n7.1.4.4 Локальне детрендування часового ряду\nУ складних системах наявні повільні мінливі тренди, тому для кількісної оцінки масштабо-інваріантності флуктуацій навколо цих трендів необхідно провести детрендування сигналу. У попередньому коді до \\(X\\) на кожному сегменті \\(v\\) підбирається поліноміальний тренд fit[v]. Параметр \\(m\\) визначає порядок полінома. Поліноміальний тренд є лінійним, якщо \\(m = 1\\), квадратичним, якщо \\(m = 2\\), і кубічним, якщо \\(m = 3\\). Рядок C = np.polyfit(Index[v], X[Index[v]], m) визначає коефіцієнти полінома C, які використовуються для створення поліноміальної залежності тренду fit[v] для кожного сегмента. Потім для залишкової варіації, X(Index[v])-fit[v], обчислюється локальне середньоквадратичне відхилення, RMS[v], в межах кожного сегмента \\(v\\). Локальна середньоквадратична варіація, RMS[v], представлена на Рис. 7.9 як відстань між червоними пунктирними трендами і червоними суцільними лініями.\n\nfit_1, RMS_1, Index_1, X = calc_rms(wti_ret, scale=1335, m=1) # оцінка локального відхилення для мультифрактала\nfit_2, RMS_2, Index_2, X = calc_rms(wti_ret, scale=1335, m=2) # оцінка локального відхилення для монофрактала\nfit_3, RMS_3, Index_3, X = calc_rms(wti_ret, scale=1335, m=3) # оцінка локального відхилення для білого шуму\n\n\nfig, ax = plt.subplots(3, 1, sharex=True)\n\n\nax[0].plot(time_ser.index[1:], X)\nfor v in list(fit_1.keys()):\n    ax[0].plot(time_ser.index[Index_1[v]], fit_1[v], 'r--')\n    ax[0].plot(time_ser.index[Index_1[v]], fit_1[v]+RMS_1[v], c='r', linestyle='-')\n    ax[0].plot(time_ser.index[Index_1[v]], fit_1[v]-RMS_1[v], c='r', linestyle='-')\n\nax[0].margins(x=0)\nax[0].set_title('Лінійне детрендування ' + r'$(m=1)$', fontsize=16)\n\n\nax[1].plot(time_ser.index[1:], X, label='Випадкове блукання мультифрактального сигналу')\nfor v in list(fit_2.keys()):\n    if v == 1:\n        ax[1].plot(time_ser.index[Index_2[v]], fit_2[v], 'r--', label='Локальний тренд')\n        ax[1].plot(time_ser.index[Index_2[v]], fit_2[v]+RMS_2[v], c='r', linestyle='-', label='+/- 1 RMS')\n        ax[1].plot(time_ser.index[Index_2[v]], fit_2[v]-RMS_2[v], c='r', linestyle='-')\n    else:\n        ax[1].plot(time_ser.index[Index_2[v]], fit_2[v], 'r--')\n        ax[1].plot(time_ser.index[Index_2[v]], fit_2[v]+RMS_2[v], c='r', linestyle='-')\n        ax[1].plot(time_ser.index[Index_2[v]], fit_2[v]-RMS_2[v], c='r', linestyle='-')\n\nax[1].margins(x=0)\nax[1].set_title('Квадратичне детрендування ' + r'$(m=2)$', fontsize=16)\n\n\nax[2].plot(time_ser.index[1:], X)\nfor v in list(fit_3.keys()):\n    ax[2].plot(time_ser.index[Index_3[v]], fit_3[v], 'r--')\n    ax[2].plot(time_ser.index[Index_3[v]], fit_3[v]+RMS_3[v], c='r', linestyle='-')\n    ax[2].plot(time_ser.index[Index_3[v]], fit_3[v]-RMS_3[v], c='r', linestyle='-')\n\nax[2].margins(x=0)\nax[2].set_title('Кубічне детрендування ' + r'$(m=3)$', fontsize=16)\n\nhandles, labels = ax[1].get_legend_handles_labels()\nfig.legend(handles, labels, loc='lower center')\n\nplt.show();\n\n\n\n\n\n\n\nРис. 7.9: Обчислення локальних флуктуацій RMS навколо лінійного, квадратичного та кубічного трендів за допомогою функції calc_rms() (\\(m = 1\\), \\(m = 2\\) та \\(m = 3\\), відповідно). Червона пунктирна лінія — це підігнаний тренд, fit[v], у семи сегментах вибірки розміром 1335. Відстань між червоним штриховим трендом і суцільними червоними лініями становить \\(\\pm\\) RMS\n\n\n\n\n\n\n\n7.1.4.5 Монофрактальний аналіз детрендованих флуктуацій\nУ DFA варіації локального середньоквадратичного відхилення кількісно оцінюються загальним середньоквадратичним відхиленням (\\(F\\)).\nШвидкі коливання часового ряду \\(X\\) впливатимуть на загальне середньоквадратичне відхилення \\(F\\) у сегментах малої довжини (масштабу), тоді як повільні коливання впливатимуть на \\(F\\) у сегментах великої довжини (масштабу). Таким чином, функція флуктуацій \\(F\\) повинна бути обчислена для декількох масштабів, щоб виокремити вплив як швидкоплинних, так і повільних коливань, які у свою чергу визначають структурні перетворення часового ряду. Функція флуктуацій \\(F(ns)\\) може бути обчислена для декількох масштабів шляхом модифікації попереднього коду:\n\ndef calc_F(arr, scale, m=1):\n    \n    X = np.cumsum(arr - np.mean(arr)) # симулюємо випадкове блукання (X)\n    X = X.T                           # транспонуємо значення X\n\n    scale = scale\n    m = m\n    segments = np.zeros(len(scale), dtype=int)\n    F = np.zeros(len(scale))\n\n    Index = {}  # словник індексів значень\n    fit = {}    # словник для збереження отриманих поліноміальних кривих\n                # для кожного сегмента\n    RMS = {}    # словник середньоквадратичних відхилень\n\n    for ns in range(len(scale)):\n        segments[ns] = np.floor(len(X) / scale[ns]).astype(int)\n        RMS[ns] = np.zeros(segments[ns])\n\n        for v in range(segments[ns]):         # проходимо по кожному сегменту\n            # визначаємо початкове значення сегмента\n            Idx_start = v * scale[ns]  \n                       \n            # визначаємо кінцеве значення\n            Idx_stop = (v + 1) * scale[ns] if v &lt; segments[ns] - 1 else len(X)    \n            \n            # формуємо масив індексів значень досліджуваного сегмента\n            Index[v, ns] = np.arange(Idx_start, Idx_stop)  \n\n            # вилучаємо значення по індексам\n            X_Idx = X[Index[v, ns]]                       \n\n            # визначаємо поліноміальні коефіцієнти порядку m\n            C = np.polyfit(Index[v, ns], X_Idx, m) \n            \n            # будуємо поліноміальну криву по визначеним коефіцієнтам\n            fit[v, ns] = np.polyval(C, Index[v, ns])  \n\n            # оцінюємо середньоквадратичне відхилення для фрагмента v на масштабі ns \n            RMS[ns][v] = np.sqrt(np.mean((X_Idx - fit[v, ns]) ** 2)) \n\n        # оцінюємо загальне середньоквадратичне відхилення в межах масштабу ns\n        F[ns] = np.sqrt(np.mean(RMS[ns] ** 2))\n\n    return F, RMS, Index, X\n\n\nscales = [16, 32, 64, 128, 256, 512, 1024][::-1]\nF, RMS, Index, X = calc_F(wti_ret, scale=scales) # оцінка узагальненої функції флуктуацій по різним масштабам\n\n\nfig, ax = plt.subplots(len(scales), sharex=True)\n\nfor scale, val in enumerate(scales):\n    l = [Index[val] for val in Index.keys() if (val[1] == scale)]\n\n    x = np.array([])\n    for v in l:\n        x = np.concatenate([x, v])\n\n    y = np.array([])\n    for idx, v in enumerate(l): \n        y = np.concatenate([y, RMS[scale][idx]*np.ones(len(v))])\n\n    if scales[scale] == 16:\n        ax[scale].plot(time_ser.index[1:], y, c='b', label=\"Локальні флуктуацій: RMS\")\n        ax[scale].axhline(y=F[scale], c='r', linestyle='-', label=r\"RMS локальних флуктуацій: $F$\")\n        ax[scale].set_title(f\"Масштаб = {scales[scale]}\", fontsize=16)\n        ax[scale].margins(x=0)\n    else: \n        ax[scale].plot(time_ser.index[1:], y, c='b')\n        ax[scale].axhline(y=F[scale], c='r', linestyle='-')\n        ax[scale].set_title(f\"Масштаб = {scales[scale]}\", fontsize=16)\n        ax[scale].margins(x=0)       \n\nhandles, labels = ax[-1].get_legend_handles_labels()\nfig.legend(handles, labels, loc='upper left', fontsize=14)\n\nfig.tight_layout(pad=0.05)\nplt.show();\n\n\n\n\n\n\n\nРис. 7.10: Локальні флуктуації RMS[ns] обчислені для сегментів із різними масштабами. Функція флуктуацій F[ns] є загальним середньоквадратичним відхиленням локальних коливань RMS[ns]. Зверніть увагу, що F[ns] зменшується зі зменшенням масштабу\n\n\n\n\n\nDFA визначає монофрактальну структуру часового ряду відповідно до степеневої залежність між загальним середнім квадратичним відхиленням (тобто \\(F\\)), обчисленим для декількох масштабів. Степенева залежність між загальним середнім квадратичним відхиленням позначається нахилом (\\(H\\)) лінії регресії, розрахованим за допомогою наступного коду:\n\nC = np.polyfit(np.log(scales), np.log(F), 1)\nH = C[0]\nRegLine = np.polyval(C, np.log(scales))\n\nМодифікуємо попередній код, додавши нові фрагменти:\n\ndef calc_H(arr, scale, m=1):\n    \n    X = np.cumsum(arr - np.mean(arr)) # симулюємо випадкове блукання (X)\n    X = X.T                           # транспонуємо значення X\n\n    scale = scale\n    m = m\n    segments = np.zeros(len(scale), dtype=int)\n    F = np.zeros(len(scale))\n\n    Index = {}  # словник індексів значень\n    fit = {}    # словник для збереження отриманих поліноміальних кривих\n                # для кожного сегмента\n    RMS = {}    # словник середньоквадратичних відхилень\n\n    for ns in range(len(scale)):\n        segments[ns] = np.floor(len(X) / scale[ns]).astype(int)\n        RMS[ns] = np.zeros(segments[ns])\n\n        for v in range(segments[ns]):         # проходимо по кожному сегменту\n            # визначаємо початкове значення сегмента\n            Idx_start = v * scale[ns]  \n                       \n            # визначаємо кінцеве значення\n            Idx_stop = (v + 1) * scale[ns] if v &lt; segments[ns] - 1 else len(X)    \n            \n            # формуємо масив індексів значень досліджуваного сегмента\n            Index[v, ns] = np.arange(Idx_start, Idx_stop)  \n\n            # вилучаємо значення по індексам\n            X_Idx = X[Index[v, ns]]                       \n\n            # визначаємо поліноміальні коефіцієнти порядку m\n            C = np.polyfit(Index[v, ns], X_Idx, m) \n            \n            # будуємо поліноміальну криву по визначеним коефіцієнтам\n            fit[v, ns] = np.polyval(C, Index[v, ns])  \n\n            # оцінюємо середньоквадратичне відхилення для фрагмента v на масштабі ns \n            RMS[ns][v] = np.sqrt(np.mean((X_Idx - fit[v, ns]) ** 2)) \n\n        # оцінюємо загальне середньоквадратичне відхилення в межах масштабу ns\n        F[ns] = np.sqrt(np.mean(RMS[ns] ** 2))\n\n    # знаходимо коефіцієнти рівняння прямої \n    C = np.polyfit(np.log(scale), np.log(F), 1) \n    \n    # беремо кут нахилу прямої в якості показника Херста\n    H = C[0]\n\n    # будуємо саме рівняння\n    RegLine = np.polyval(C, np.log(scale))\n\n    return H, RegLine, F\n\nТепер розглянемо залежність загальної функції флуктуацій \\(F\\) від різних довжин (масштабів) локальних сегментів ряду для досліджуваних нами рядів:\n\nscmin = 16\nscmax = 1024\nscres = 19\nexponents = np.linspace(np.log(scmin), np.log(scmax), scres)\n\nscales_exp = np.round(np.exp(1)**exponents).astype(int)\n\nH_multifrac, RegLine_multifrac, F_multifrac = calc_H(wti_ret, scale=scales_exp, m=1)\nH_monofrac, RegLine_monofrac, F_monofrac = calc_H(pink_noise, scale=scales_exp, m=1)\nH_white_noise, RegLine_white_noise, F_white_noise = calc_H(white_noise, scale=scales_exp, m=1)\n\n\nfig, ax = plt.subplots(1, 1)\n\nax.set_xscale('log')\nax.set_yscale('log')\nax.scatter(scales_exp, F_multifrac, \n           label=fr\"Мультифрактальний ряд ($H$={H_multifrac:.2f})\", \n           color='darkblue')\nplt.plot(scales_exp, np.exp(RegLine_multifrac),  color='darkblue')\n\nax.scatter(scales_exp, F_monofrac, \n           label=fr\"Монофрактальний ряд ($H$={H_monofrac:.2f})\", \n           color='magenta')\nplt.plot(scales_exp, np.exp(RegLine_monofrac), color='magenta')\n\n\nax.scatter(scales_exp, F_white_noise, \n           label=fr\"Білий шум ($H$={H_white_noise:.2f})\", \n           color='red')\nplt.plot(scales_exp, np.exp(RegLine_white_noise), color='red')\n\nax.set_xlabel(r'$\\log{ns}$')\nax.set_ylabel(r\"$\\log{F(ns)}$\")\n\nplt.legend(fontsize=16)\n\nfig.tight_layout()\nplt.show();\n\n\n\n\n\n\n\nРис. 7.11: Графік залежності загального середньоквадратичного відхилення (тобто, функції флуктуацій \\(F\\)) від масштабу. Масштабно-інваріантна залежність позначається нахилом \\(H\\) ліній регресії (показником Херста)\n\n\n\n\n\nПоказник Херста визначає монофрактальну структуру часового ряду, вказуючи, наскільки швидко зростає загальне середньоквадратичне відхилення \\(F\\) локальних коливань RMS зі збільшенням розміру локальних сегментів ряду (тобто, масштабу). Рис. 7.11 показує, що загальне середньоквадратичне значення локальних флуктуацій \\(F\\) у порівнянні з індексом нафти та білим шумом зростає швидше зі збільшенням розміру вибірки сегментів для монофрактального рожевого шуму. Рис. 7.12 ілюструє, що показник Херста визначає континуум між часовими рядами, подібними до шуму, і часовими рядами, подібними до випадкового блукання. Показник Херста знаходиться в інтервалі від 0 до 1 для зашумлений часових рядів, тоді як для часових рядів, подібних до випадкових блукань, він перевищує 1. Часовий ряд має довгострокову залежну (тобто корельовану) структуру, коли показник Херста знаходиться в інтервалі 0.5-1, і антикорельовану структуру, коли показник Херста знаходиться в інтервалі 0-0.5. Часовий ряд має незалежну або короткострокову залежну структуру в окремому випадку, коли показник Херста дорівнює 0.5. Згідно з попереднім рисунком, часові ряди білого шуму та нафти представляються непередбачуваними, оскільки показник Херста близький до 0.5, тоді як рожевий шум довгостроково залежну структуру з показником Херста близьким до 1.\n\nbetas = np.linspace(0.0, 2.0, 12)[::-1]\nscmin = 16\nscmax = 1024\nscres = 19\nexponents = np.linspace(np.log(scmin), np.log(scmax), scres)\nscales_exp = np.round(np.exp(1)**exponents).astype(int)\n\n\ncolor = iter(plt.cm.rainbow(np.linspace(0, 1, len(betas))))\n\nfig, ax = plt.subplots(len(betas), 1, sharex=True)\n\nfor idx, beta in enumerate(betas):\n\n    noise = nk.signal_noise(duration=wti_length,  # генеруємо шум із різними значеннями beta \n                              sampling_rate=1, \n                              beta=beta, \n                              random_state=123)   \n\n    H_noise, _, _ = calc_H(arr=noise, scale=scales_exp, m=1)\n\n    c = next(color)\n    ax[idx].plot(np.arange(len(noise)), noise, label=fr\"$H$ = {H_noise:.2f}\", c=c)\n    ax[idx].legend(loc=\"upper right\", fontsize=12)\n    ax[idx].margins(x=0)\n\nfig.subplots_adjust(hspace=0)\n\nplt.show();\n\n\n\n\n\n\n\nРис. 7.12: Діапазон показників Херста визначає континуум фрактальних структур між білим шумом (\\(Н = 0.5\\)) і коричневим шумом (\\(H = 1.5\\)). Рожевий шум \\(H = 1\\) розділяє шуми \\(H &lt; 1\\), які мають більш помітні швидкі флуктуації, і випадкові блукання \\(H &gt; 1\\), які мають більш помітні повільні флуктуації\n\n\n\n\n\n\n\n7.1.4.6 Мультифрактальний аналіз детрендованих флуктуацій\nСтруктури монофрактального та мультифрактального часових рядів відрізняється, хоча вони мають схожі загальні середньоквадратичні значення. Мультифрактальні часові ряди містять локальні флуктуації як з екстремально малими, так і з екстремально великими значеннями, що не характерно для монофрактальних часових рядів. Відсутність флуктуацій з екстремально великими та малими значеннями призводить до нормального розподілу для монофрактального часового ряду, де варіація описується лише статистичним моментом другого порядку (дисперсією). Отже, монофрактальний DFA базується на статистиці другого порядку загального середньоквадратичного відхилення (тобто, \\(F\\,\\)). У мультифрактальному часовому ряді локальні коливання, RMS[ns][v], будуть екстремально великими для сегментів \\(v\\) в межах часових періодів великих коливань і екстремально малими для сегментів \\(v\\) в межах часових періодів малих коливань. Отже, мультифрактальні часові ряди не є нормально розподіленими і слід враховувати всі статистичні моменти \\(q\\)-го порядку. Таким чином, необхідно розширити загальне середньоквадратичне значення монофрактального DFA до середньоквадратичної функції флуктуацій \\(q\\)-го порядку мультифрактального DFA \\((F_{q})\\):\n\ndef calc_Fq(arr, scale, q, m=1):\n    \n    X = np.cumsum(arr - np.mean(arr)) # симулюємо випадкове блукання (X)\n    X = X.T                           # транспонуємо значення X\n\n    scale = scale \n    qs = q\n    m = m\n    segments = np.zeros(len(scale), dtype=int)\n    Fq = np.zeros((len(qs), len(scale)))\n    Index = {}\n    RMS = {}    # словник локальних середньоквадратичних відхилень\n    fit = {}    # словник для збереження отриманих поліноміальних кривих\n                # для кожного сегмента\n    qRMS = {}   # словник локальних відхилень зважених показником q\n\n    for ns in range(len(scale)):\n        segments[ns] = np.floor(len(X) / scale[ns]).astype(int)\n        RMS[ns] = np.zeros(segments[ns])\n\n        # проходимо по кожному сегменту\n        for v in range(segments[ns]): \n\n            # визначаємо початкове значення сегмента\n            Idx_start = v * scale[ns]  \n                       \n            # визначаємо кінцеве значення\n            Idx_stop = (v + 1) * scale[ns] if v &lt; segments[ns] - 1 else len(X)    \n            \n            # формуємо масив індексів значень досліджуваного сегмента\n            Index[v] = np.arange(Idx_start, Idx_stop)  \n\n            # вилучаємо значення по індексам\n            X_Idx = X[Index[v]]                       \n\n            # визначаємо поліноміальні коефіцієнти порядку m\n            C = np.polyfit(Index[v], X_Idx, m) \n            \n            # будуємо поліноміальну криву по визначеним коефіцієнтам\n            fit = np.polyval(C, Index[v])  \n\n            # оцінюємо середньоквадратичне відхилення для фрагмента v на масштабі ns \n            RMS[ns][v] = np.sqrt(np.mean((X_Idx - fit) ** 2)) \n        \n        # приводимо q значення до типу float\n        qs = np.asarray_chkfinite(qs, dtype=float)\n\n        # для мультифрактальності\n        # ----------------------------\n        for nq, qval in enumerate(qs):\n            if (qval != 0.): \n                qRMS[nq, ns] = RMS[ns] ** q[nq]\n                Fq[nq, ns] = np.mean(qRMS[nq, ns]) ** (1 / q[nq])\n            else:\n                Fq[nq, ns] = np.exp(0.5 * np.mean(np.log(RMS[ns] ** 2)))\n        # ----------------------------\n\n    return Fq, qRMS, Index\n\nУ новому блоці коду запускається цикл, який обчислює загальне середньоквадратичне значення \\(q\\)-порядку \\(F_{q}(nq)\\) від від’ємних до додатних \\(q\\). Порядок \\(q\\) зважує вплив сегментів ряду з великими та малими коливаннями, RMS, як показано на наступному рисунку. На величину \\(F_{q}(nq)\\) для від’ємних \\(q\\) впливають сегменти \\(v\\) з малими RMS(v). Навпаки, на \\(F_{q}(nq)\\) для додатних \\(q\\) впливають відрізки \\(v\\) з великими RMS(v). Локальні флуктуації RMS з великими та малими величинами класифікуються за величиною від’ємного або додатного порядку \\(q\\) відповідно. На \\(F_{q}\\) для \\(q = -3\\) і \\(3\\) більше впливають відрізки \\(v\\) з найменшим і найбільшим RMS(v), відповідно, порівняно з \\(F_{q}\\) для \\(q = -1\\) і \\(1\\). Середня точка \\(q = 0\\) є нейтральною щодо впливу відрізків з малим та великим RMS. Зверніть увагу, що в останньому рядку коду нового блоку перевизначено окремий випадок \\(q(nq) = 0\\), оскільки \\(1/0\\) прямує до нескінченності (тобто, \\(1/q(q = 0) = \\infty\\)). Читач також повинен помітити, що \\(F_{q}[q == 2]\\) дорівнює статистиці другого порядку \\(F\\), оскільки \\(\\sqrt{x} = x^{1/2}\\). Монофрактальний DFA тепер розширюється до MFDFA.\n\nscales = np.array([32])\nnq = np.array([-3, -1, 1, 3])\n\nFq, qRMS, Index = calc_Fq(wti_ret, scale=scales, q=nq, m=1)\nFq_pink, qRMS_pink, Index = calc_Fq(pink_noise, scale=scales, q=nq, m=1)\n\n\nfig, ax = plt.subplots((len(nq)+1), 1, sharex=True)\n\nax[0].plot(time_ser.index[1:], wti_ret, label=\"Мультифрактал\")\nax[0].plot(time_ser.index[1:], pink_noise, label=\"Монофрактал\")\nax[0].grid(False)\nax[0].margins(x=0)\nax[0].legend(loc='upper left', fontsize=12)\nax[0].get_xaxis().set_visible(False)\n\n\nfor idx in range(1, len(nq)+1):\n    l = [Index[val] for val in Index.keys()]\n\n    x = np.array([])\n    for v in l:\n        x = np.concatenate([x, v])\n\n    y = np.array([])\n    y_pink = np.array([])\n    for i, v in enumerate(l): \n        y = np.concatenate([y, qRMS[(idx-1, 0)][i]*np.ones(len(v))])\n        y_pink = np.concatenate([y_pink, qRMS_pink[(idx-1, 0)][i]*np.ones(len(v))])\n    \n    ax[idx].set_title(fr\"Локальні варіації для {scales[0]}-го масштабу при $q=${nq[idx-1]}\", fontsize=14)\n    ax[idx].plot(time_ser.index[1:], y)\n    ax[idx].plot(time_ser.index[1:], y_pink)\n    ax[idx].margins(x=0)       \n\nhandles, labels = ax[0].get_legend_handles_labels()\n\nfig.tight_layout(pad=0.01)\nplt.show();\n\n\n\n\n\n\n\nРис. 7.13: Ілюстрація залежності локальних флуктуацій qRMS від \\(q\\) при масштабі 32\n\n\n\n\n\nqRMS на Рис. 7.13 — це \\(q\\)-порядок локальних флуктуацій (тобто, RMS) і є складовою частиною загального\\(q\\)-порядку RMS (тобто, \\(F_{q}\\)). qRMS представлено для монофрактального (зелена смуга) та мультифрактальних (синя смуга) часових рядів. Від’ємний порядок \\(q\\) (\\(q = -3\\) і \\(-1\\)) підсилює сегменти в мультифрактальному часовому ряді з екстремально малими RMS, тоді як додатний порядок \\(q\\) (\\(q = 3\\) і \\(1\\)) підсилює відрізки з екстремально великими RMS. Зверніть увагу, що \\(q = -3\\) і \\(q = 3\\) підсилюють малу і велику варіацію відповідно більше, ніж \\(q = -1\\) і \\(q = 1\\). Зауважте також, що монофрактальний часовий ряд не має відрізків з екстремально великими або малими коливаннями і, таким чином, не має піків у qRMS. Загальне середньоквадратичне відхилення \\(q\\)-го порядку здатне розрізняти структуру малих і великих флуктуацій і, відповідно, монофрактальних і мультифрактальних часових рядів.\nТепер можна визначити показники Херста \\(q\\)-го порядку як нахили \\(h(q)\\) ліній регресії для кожного середньоквадратичного значення \\(F_{q}\\) \\(q\\)-го порядку. І \\(h(q)\\), і лінія регресії визначаються в циклі для кожного \\(q\\)-го порядку:\n\ndef calc_Hq(arr, scale, q, m=1):\n    \n    X = np.cumsum(arr - np.mean(arr)) # симулюємо випадкове блукання (X)\n    X = X.T                           # транспонуємо значення X\n\n    scale = scale \n    qs = q\n    m = m\n    segments = np.zeros(len(scale), dtype=int) \n    Fq = np.zeros((len(qs), len(scale)))       # масив для збереження загальної функції флуктуацій \n    hq = np.zeros(len(qs), dtype=float)        # масив для збереження Херста q-го порядку\n    qRegLine = {} # словник для збереження ліній регресій\n    Index = {}    # словник для збереження індексів сегментів ряду\n    RMS = {}      # словник локальних середньоквадратичних відхилень\n    fit = {}      # словник для збереження отриманих поліноміальних кривих\n                  # для кожного сегмента\n    qRMS = {}     # словник локальних відхилень зважених показником q\n\n    for ns in range(len(scale)):\n        segments[ns] = np.floor(len(X) / scale[ns]).astype(int)\n        RMS[ns] = np.zeros(segments[ns])\n\n        # проходимо по кожному сегменту\n        for v in range(segments[ns]): \n\n            # визначаємо початкове значення сегмента\n            Idx_start = v * scale[ns]  \n                       \n            # визначаємо кінцеве значення\n            Idx_stop = (v + 1) * scale[ns] if v &lt; segments[ns] - 1 else len(X)    \n            \n            # формуємо масив індексів значень досліджуваного сегмента\n            Index[v] = np.arange(Idx_start, Idx_stop)  \n\n            # вилучаємо значення по індексам\n            X_Idx = X[Index[v]]                       \n\n            # визначаємо поліноміальні коефіцієнти порядку m\n            C = np.polyfit(Index[v], X_Idx, m) \n            \n            # будуємо поліноміальну криву по визначеним коефіцієнтам\n            fit = np.polyval(C, Index[v])  \n\n            # оцінюємо середньоквадратичне відхилення для фрагмента v на масштабі ns \n            RMS[ns][v] = np.sqrt(np.mean((X_Idx - fit) ** 2)) \n        \n        # приводимо q значення до типу float\n        qs = np.asarray_chkfinite(qs, dtype=float)\n\n        # для мультифрактальності\n        # ----------------------------\n        for nq, qval in enumerate(qs):\n            if (qval != 0.): \n                qRMS[nq, ns] = RMS[ns] ** q[nq]\n                Fq[nq, ns] = np.mean(qRMS[nq, ns]) ** (1 / q[nq])\n            else:\n                Fq[nq, ns] = np.exp(0.5 * np.mean(np.log(RMS[ns] ** 2)))\n\n        for nq, _ in enumerate(qs): \n            # якщо флуктуації дорів. 0, log2 стикнеться з діленням на 0 \n            old_setting = np.seterr(divide=\"ignore\", invalid=\"ignore\")\n            C = np.polyfit(np.log(scale), np.log(Fq[nq, :]), m)\n            np.seterr(**old_setting)\n            hq[nq] = C[0]\n            qRegLine[nq] = np.polyval(C, np.log(scale))\n        # ----------------------------\n\n    return hq, qRegLine, Fq \n\n\nscmin = 16\nscmax = 1024\nscres = 19\n\nq_min = -5.0\nq_max = 5.0\nq_step = 0.1\n\nnq = np.arange(q_min, q_max+q_step, q_step)\n\nexponents = np.linspace(np.log(scmin), np.log(scmax), scres)\nscales_exp = np.round(np.exp(1)**exponents).astype(int)\n\nHq_multifrac, qRegLine_multifrac, Fq_multifrac = calc_Hq(wti_ret, scale=scales_exp, q=nq, m=1)\nHq_monofrac, qRegLine_monofrac, Fq_monofrac = calc_Hq(pink_noise, scale=scales_exp, q=nq, m=1)\nHq_white_noise, qRegLine_white_noise, Fq_white_noise = calc_Hq(white_noise, scale=scales_exp, q=nq, m=1)\n\n\nfig, ax = plt.subplots(2, 2)\n\nax[0][0].set_title(\"Мультифрактал\")\nax[0][0].set_xlabel(r\"$ns$\")\nax[0][0].set_ylabel(r\"$F_{q}(ns)$\")\nax[0][0].set_xscale('log')\nax[0][0].set_yscale('log')\nfor i in range(len(nq)):\n    ax[0][0].scatter(scales_exp, Fq_multifrac[i, :], color='darkblue')\n    ax[0][0].plot(scales_exp, np.exp(qRegLine_multifrac[i]),  color='darkblue')\n\nax[0][1].set_title(\"Монофрактал\")\nax[0][1].set_xlabel(r\"$ns$\")\nax[0][1].set_xscale('log')\nax[0][1].set_yscale('log')\nfor i in range(len(nq)):\n    ax[0][1].scatter(scales_exp, Fq_monofrac[i, :], color='magenta')\n    ax[0][1].plot(scales_exp, np.exp(qRegLine_monofrac[i]),  color='magenta')\n\nax[1][0].set_title(\"Білий шум\")\nax[1][0].set_xlabel(r\"$ns$\")\nax[1][0].set_ylabel(r\"$F_{q}(ns)$\")\nax[1][0].set_xscale('log')\nax[1][0].set_yscale('log')\nfor i in range(len(nq)):\n    ax[1][0].scatter(scales_exp, Fq_white_noise[i, :], color='red')\n    ax[1][0].plot(scales_exp, np.exp(qRegLine_white_noise[i]),  color='red')\n\nax[1][1].set_title(r\"Показники Херста $q$-го порядку\")\nax[1][1].set_xlabel(r\"$q$\")\nax[1][1].set_ylabel(r\"$h(q)$\")\nax[1][1].plot(nq, Hq_multifrac, linestyle='-', marker='o', label=\"Мультифрактал\", color='darkblue')\nax[1][1].plot(nq, Hq_monofrac,linestyle='-', marker='o', label=\"Монофрактал\", color='magenta')\nax[1][1].plot(nq, Hq_white_noise, linestyle='-', marker='o', label=\"Білий шум\", color='red')\nax[1][1].legend(loc='center right', fontsize=12)\n\nfig.tight_layout(pad=0.1)\n\nplt.show();\n\n\n\n\n\n\n\nРис. 7.14: Середньоквадратичні значення \\(F_{q}\\) для різних \\(q\\)-их порядків та відповідні лінії регресії, обчислені за допомогою MFDFA для мультифракталу, монофракталу та білого шуму\n\n\n\n\n\nМожемо бачити, що узагальнена функція флуктуацій для мультифракталу залежить не лише від масштабу, але й від \\(q\\), що демонструють різні нахили ліній регресії \\(h(q)\\). Масштабуючі узагальнені функції флуктуацій \\(F_{q}\\) для монофракталу та білого шуму є \\(q\\)-незалежними, оскільки їх лінії регресії для різних масштабів мають один і той самий кут нахилу. Показник Херста \\(q\\)-го порядку \\(h(q)\\) для мультифрактального ряду (синя лінія) представляється незалежним для \\(q&lt;0\\) і змінним для \\(q&gt;0\\). Це вказує на те, що джерелом мультифрактальності нафти є аномально великі флуктуацій як, наприклад, криза коронавірусної пандемії. Для монофракталу (рожева лінія) та білого шуму (червона лінія) \\(h(q)\\) залишаються сталими.\n\n\n7.1.4.7 Мультифрактальний спектр часових рядів\nПоказник Херста \\(q\\)-го порядку \\(h(q)\\) є лише одним з декількох типів масштабних показників, що використовуються для параметризації мультифрактальної структури часових рядів. Як уже було представлено попередньо, ми можемо вивести показник маси \\(q\\)-го порядку (\\(\\tau(q)\\)), а потім через \\(\\tau(q)\\) отримати показник сингулярності \\(q\\)-го порядку (\\(\\alpha(q)\\)) і фрактальну розмірність (\\(f(\\alpha)\\)) флуктуацій (областей) із ступенем сингулярності \\(\\alpha(q)\\). Графік залежності \\(\\alpha(q)\\) від \\(f(\\alpha)\\) представляє мультифрактальний спектр. Показники маси, сингулярності та фрактальності можна обчислити згідно коду, що наведений нижче:\n\ntau_multifrac = nq * Hq_multifrac - 1 \ntau_monofrac = nq * Hq_monofrac - 1 \ntau_white_noise = nq * Hq_white_noise - 1 \n\nalpha_multifrac = np.gradient(tau_multifrac, nq)\nalpha_monofrac = np.gradient(tau_monofrac, nq)\nalpha_white_noise = np.gradient(tau_white_noise, nq)\n\nf_multifrac = nq * alpha_multifrac - tau_multifrac\nf_monofrac = nq * alpha_monofrac - tau_monofrac\nf_white_noise = nq * alpha_white_noise - tau_white_noise\n\n\nfig, ax = plt.subplots(1, 3)\n\nax[0].set_xlabel(r\"$q$\")\nax[0].set_ylabel(r\"$\\tau(q)$\")\nax[0].plot(nq, tau_multifrac, linestyle='-', marker='o', label=\"Мультифрактал\", color='darkblue')\nax[0].plot(nq, tau_monofrac, linestyle='-', marker='o', label=\"Монофрактал\", color='magenta')\nax[0].plot(nq, tau_white_noise, linestyle='-', marker='o', label=\"Білий шум\", color='red')\nax[0].legend()\n\nax[1].set_xlabel(r\"$\\alpha$\")\nax[1].set_ylabel(r\"$f(\\alpha)$\")\nax[1].plot(alpha_multifrac, f_multifrac, linestyle='-', marker='o', label=\"Мультифрактал\", color='darkblue')\nax[1].plot(alpha_monofrac, f_monofrac, linestyle='-', marker='o', label=\"Монофрактал\", color='magenta')\nax[1].plot(alpha_white_noise, f_white_noise, linestyle='-', marker='o', label=\"Білий шум\", color='red')\n\nax[2].set_xlabel(r\"$q$\")\nax[2].set_ylabel(r\"$f(\\alpha)$\")\nax[2].plot(nq, f_multifrac, linestyle='-', marker='o', label=\"Мультифрактал\", color='darkblue')\nax[2].plot(nq, f_monofrac, linestyle='-', marker='o', label=\"Монофрактал\", color='magenta')\nax[2].plot(nq, f_white_noise, linestyle='-', marker='o', label=\"Білий шум\", color='red')\n\nfig.tight_layout(pad=0.01) \n\nplt.show();\n\n\n\n\n\n\n\nРис. 7.15: Множинне представляння мультифрактального спектра для мультифрактала, монофрактала та білого шуму\n\n\n\n\n\nПоказники сингулярності \\(\\alpha\\) для великих висококонцентрованих флуктуацій малі й розташовані в лівому хвості спектра, тоді як \\(\\alpha\\) для малих флуктуацій великі й розташовані в правому хвості спектра.\nТаким чином, сила мультифрактальності описується великим відхиленням експоненти локальної сингулярності \\(\\alpha\\) від центральної тенденції \\(\\alpha(0)\\). Монофрактальний сигнал — це випадок, коли \\(\\alpha\\) залишається майже константною, і в деяких випадках мультифрактальний спектр зводиться до однієї точки за даною \\(\\alpha\\).\nДіапазон \\(\\alpha\\) вказує на різноманітність експонент сингулярності, що описують динаміку системи, а величина \\(f(\\alpha)\\) вказує на величину внеску елементів із відповідним показником \\(\\alpha\\).\nМультифрактальний спектр може характеризуватися різною шириною, що вказує на варіативність процесів, що відбуваються всередині системи. Так само він може бути як симетричним, так і асиметричним. Асиметрія буває як правосторонньою, так і лівосторонньою, що вказує на різний ступінь впливу висококонцентрованих і низькоконцентрованих елементів (флуктуацій). Мультифрактальний спектр матиме довгий лівий хвіст, коли часовий ряд має мультифрактальну структуру, чутливу до локальних флуктуацій з великими амплітудами.\nНавпаки, мультифрактальний спектр матиме довгий правий хвіст, коли він чутливий до локальних флуктуацій з малими амплітудами.\nПроілюструємо залежність ширини спектра мультифрактальності від рівня флуктуацій у ряді. Дану залежність будемо демонструвати на прикладі рядів, що розподілятимуться згідно альфа-стабільному розподілу Леві, який ми ще розглядатимемо в наступних роботах. Для генерації випадкових величин із даного розподілу, використовуватимемо модуль scipy.stats. З нього імпортуємо клас levy_stable для використання методу rvs(). Метод приймає показник \\(\\alpha\\), що відповідає за частоту виникнення подій, що виходять за межі нормального розподілу. Розглянемо діапазон таких значень \\(\\alpha\\) та спектри згенерованих рядів.\n\nfrom scipy.stats import levy_stable\n\nalphas = np.linspace(1.5, 2.0, 7)\nscmin = 16\nscmax = 1024\nscres = 19\n\nq_min = -5.0\nq_max = 5.0\nq_step = 0.1\nnq_levy = np.arange(q_min, q_max+q_step, q_step)\n\nexponents = np.linspace(np.log(scmin), np.log(scmax), scres)\nscales_exp = np.round(np.exp(1)**exponents).astype(int)\n\n\ncolor = iter(plt.cm.plasma(np.linspace(0, 0.8, len(alphas))))\n\nfig = plt.figure()\nsubfigs = fig.subfigures(1, 2)\nax1 = subfigs[0].subplots(len(alphas), 1, sharex=True)\nax2 = subfigs[1].subplots(1, 1)\n\nfor i in range(len(alphas)):\n    \n    # генеруємо альфа-стабільний процес\n    r = levy_stable.rvs(alpha=alphas[i], beta=0, loc=0, \n                        scale=1, size=len(wti_ret), random_state=123)\n\n    Hq_levy, qRegLine_levy, Fq_levy = calc_Hq(r, scale=scales_exp, q=nq_levy, m=1)\n    tau_levy = nq_levy * Hq_levy - 1\n    alpha_levy = np.gradient(tau_levy, nq_levy)\n    f_levy = nq_levy * alpha_levy - tau_levy\n    \n    c = next(color)\n    ax1[i].plot(np.arange(len(r)), r, label=fr'$\\alpha$={alphas[i]:.2f}', c=c)\n    ax1[i].margins(x=0)\n    ax1[i].legend(loc=\"upper left\", fontsize=12)\n    ax2.plot(alpha_levy, f_levy, marker='o', c=c)\n\nax1[0].set_title(\"Мультифрактальні часові ряди\", fontsize=16)\nax1[-1].set_xlabel(\"Час (порядковий номер)\")\nax1[len(alphas) // 2].set_ylabel('Амплітуда коливань')\n\nax2.set_title(\"Мультифрактальні спектри\", fontsize=16)\nax2.set_xlabel(r\"$\\alpha$\")\nax2.set_ylabel(r\"$f(\\alpha)$\")\n\nfig.subplots_adjust(hspace=0.1)\n\nplt.show();\n\n\n\n\n\n\n\nРис. 7.16: Ілюстрація множини мультифрактальних часових рядів (Леві альфа-стабільних процесів) та їх мультифрактальних спектрів, що були згенеровані з різними значеннями \\(\\alpha\\). Зверніть увагу на зростання структурних відмінностей між періодами з малими і великими флуктуаціями зі збільшенням ширини мультифрактального спектра\n\n\n\n\n\nСистема, складність якої зумовлена висококонцентрованими елементами, матиме чітко виражений лівосторонній спектр. Складність системи, що зумовлена слабко концентрованими елементами, характеризує правий хвіст мультифрактального спектра. Якщо складність системи розвивається за рахунок елементів двох типів, тоді спектр представлятиметься симетричним, де елементи двох типів будуть рівноймовірними. Для згенерованих вище Леві альфа-стабільних процесів видно, що чим нижче значення \\(\\alpha\\), тим сильніша домінація висококонцентрованих (великих) флуктуацій. При \\(\\alpha=2.0\\) спектр усе сильніше звужується до сингулярної точки.\nДалі буде показано, що для отриманої мультифрактальної параболи мультифрактального спектра можуть бути розраховані показники як всієї ширини спектра \\((\\Delta\\alpha)\\), так і окремо його правого та лівого хвостів (\\(R\\) та \\(L\\)). Також можна розрахувати значення сингулярності, де \\(f(\\alpha)\\) приймає максимальне значення \\(\\alpha_0\\), і навіть так звану “асиметрію” цього спектра \\((\\Delta f)\\). На Рис. 7.17 схематично представлено положення ключових індикаторів мультифрактальності спектра.\n\n\n\n\n\n\nРис. 7.17: Графік мультифрактального спектра із відміченними на ньому показниками ширини спектра мультифрактальності (\\(\\Delta\\alpha\\)), значень мінімальної, центральної та максимальної сингулярності (\\(\\alpha_{min}, \\alpha_0, \\alpha_{max}\\)), ширини лівого та правого хвостів спектра (\\(L, R\\)) та різниці між фрактальними розмірностями в кінцях параболи (\\(\\Delta f\\))\n\n\n\nТакож варто зазначити, що дана схема не представляє вичерпний список індикаторів мультифрактальності системи, що ми використовуватимемо в подальшому, але має надавати інтуїтивне розуміння того, як виводиться більшість мультифрактальних показників.\n\n\n7.1.4.8 Узагальнені фрактальні розмірності\nНаряду з мультифрактальним спектром корисно буде розглянути спектр узагальнених фрактальних розмірностей або по іншому, розмірностей Реньї, оскільки вони також мають інформаційно-теоретичне значення. З’ясуємо фізичний сенс узагальнених фрактальних розмірностей для деяких значень \\(q\\). При \\(q=0\\), \\(Z(0, \\varepsilon) = N(\\varepsilon)\\). З іншого боку, можна визначити, що \\(Z(0, \\varepsilon) \\approx \\varepsilon^{\\tau(0)} = \\varepsilon^{-D_{0}}\\). Співставляючи зазначені рівності, можемо прийти до співвідношення \\(N(\\varepsilon) \\approx \\varepsilon^{-D_{0}}\\). Отже, величина \\(D_{0}\\) представляє собою звичайну хаусдорфову розмірність множини \\(\\Omega\\). Також вона відповідає максимуму мультифрактального спектра, \\(f(\\alpha)\\), що завжди дорівнює одиниці для одновимірного сигналу. Для задач розпізнавання кризових явищ ця характеристика є найгрубішою і не несе інформації про статистичні властивості системи.\nТепер з’ясуємо сенс величини \\(D_{1}\\). Оскільки при \\(q=1\\) статистична сума \\(Z(1, \\varepsilon) = 1\\), то \\(\\tau(1)=0\\). Таким чином, ми маємо невизначеність, коли \\(D_{1}=\\tau(1)/(1-1)\\). Розкриємо цю невизначеність за допомогою наступної рівності:\n\\[\nZ(q, \\varepsilon) = \\sum_{i=1}^{N(\\varepsilon)} p_{i}^{q} = \\sum_{i=1}^{N(\\varepsilon)} p_{i}\\exp{[(q-1)\\ln{p_{i}}]}.\n\\]\nТепер, спрямовуючи \\(q \\to 1\\), розкладаючи експоненту і враховуючи умову нормування ймовірностей \\(p_{i}\\), отримуємо\n\\[\nZ(q \\to 1, \\varepsilon) \\approx \\sum_{i=1}^{N(\\varepsilon)} [p_{i} + (q-1)p_{i}\\ln{p_{i}}] = 1 + (q-1)\\sum_{i=1}^{N(\\varepsilon)} p_{i}\\ln{p_{i}}.\n\\]\nУ результаті ми приходимо до наступного виразу:\n\\[\nD_{1} = \\lim_{\\varepsilon \\to 0} \\sum_{i=1}^{N(\\varepsilon)}p_{i}\\ln{p_{i}} \\bigg/ \\ln{\\varepsilon}.\n\\]\nЗ точністю до знака, чисельник у цій формулі представляє собою інформаційну ентропію фрактальної множини \\(S(\\varepsilon)\\):\n\\[\nS(\\varepsilon) = -\\sum_{i=1}^{N(\\varepsilon)}p_i\\ln{p_i}.\n\\]\nТаким чином, результуюча величина узагальненої фрактальної розмірності \\(D_{1}\\) пов’язана з ентропією \\(S(\\varepsilon)\\) наступним співвідношенням:\n\\[\nD_{1} = - \\lim_{\\varepsilon \\to 0} S(\\varepsilon) \\big/ \\ln{\\varepsilon}.\n\\]\nПовертаючись до задачі розподілу точок на фрактальній множині \\(\\Omega\\), можно сказати, що оскільки \\(S(\\varepsilon) \\approx \\varepsilon^{-D_1}\\), величина \\(D_{1}\\) характеризує інформацію, необхідну для опису положення точки в деякій комірці.\n\n\n\n\n\n\nДодаткова інформація по інформаційній розмірності\n\n\n\nІнформаційна розмірність може бути використана для опису просторової неоднорідності системи. Що однорідніший атрактор, то вищим має бути цей показник. Тобто, чим більшу кількість конфігурацій здатні займати елементи даної системи, тим більша кількість інформації нам потрібна для обліку кожного елемента. За просторової однорідності інформаційна ентропія так само зростає, що пов’язує інформаційну розмірність із поняттям ентропії. Оскільки \\(D_{1}\\) це тангенс кута нахилу лінії регресії, що будується для залежності між ентропією та радіусом кіл, у яких вимірюється частота влучання окремих елементів атрактора, можна сказати, що інформаційна розмірність відображає швидкість зміни інформаційної ентропії. Чим вища \\(D_{1}\\), тим стрімкіше зростає ентропія — міра нашого нинішнього незнання про систему. Чим нижча \\(D_{1}\\), тим менша сама ентропія. Інакше кажучи, тим більша просторова асиметрія, упорядкованіша складність, вищі наші знання про поточний стан системи, і тим менше інформації нам потрібно для опису тих конфігурацій, які система може займати\n\n\nДля узагальненої фрактальної розмірності при \\(q=2\\) справедливий наступний вираз:\n\\[\nD_{2} = \\lim_{\\varepsilon \\to 0} \\ln{\\sum_{i=1}^{N(\\varepsilon)}p_{i}^{2}} \\bigg/ \\ln{\\varepsilon}.\n\\]\nВеличина \\(p_{i}\\) представляє собою ймовірність попадання точки в комірку розміром \\(\\varepsilon\\). Тоді величина \\(p_{i}^{2}\\) є ймовірністю попадання в цю комірку двох точок. Знаходячи суму \\(p_{i}^{2}\\) по всім зайнятим коміркам, ми отримуємо ймовірність того, що дві навмання обрані точки з множини \\(\\Omega\\) знаходяться всередині однієї комірки з розміром \\(\\varepsilon\\). Отже, відстань між цима двома точками буде менше або порядку \\(\\varepsilon\\). Ймовірність знаходження двох траєкторій у межах околиці з радіусом \\(\\varepsilon\\) можна знайти за допомогою кореляційного інтеграла.\nУ такому разі ми приходимо до висновку, що узагальнена розмірність визначає залежність кореляційного інтеграла \\(C(\\varepsilon)\\) від \\(\\varepsilon\\). З цієї причини величину \\(D_{2}\\) в літературі іменують кореляційною розмірністю.\nТепер проаналізуємо поведінку \\(f(\\alpha)\\). Значення функції в максимумі легко визначити, якщо скористатися виразом (7.7), де \\(\\tau(q) = q\\alpha(q) - f(\\alpha(q))\\) або \\((q-1)D_{q} = q\\alpha(q) - f(\\alpha(q))\\). При \\(q=0\\) ми отримаємо, що \\(f(\\alpha_0)=D_{0}\\), тобто максимальне значення спектра дорівнює хаусдорфовій розмірності.\n\n\n\n\n\n\nРис. 7.18: Максимум функції \\(f(\\alpha)\\) дорівнює фрактальній розмірності \\(D_{0}\\)\n\n\n\nРозглянемо випадок, коли \\(q=1\\). Оскільки \\(\\tau(1) = 0\\), тоді з рівняння вище слідує, що \\(\\alpha(1) = f(\\alpha(1))\\). З іншого боку ми знаємо, що оскільки \\(q = df(\\alpha) \\big/ d\\alpha\\), похідна \\(f(\\alpha)\\) в цій точці дорівнює 1. Диференцюючи співвідношення \\(\\tau(q) = (q-1)D_{q}\\) по \\(q\\),\n\\[\n\\frac{d\\tau}{dq} = D_{q} + (q-1)D_{q}^{'} = \\alpha(q),\n\\]\nі припускаючи, що \\(q=1\\), ми отримуємо, що \\(\\alpha(1)=D_{1}\\). Таким чином, ми маємо \\(D_{1} = \\alpha(1) = f(\\alpha(1))\\). Отже, інформаційна розмірність \\(D_{1}\\) лежить на кривій \\(f(\\alpha)\\) в точці, де \\(\\alpha=f(\\alpha)\\) і \\(f^{'}(\\alpha)=1\\).\n\n\n\n\n\n\nРис. 7.19: Положення інформаційної розмірності \\(D_{1}: D_{1}=\\alpha=f(\\alpha)\\)\n\n\n\nТепер розглянемо випадок, коли \\(q=2\\). Користуючись попередньою формулою, отримаємо, що \\(D_{2} = 2\\alpha(2) - f(\\alpha(2))\\) або \\(f(\\alpha(2)) = 2\\alpha(2)-D_{2}\\).\n\n\n\n\n\n\nРис. 7.20: Геометричне визначення кореляційної розмірності \\(D_{2}\\)\n\n\n\nДалі розглянемо залежність узагальненої фрактальної розмірності \\(D_{q}\\) від різних значень \\(q\\) для мультифрактального ряду, монофрактального та білого шуму.\n\ndifference_zero = np.absolute(nq-0)\nidx_zero = difference_zero.argmin()\n\ndifference_one = np.absolute(nq-1)\nidx_one = difference_one.argmin()\n\ndifference_two = np.absolute(nq-2)\nidx_two = difference_two.argmin()\n\n# ініціалізуємо масиви під розмірності\nDq_multifrac = np.zeros(len(nq))\nDq_monofrac = np.zeros(len(nq))\nDq_white_noise = np.zeros(len(nq))\n\n# Визначаємо узагальнені фрактальні розмірності там де q!=1 \nDq_multifrac[nq!=nq[idx_one]] = tau_multifrac[nq!=nq[idx_one]] / (nq[nq!=nq[idx_one]] - 1)\nDq_monofrac[nq!=nq[idx_one]] = tau_monofrac[nq!=nq[idx_one]] / (nq[nq!=nq[idx_one]] - 1)\nDq_white_noise[nq!=nq[idx_one]] = tau_white_noise[nq!=nq[idx_one]] / (nq[nq!=nq[idx_one]] - 1)\n\n# Визначаємо окремо узагальнені фрактальні розмірності при q=1\nDq_multifrac[nq==nq[idx_one]] = -tau_multifrac[nq==nq[idx_one]] \nDq_monofrac[nq==nq[idx_one]] = -tau_monofrac[nq==nq[idx_one]]\nDq_white_noise[nq==nq[idx_one]] = -tau_white_noise[nq==nq[idx_one]]\n\n\nfig, ax = plt.subplots(1, 1)\n\nax.plot(nq, Dq_multifrac, linestyle='-', marker='o', label=\"Мультифрактал\", color='darkblue')\nax.plot(nq, Dq_monofrac, linestyle='-', marker='o', label=\"Монофрактал\", color='magenta')\nax.plot(nq, Dq_white_noise, linestyle='-', marker='o', label=\"Білий шум\", color='red')\nax.set_xlabel(r\"$q$\")\nax.set_ylabel(r\"$D_{q}$\")\nax.legend(loc=\"upper right\")\n\nax.annotate(fr'$D_{0}$={Dq_multifrac[nq==nq[idx_zero]][0]:.2f}', \n            xy=(nq[idx_zero], Dq_multifrac[nq==nq[idx_zero]]), \n            xytext=(nq[idx_zero]-2, Dq_multifrac[nq==nq[idx_zero]]+2),\n            arrowprops=dict(facecolor='black', shrink=0.05), fontsize=16)\n\nax.annotate(fr'$D_{1}$={Dq_multifrac[nq==nq[idx_one]][0]:.3f}', \n            xy=(nq[idx_one], Dq_multifrac[nq==nq[idx_one]]), \n            xytext=(nq[idx_one]-3, Dq_multifrac[nq==nq[idx_one]]-1.5),\n            arrowprops=dict(facecolor='black', shrink=0.05), fontsize=16)\n\nax.annotate(fr'$D_{2}$={Dq_multifrac[nq==nq[idx_two]][0]:.2f}', \n            xy=(nq[idx_two], Dq_multifrac[nq==nq[idx_two]]), \n            xytext=(nq[idx_two], Dq_multifrac[nq==nq[idx_two]]-1.5),\n            arrowprops=dict(facecolor='black', shrink=0.05), fontsize=16)\n\nplt.show();\n\n\n\n\n\n\n\nРис. 7.21: Залежність узагальнених фрактальних розмірностей \\(D(q)\\) від \\(q\\)\n\n\n\n\n\nЗ представленого рисунку видно, що, по-переше, для всіх сигналів \\(D_{0}=1\\), що відповідає теоретичним міркуванням. Інформаційна розмірність \\(D_{1}\\) для мультифрактала та білого шуму однакова, що може говорити і про інформаційну зачущість обох сигналів. Для монофрактала вона близька до нуля. Кореляційна розмірність \\(D_{2}\\) показує, що загалом як WTI, так і білий шум доволі схожі: їх значення в більшості своїй постають незалежними один від одного. Це розбігається з тими висновками, що були зробені в попередній роботі, де наближення \\(D_{2}\\) до нуля вказувало на зростання ступеня корельованості системи. Для монофракталу \\(D_{2}\\) знаходиться на рівні 1, що вказує на вищий ступінь кореляцій у цьому сигналі, порівнюючи з мульти- та монофракталами.\n\n\n7.1.4.9 Аналогії мультифракталів із термодинамікою\nВикористовуючи концепції MFDFA, ми можемо по-новому поглянути на часовий сигнал як на термодинамічну систему. В рамках MFDFA показник маси \\(\\tau(q)\\) можна розглядати як аналоги вільної енергії, показник сингулярності \\(\\alpha\\) як аналог внутрішньої енергії \\(U\\), мультифрактальний спектр \\(f(\\alpha)\\) як ентропію. Дійсно, форма мультифрактального спектра нагадує залежність ентропії термодинамічної системи від енергії \\(U\\). Показники \\(\\alpha_{min}\\) та \\(\\alpha_{max}\\) можна охарактеризувати як верхній та нижній ліміти внутрішньої енергії системи. Функція \\(Z(q)\\) є формальним аналогом статистичної суми \\(Z(\\beta)\\) в термодинаміці, де \\(\\beta=1/T=q\\).\n\n\n\n\n\n\nРис. 7.22: Схематичне представлення аналогії мультифракталів із концепціями термодинаміки\n\n\n\nОкрім цього, ми можемо прорахувати “температуру” досліджуваного сигнала. Більш конкретно, мультифрактальну “питому теплоємність” \\(C(q)\\)  [23] можна визначити як\n\\[\nC(q) \\equiv -\\left(\\partial^{2} \\tau(q) \\big/ \\partial q^{2} \\right) = -\\left(\\partial \\alpha \\big/ \\partial q \\right) \\approx \\tau(q+1) - 2\\tau(q) + \\tau(q-1).\n\\]\nПитома теплоємність, як міра швидкості зміни енергії, слугує індикатором явищ фазового переходу. У термодинамічній системі фаза характеризується однорідними фізичними властивостями, а фазовий перехід — стрибкоподібною зміною певних властивостей за критичної зовнішньої умови. Вивчення фазових переходів у мультифрактальному спектрі обмежувалося простими системами, такими як множина Кантора та логістична карта. Однак наш аналіз показує наявність фазових переходів  [24] у мультифрактальному спектрі фінансових систем. Зокрема, “енергія” \\(\\alpha\\) виявляє значні флуктуації в околі \\(q_c\\), які відображаються піком питомої теплоємності \\(C(q_c)\\).\n\nC_q_multifrac = -np.gradient(alpha_multifrac, nq, edge_order=2)\nC_q_monofrac = -np.gradient(alpha_monofrac, nq, edge_order=2)\nC_q_white_noise = -np.gradient(alpha_white_noise, nq, edge_order=2)\n\n\nfig, ax = plt.subplots(1, 2)\n\nax[0].plot(nq, C_q_multifrac, linestyle='-', marker='o', label=\"Мультифрактал\", color='darkblue')\nax[0].plot(nq, C_q_monofrac, linestyle='-', marker='o', label=\"Монофрактал\", color='magenta')\nax[0].plot(nq, C_q_white_noise, linestyle='-', marker='o', label=\"Білий шум\", color='red')\nax[0].set_xlabel(r\"$q$\")\nax[0].set_ylabel(r\"$C(q)$\")\nax[0].legend(loc='center left')\n\nax[1].plot(alpha_multifrac, C_q_multifrac, linestyle='-', marker='o', label=\"Мультифрактал\", color='darkblue')\nax[1].plot(alpha_monofrac, C_q_monofrac, linestyle='-', marker='o', label=\"Монофрактал\", color='magenta')\nax[1].plot(alpha_white_noise, C_q_white_noise, linestyle='-', marker='o', label=\"Білий шум\", color='red')\nax[1].set_xlabel(r\"$\\alpha$\")\n\nfig.tight_layout(pad=0.3) \n\nplt.show();\n\n\n\n\n\n\n\nРис. 7.23: Залежність мультифрактальної теплоємності \\(C(q)\\) від \\(q\\) та \\(\\alpha\\)\n\n\n\n\n\nРис. 7.23 демонструє, що \\(C(q)\\) досягає максимуму в діапазоні \\(q\\) від 2 до 3, що свідчить про те, що індекс WTI стає вкрай нерегулярним через велику флуктуацію катастрофи “COVID-19”, яка слугує квазіфазовим переходом досліджуваного індексу. Можна стверджувати, що колапс коронавірусної пандемії має найбільший вплив на мультифрактальність досліджуваного ряду.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Лабораторна робота № 7</span>"
    ]
  },
  {
    "objectID": "lab_7.html#хід-роботи",
    "href": "lab_7.html#хід-роботи",
    "title": "7  Лабораторна робота № 7",
    "section": "7.2 Хід роботи",
    "text": "7.2 Хід роботи\nЗвісно ж фрактальний аналіз усього ряду є важливим, але такий підхід ігнорує припущення про існування в часовій послідовності як монофрактальних, так і мультифрактальних ділянок. Тобто, ігнорується припущення про зміну ступеня складності з плином часу. Кількісні міри мультифрактальності, розраховані в рамках підходу ковзного вікна, є найбільш об’єктивними та практичними при аналізі систем. Окрім цього кількісні показники можуть бути використані як індикатори або індикатори-провісники аномальних явищ, або як базис для побудови іншої прогностичної моделі.\nПовторно зчитаємо значення ряду із сайту FRED:\n\nsymbol = 'DCOILWTICO'    # cимвол індексу, як указано на сайті FRED\nstart = \"1986-01-01\"     # Дата початку зчитування даних\nend = \"2023-01-21\"       # Дата закінчення зчитування даних\n\nwti = web.DataReader(symbol, 'fred', start, end) # зчитуємо значення ряду \ntime_ser = wti[symbol].copy()                    # зберігаємо саме ціни закриття\n\n#------- фільтрація від'ємних значень -------\ntime_ser = time_ser.dropna()    # видаляємо значення NaN\ntime_ser[time_ser.values&lt;0] = 5 # замінюємо від'ємне значення на 5\n#--------------------------------------------\n\nxlabel = 'time, days'    # підпис по вісі Ох \nylabel = symbol          # підпис по вісі Оу\n\n\n\n\n\n\n\nУвага\n\n\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance або FRED, а із власного файлу. Зрозуміло, що й аналіз результатів, і висновки залежать від того з яким рядом ми працюємо\n\n\n\n\nsymbol = 'sMpa11'                  # Символ індексу\n\npath = \"databases\\sMpa11.txt\"      # шлях по якому здійснюється зчитування файлу\ndata = pd.read_csv(path,           # зчитування даних \n                   names=[symbol])\ntime_ser = data[symbol].copy()     # копіюємо значення кривої \n                                   # \"напруга-видовження\" до окремої змінної\n\nxlabel = r'$\\varepsilon$'          # підпис по вісі Ох \nylabel = symbol                    # підпис по вісі Оу\n\n\nВиводимо досліджуваний ряд:\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\n\n\n\nРис. 7.24: Динаміка щоденних змін індексу сирої нафти\n\n\n\n\n\nДеякі графіки будуть представляти парну побудову лише часового ряду та мультифрактального індикатора. Скористаємось функцією plot_pair(), що ми визначали в попередніх лабораторних:\n\ndef plot_pair(x_values, \n              y1_values,\n              y2_values,  \n              y1_label, \n              y2_label,\n              x_label, \n              file_name, \n              clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(x_values, \n                  y1_values, \n                  \"b-\", label=fr\"{y1_label}\")\n    p2, = ax2.plot(x_values,\n                   y2_values, \n                   color=clr, \n                   label=y2_label)\n\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(f\"{y1_label}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n\n\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(file_name + \".jpg\")\n        \n    plt.show();\n\n\n7.2.0.1 Віконна процедура\nДля подальших розрахунків знову скористаємось бібліотекою fathon, що ми використовували попередньо для виконання класичного аналізу детрендованих флуктуацій. Переваги саме цієї бібліотеки полягають у можливості використання процедури розрахунку розбиття ряду на сегменти починаючи з кінця ряду, оскільки довжина ряду не завжди дозволяє поділити його на локальні сегменти націло. Тобто, теоретично у нас залишається сегмент ряду, що не піддається розбиттю на локальні сегменти. Тому повторна процедура розбиття на локальні сегменти починаючи з кінця ряду дозволяє обійти дану проблему. Для спрощення представлення теоретичного матеріалу ми не стали імплементувати дану процедуру, але вона доступна в бібліотеці fathon. Окрім цього, бібліотека надає можливість обчислення крос-кореляційного аналізу детрендованих флуктуацій та його мультифрактального аналогу.\n\nimport fathon\nfrom fathon import fathonUtils as fu\n\nПриступимо до ініціалізації параметрів.\n\nwindow = 500        # розмір вікна\ntstep = 5           # крок вікна\nret_type = 4        # вид ряду: \n                    # 1 - вихідний, \n                    # 2 - детрендований (різниця між теп. значенням та попереднім)\n                    # 3 - прибутковості звичайні, \n                    # 4 - стандартизовані прибутковості, \n                    # 5 - абсолютні значення (волатильності)\n                    # 6 - стандартизований ряд\n\nwin_beg = 10        # Початкова ширина сегменту\nwin_end = window-1  # Кінцева ширина сегменту\n\nscales_exp_wind = fu.linRangeByStep(win_beg, win_end) # генеруємо масив \n                                                      # лінійно розділених \n                                                      # елементів\n\nrev = True      # чи повторювати розрахунок ф-ції флуктуацій з кінця\n\nlength = len(time_ser.values)\n\nq_min = -5         # мінімальне значення q\nq_max = 5          # максимальне значення q\nq_step = 1         # крок збільшення q\n\nnq = np.arange(q_min, \n               q_max+q_step, \n               q_step)\n\norder = 1           # порядок поліноміального тренду\n\ndelta_alph = []\ndelta_spec = []\nmax_alph = []\nmin_alph = []\nmean_alph = []\nalpha_zero = []\ndelta_alph_right = []\ndelta_alph_left = []\nassym = []\ndelta_s = []\nD_0 = []\nD_1 = []\nD_2 = []\nD_left = []\nD_right = []\nC_q = []\nh_q = []\ntau_q = []\nD_q = []\nmfSpect = []\nalpha = []\nhFI = []\nalphaCF = []\nC_q_area_wind = []\n\nРозпочнемо процедуру рухомого вікна, що об’єднуватиме у собі попередні етапи розрахунків:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # знаходження кумулятивного ряду\n    cumulative = fu.toAggregated(fragm)\n\n    # інціалізації процедури mfdfa\n    pymfdfa = fathon.MFDFA(cumulative)\n\n    # обчислення функції флуктуацій та отримання узагальненого показника Херста\n    n, F = pymfdfa.computeFlucVec(scales_exp_wind, nq, revSeg=rev, polOrd=order)\n    Hq_fragm, _ = pymfdfa.fitFlucVec()\n\n    # отримання показника tau\n    tau_wind = nq * Hq_fragm - 1\n\n    # отримання показника сингулярності\n    alpha_wind = np.gradient(tau_wind, nq, edge_order=2)\n\n    # отримання мультифрактального спектра\n    f_wind = nq * alpha_wind - tau_wind\n\n    # отримання мультифрактальної теплоємності\n    C_q_wind = -np.gradient(alpha_wind, nq, edge_order=2)\n\n    # інтегральний показник C(q) \n    C_q_area = cumulative_trapezoid(np.abs(C_q_wind), nq, initial=0)[-1]\n\n    # ширина спектра мультифрактальності\n    delta_alpha_wind = alpha_wind.max() - alpha_wind.min()\n\n    # відстань між кінцями спектра мультифрактальності\n    delta_phi = f_wind[-1] - f_wind[0]\n\n    # максимальне значення альфи\n    maximal_alpha = alpha_wind.max()\n\n    # мінімальне значення альфи\n    minimal_alpha = alpha_wind.min()\n\n    # середнє значення альфи\n    mean_alpha = np.mean(alpha_wind)\n\n    # значення сингулярності при якому спектр приймає максимальне значення (α0)\n    alpha_0 = alpha_wind[np.nanargmax(f_wind)]\n\n    # ширина правого хвоста спектра\n    delt_alpha_right = maximal_alpha - alpha_0\n\n    # ширина лівого хвоста спектра\n    delt_alpha_left = alpha_0 - minimal_alpha\n\n    # різниця між шириною лівого та правого хвостів\n    delt_s = delt_alpha_right - delt_alpha_left\n\n    # показник асиметрії\n    A = (delt_alpha_left - delt_alpha_right)/(delt_alpha_left + delt_alpha_right)\n\n    # визначаємо індекс при q=0\n    difference_zero = np.absolute(nq-0)\n    idx_zero = difference_zero.argmin()\n\n    # визначаємо індекс при q=1\n    difference_one = np.absolute(nq-1)\n    idx_one = difference_one.argmin()\n\n    # визначаємо індекс при q=2\n    difference_two = np.absolute(nq-2)\n    idx_two = difference_two.argmin()\n\n    # ініціалізуємо масиви під розмірності\n    Dq_wind = np.zeros(len(nq))\n\n    # Визначаємо узагальнені фрактальні розмірності там де q!=1 \n    Dq_wind[nq!=nq[idx_one]] = tau_wind[nq!=nq[idx_one]] / (nq[nq!=nq[idx_one]] - 1)\n\n    # Визначаємо окремо узагальнені фрактальні розмірності при q=1\n    Dq_wind[nq==nq[idx_one]] = -tau_wind[nq==nq[idx_one]] \n\n    # Узагальнені фрактальні розмірності отримані з мультифрактального спектра\n    D_zero = f_wind[nq==nq[idx_zero]]\n    D_one = f_wind[nq==nq[idx_one]]\n    D_two = 2 * alpha_wind[nq==nq[idx_two]] - f_wind[np.where(alpha_wind[nq==nq[idx_two]])]\n\n    # відстань від центру розподілу узагальнених розмірностей до лівого кінця\n    delta_D_Q_left = Dq_wind[nq==q_min] - Dq_wind[nq==nq[idx_zero]]\n\n    # відстань від центру розподілу узагальнених розмірностей до правого кінця\n    delta_D_Q_right = Dq_wind[nq==nq[idx_zero]] - Dq_wind[nq==q_max]\n\n    # індекс h-флуктуацій (hFI)\n    fluct = np.sum(np.gradient(np.gradient(Hq_fragm, nq, edge_order=2), nq, edge_order=2) ** 2)/(2 * np.max(np.abs(nq)) + 2)\n\n    # кумулятивний індекс інкрементів узагальнених показників Херста (αCF)\n    incr = np.sum(np.gradient(Hq_fragm, edge_order=2) ** 2 / np.gradient(nq, edge_order=2))\n\n    delta_alph.append(delta_alpha_wind)\n    delta_spec.append(delta_phi)\n    max_alph.append(maximal_alpha)\n    min_alph.append(minimal_alpha)\n    mean_alph.append(mean_alpha)\n    alpha_zero.append(alpha_0)\n    delta_alph_right.append(delt_alpha_right)\n    delta_alph_left.append(delt_alpha_left)\n    delta_s.append(delt_s)\n    assym.append(A)\n    D_0.append(D_zero)\n    D_1.append(D_one)\n    D_2.append(D_two)\n    D_left.append(delta_D_Q_left)\n    D_right.append(delta_D_Q_right)\n    C_q.append(C_q_wind)\n    mfSpect.append(f_wind)\n    alpha.append(alpha_wind)\n    hFI.append(fluct)\n    alphaCF.append(incr)\n    C_q_area_wind.append(C_q_area)\n    h_q.append(Hq_fragm)\n    tau_q.append(tau_wind)\n    D_q.append(Dq_wind)\n\n100%|██████████| 1768/1768 [04:03&lt;00:00,  7.27it/s]\n\n\nЗберігаємо абсолютні значення показників до текстових файлів.\n\n# перелік назв кожного індикатора для збереження до txt\nsubtitle_of_txts = ['delta_alpha', 'delta_f', 'max_alpha', 'min_alpha', 'mean_alpha', \n                    'zero_alpha', 'delta_alpha_right', 'delta_alpha_left', 'assymetry',\n                    'delta_s', 'D_0', 'D_1', 'D_2', 'hFI', 'alphaCF', 'C_q_area',\n                    'delta_d_left', 'delta_d_right']\n\n# перелік вихідних значень індикаторів для збереження до txt\nmfdfa_indicators = [delta_alph, delta_spec, max_alph, min_alph, mean_alph, alpha_zero,\n                    delta_alph_right, delta_alph_left, assym, delta_s, D_0, D_1, D_2,\n                    hFI, alphaCF, C_q_area_wind, D_left, D_right]\n\nfor i in range(len(subtitle_of_txts)):\n    np.savetxt(f\"mfdfa_{subtitle_of_txts[i]}_name={symbol}_ret={ret_type}_ \\\n               order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_ \\\n               wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}.txt\", mfdfa_indicators[i])\n\nПроаналізуємо динаміку отриманих індикаторів.\n\n\n7.2.1 Ширина спектра мультифрактальності (\\(\\Delta\\alpha\\))\nПершим і одним із найпрактичніших індикаторів складності системи є ширина спектра мультифрактальності, \\(\\Delta\\alpha\\), яку можна подати як різницю між максимальним ступенем синґулярності та мінімальним:\n\\[\n\\Delta\\alpha = \\alpha_{max} - \\alpha_{min}.\n\\tag{7.10}\\]\nЯкщо проводити аналогію з термодинамічними показниками, то в такому разі ширина спектра мультифрактальності становитиме різницю між найбільшим і найменшим значенням внутрішньої енергії системи. Розглянемо динаміку даного показника для ринку нафти.\n\nmeasure_label = r'$\\Delta\\alpha$'\nfile_name = f\"mfdfa_delta_alpha_name={symbol}_ret={ret_type}_order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_ \\\n           wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          delta_alph, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr='red')\n\n\n\n\n\n\n\nРис. 7.25: Динаміка індексу сирої нафти WTI та показника ширини спектра мультифрактальності \\(\\Delta\\alpha\\)\n\n\n\n\n\nНа Рис. 7.25 видно, що ширина спектра мультифрактальності зростає під час кризових подій, що вказує на зростання загального ступеня складності та періодизації. Тобто, даний показник слугує ще одним підтвердженням, що трейдери на ринку, наприклад, нафти поводять себе у синхронній манері в ході кризи. Зростання загального ступеня мультифрактальності є індикатором зростання кореляцій в системі, що підтверджувалось і попередніми індикаторами складності.\n\n\n7.2.2 Різниця між кінцями спектра мультифрактальності (\\(\\Delta f\\))\nОднак проста ширина спектра мультифрактальності не показує, наприклад, флуктуації якого типу є найвірогіднішими, елементи якого типу щільності відіграють найбільшу роль у зростанні або зниженні складності системи. Надалі було запропоновано такий показник мультифрактальності як \\(\\Delta f\\), який можна представити наступним чином  [25–27]:\n\\[\n\\Delta f = f(\\alpha_{min}) - f(\\alpha_{max}).\n\\tag{7.11}\\]\n\nmeasure_label = r'$\\Delta f$'\nfile_name = f\"mfdfa_delta_f_name={symbol}_ret={ret_type}_order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_ \\\n           wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          delta_spec, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr='brown')\n\n\n\n\n\n\n\nРис. 7.26: Динаміка індексу сирої нафти WTI та показника відстані між кінцями спектра мультифрактальності \\(\\Delta f\\)\n\n\n\n\n\nСенс даного показника полягає в тому, що він дає нам змогу визначити ступінь імовірності появи елементів з великими щільностями і малими. Якщо цей показник менший за нуль, тоді флуктуації, що відображають елементи з найбільшою концентрацією (найбільшими фруктуаціями), мають найбільшу ймовірність. Якщо цей показник вищий за нуль, тоді флуктуації, що відображають малоконцентровані елементи (малі флуктуації), визначають динаміку системи. Якщо цей показник перебуває в нулі, тоді як високосингулярні, так і малосингулярні елементи мають рівномірний внесок у динаміку системи.\nЗвертаючись до термодинаміки, можна згадати, що \\(f(\\alpha)\\) — це ентропія системи. Тоді стає зрозуміло, що варіативність спектра мультифрактальності дає нам змогу визначити ступінь внеску висококонцентрованих і малоконцентрованих елементів у мінімізацію ентропії системи. Лівостороння асиметрія мультифрактального спектра \\((\\Delta f &gt; 0)\\) підказує нам, що висококонцентровані елементи фазового простору дають найбільший внесок у мінімум термодинамічної ентропії. Іншими словами, ці елементи і є двигуном зростання впорядкованості динаміки системи. Своєю чергою, правостороння асиметрія мультифрактального спектра \\((\\Delta f &lt; 0)\\) вказує на мінімізацію ентропії за рахунок малоконцентрованих елементів. Симетрія кінців спектра вказує на рівний внесок високощільних і розріджених областей на мінімізацію ентропії. Як уже згадувалося, трапляються випадки, коли мультифрактальний спектр практично сходиться в сингулярність (одну точку). У такому разі ми маємо справу з простою монофрактальною системою, яка в нашому випадку характеризувалася незалежними і нормально розподіленими випадковими величинами. Для такого спектра і \\(\\Delta\\alpha\\), і \\(\\Delta f\\) будуть прямувати до нуля. Для такого часового ряду вже спостерігається не множина фрактальних розмірностей, а тільки один фрактальний показник, \\(f[\\alpha(q=0)] = 1\\). Це саме та область, де система досягає своєї термодинамічної рівноваги — максимуму ентропії. У свою чергу \\(\\Delta f\\) можна охарактеризувати як різницю ентропій за граничної максимальної та мінімальної внутрішньої енергії системи.\n\n\n7.2.3 Ширина лівого (\\(\\Delta\\alpha_{L}\\)) та правого (\\(\\Delta\\alpha_{R}\\)) хвостів мультифрактального спектра\nКрім цього, ми можемо дослідити ступінь складності динаміки окремо високощільних областей (з великими флуктуаціями) і низькощільних (з малими флуктуаціями). Для цього виміряємо ширину окремо лівого і правого хвостів. Ширину лівого хвоста визначимо як\n\\[\n\\Delta\\alpha_{L} = \\alpha_{0} - \\alpha_{min},\n\\tag{7.12}\\]\nа ширину правого хвоста як\n\\[\n\\Delta\\alpha_{R} = \\alpha_{max} - \\alpha_{0}.\n\\tag{7.13}\\]\nУ свою чергу ширина лівого хвоста вимірює ступінь складності флуктуацій з великою амплітудою, а ширина правого хвоста — малих. Зростання ширини кожного з хвостів відображатиме зростання ступеня кореляцій між елементами.\n\nfig, ax = plt.subplots(1, 1)\n\nax2 = ax.twinx()\nax3 = ax.twinx()\n\nax2.spines.right.set_position((\"axes\", 1.03))\nax3.spines.right.set_position((\"axes\", 1.12))\n\np1, = ax.plot(time_ser.index[window:length:tstep], \n              time_ser.values[window:length:tstep], \n              \"b-\", label=fr\"{ylabel}\")\np2, = ax2.plot(time_ser.index[window:length:tstep], \n               delta_alph_left, color=\"r\", label=r\"$\\Delta\\alpha_{L}$\")\np3, = ax3.plot(time_ser.index[window:length:tstep], \n               delta_alph_right, color=\"g\", label=r\"$\\Delta\\alpha_{R}$\")\n\n\nax.set_xlabel(xlabel)\nax.set_ylabel(f\"{ylabel}\")\n\nax.yaxis.label.set_color(p1.get_color())\nax2.yaxis.label.set_color(p2.get_color())\nax3.yaxis.label.set_color(p3.get_color())\n\ntkw = dict(size=4, width=1.5)\nax.tick_params(axis='y', colors=p1.get_color(), **tkw)\nax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\nax3.tick_params(axis='y', colors=p3.get_color(), **tkw)\nax.tick_params(axis='x', **tkw)\n\nax3.legend(handles=[p1, p2, p3])\n\nplt.savefig(f\"mfdfa_delta_alpha_left_right_name={symbol}_ret={ret_type}_order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_ \\\n           wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}.jpg\")\nplt.show();\n\n\n\n\n\n\n\nРис. 7.27: Динаміка індексу сирої нафти WTI та ширини лівого і правого хвостів мультифрактального спектра\n\n\n\n\n\nІз Рис. 7.27 видно, що досліджувані індикатори реагують у характерний спосіб на кризові події. Ширина лівої сторони спектра мультифрактальності зростає під час 1992, 1996-2000, 2008, 2016 років та коронавірусної пандемії. Це вказує на домінацію висококонцентрованих флуктуацій (з великою амплітудою коливань). Окрім цього, зростання ширини лівого хвоста вказує на те, що флуктуації з великою амплітудою коливань характеризуються зростанням ступеня кореляцій під час кризових подій, що в свою чергу може слугувати індикатором зростання процесів самоорганізації.\nХоча і меншою, але не менш примітною є динаміка ширини правого хвоста мультифрактального спектра. Цей індикатор працює майже аналогічно до ширини лівого хвоста, але характеризує динаміку малоконцентрованих величин — флуктуацій з малою амплітудою коливань. Майже синхронна динаміка обох показників указує на зростання впливу коливань як флуктуацій з великою амплітудою коливань, так і флуктуацій з малою амплітудою коливань. Тобто, два типи флуктуацій є джерелом зростання нелінійних кореляцій під час кризових подій.\n\n\n7.2.4 Показник сингулярності \\(\\alpha\\) та його різновиди\nВ якості можливих індикаторів складності системи можна взяти \\(\\alpha_{min}\\), \\(\\alpha_{max}\\), \\(\\alpha_{mean}\\) і \\(\\alpha_0\\), які, відповідно, характеризують мінімальну силу сингулярності, максимальну, середню і сингулярність за умови рівноважного врахування як великих флуктуацій, так і малих.\n\nfig, ax = plt.subplots(1, 1)\n\nax2 = ax.twinx()\nax3 = ax.twinx()\nax4 = ax.twinx()\nax5 = ax.twinx()\n\nax3.spines.right.set_position((\"axes\", 1.08))\nax4.spines.right.set_position((\"axes\", 1.18))\nax5.spines.right.set_position((\"axes\", 1.27))\n\np1, = ax.plot(time_ser.index[window:length:tstep], time_ser[window:length:tstep], \"b-\", label=fr\"{ylabel}\")\np2, = ax2.plot(time_ser.index[window:length:tstep], max_alph, \"r-\", label=r\"$\\alpha_{max}$\")\np3, = ax3.plot(time_ser.index[window:length:tstep], min_alph, \"g-\", label=r\"$\\alpha_{min}$\")\np4, = ax4.plot(time_ser.index[window:length:tstep], mean_alph, \"c-\", label=r\"$\\alpha_{mean}$\")\np5, = ax5.plot(time_ser.index[window:length:tstep], alpha_zero, \"m-\", label=r\"$\\alpha_{0}$\")\n\nax.set_xlabel(xlabel)\nax.set_ylabel(fr\"{ylabel}\")\n\nax.yaxis.label.set_color(p1.get_color())\nax2.yaxis.label.set_color(p2.get_color())\nax3.yaxis.label.set_color(p3.get_color())\nax4.yaxis.label.set_color(p4.get_color())\nax5.yaxis.label.set_color(p5.get_color())\n\ntkw = dict(size=4, width=1.5)\nax.tick_params(axis='y', colors=p1.get_color(), **tkw)\nax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\nax3.tick_params(axis='y', colors=p3.get_color(), **tkw)\nax4.tick_params(axis='y', colors=p4.get_color(), **tkw)\nax5.tick_params(axis='y', colors=p5.get_color(), **tkw)\nax.tick_params(axis='x', **tkw, pad=10)\n\nax5.legend(handles=[p1, p2, p3, p4, p5])\n\nplt.savefig(f\"mfdfa_alpha_min_max_mean_zero_name={symbol}_ret={ret_type}_order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_ \\\n            wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}.jpg\", bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\nРис. 7.28: Динаміка індексу сирої нафти WTI та показників сингулярності\n\n\n\n\n\nЯк видно з Рис. 7.28, усі показники сингулярності зростають в області фінансового фазового переходу зі стану стабільності до стану кризи. Це говорить про зростання складності системи: різкому прирості кількості агентів, що задіяні в самоорганізованому розвитку досліджуваної системи. З погляду термодинаміки можна було б сказати, що при фінансових крахових подіях зростає внутрішня енергія системи.\n\n\n7.2.5 Тип довгого хвоста мультифрактального спектра (\\(\\Delta S\\))\nКрім такої міри як, наприклад, \\(\\Delta f\\) можна представити й інші міри асиметрії мультифрактального спектра. Наприклад, ми можемо визначити тип довгого хвоста мультифрактального спектра за допомогою показника \\(\\Delta S\\)  [28]:\n\\[\n\\Delta S = \\Delta\\alpha_{R} - \\Delta\\alpha_{L}.\n\\tag{7.14}\\]\nЯкщо \\(\\Delta S &lt; 0\\), мультифрактальний спектр має довгий лівий хвіст, що свідчить про чутливість часового ряду до локальних флуктуацій з великою амплітудою. Якщо \\(\\Delta S &gt; 0\\), мультифрактальний спектр має довгий правий хвіст, який вказує на чутливість структури сигналу до локальних флуктуацій з малою амплітудою коливань. У тих випадках, коли високо- і низькофлуктуаційні компоненти сигналу співставні, спектр сингулярностей буде приблизно симетричним і \\(\\Delta\\alpha_{R} = \\Delta\\alpha_{L}\\).\n\nmeasure_label = r'$\\Delta S$'\nfile_name = f\"mfdfa_delta_s_name={symbol}_ret={ret_type}_order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_ \\\n           wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          delta_s, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr='darkorange')\n\n\n\n\n\n\n\nРис. 7.29: Динаміка індексу сирої нафти WTI та показника типу хвоста мультифрактального спектра \\(\\Delta S\\)\n\n\n\n\n\nЗ огляду на Рис. 7.29 видно, що \\(\\Delta S &lt; 0\\), що свідчить про те, що найбільш крахові ділянки нафтового ринку зумовлені флуктуаціями з великою амплітудою коливань. Особливо помітними тут предстають кризи 1992 та 2020 років.\n\n\n7.2.6 Показник асиметрії \\(A\\)\nДалі ми можемо визначити наступний параметр асиметрії  [29–31]:\n\\[\nA = (\\Delta\\alpha_{L} - \\Delta\\alpha_{R})\\big/(\\Delta\\alpha_{R} + \\Delta\\alpha_{L}) = -\\Delta S \\big/ \\Delta\\alpha.\n\\tag{7.15}\\]\nПараметр асиметрії пов’язаний з предомінуючим типом коливань у досліджуваній системі. Якщо \\(A=0\\) (\\(\\Delta\\alpha_{L}=\\Delta\\alpha_{R}\\)), динаміку системи представляє симетричний спектр. Якщо \\(A&lt;0\\) (\\(\\Delta\\alpha_{L}&lt;\\Delta\\alpha_{R}\\)), мультифрактальний спектр має правосторонню асиметрію, що підкреслює сильніший вплив малих флуктуацій на мультифрактальність. І навпаки, коли \\(A&gt;0\\) (\\(\\Delta\\alpha_{L}&gt;\\Delta\\alpha_{R}\\)), тоді ми маємо справу з лівостороннім спектром, який позначає більшу неоднорідність для великих флуктуацій і вказує на те, що в часовому ряді переважає мультифрактальна природа неоднорідностей з високими щільностями. Оскільки асиметрію виявляють за знаком \\(A\\), що еквівалентний знаку \\(\\Delta S\\), то, ґрунтуючись тільки по знаку \\(\\Delta S\\), можна зробити висновки як про тип довгого хвоста, так і про знак показника \\(A\\) мультифрактального спектра, тобто нечутливість і тип домінантних коливань мультифрактальності часового ряду.\n\nmeasure_label = r'$A$'\nfile_name = f\"mfdfa_A_name={symbol}_ret={ret_type}_order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_ \\\n           wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          assym, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr='darkviolet')\n\n\n\n\n\n\n\nРис. 7.30: Динаміка індексу сирої нафти WTI та показника асиметрії мультифрактального спектра \\(A\\)\n\n\n\n\n\nЗ Рис. 7.30 видно, що, як правило, показник асиметрії зростає під час крахових подій і вказує на домінацію лівостороннього спектра (висококонцентрованих флуктуацій з великою амплітудою коливань). Окрім цього видно, що, наприклад, для криз 2008 та 2015-2016 років спостерігалась помітна короткочасна правостороння асиметрія, що характеризується швидкоплинним сплеском показника асиметрії у сторону від’ємних значень. Асоціювати малі та великі флуктуації з конкретними настроями або поведінковими патернами ринку доволі складно. На даний момент ми можемо відзначити тільки те, що дані події представляли найбільш багату варіацію як короткострокових, так і довгострокових кореляцій.\n\n\n7.2.7 Індекс \\(h\\)-флуктуацій (\\(hFI\\))\nФлуктуацію можна проаналізувати за допомогою другої похідної узагальненого показника Херста. Зауважимо, що амплітуда другої похідної у випадку мультифрактальних сигналів більша, ніж для монофрактальних. Для одержання потрібної інформації з \\(h(q)\\) було запропоновано \\(h\\)-індекс флуктуації \\((hFI)\\)  [32], що визначається як степінь другої похідної від \\(h(q)\\):\n\\[\nhFI = \\frac{1}{2|q_{max}|+2}\\sum_{q=q_{min}-2}^{q_{max}}\\left[ h(q) - 2h(q-1) + h(q-2) \\right]^{2}.\n\\tag{7.16}\\]\nЧим вище значення даного показника, тим вища самоорганізованість системи.\n\nmeasure_label = r'$hFI$'\nfile_name = f\"mfdfa_hFI_name={symbol}_ret={ret_type}_order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_ \\\n           wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          hFI, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr='green')\n\n\n\n\n\n\n\nРис. 7.31: Динаміка індексу сирої нафти WTI та індексу \\(h\\)-флуктуацій \\(hFI\\)\n\n\n\n\n\nВидно, що за \\(hFI\\) найвищий ступінь мультифрактальності проявляється саме для криз 1992, 2008, 2016 та 2020 років. Дані крахові події включають в себе найбільшу кількість різноманітних факторів, що впливали на динаміку досліджуваної системи. Особливо помітно це для коронавірусної пандемії.\n\n\n7.2.8 Кумулятивний індекс інкрементів узагальнених показників Херста (\\(\\alpha CF\\))\nКумулятивна функція квадратів інкрементів (\\(\\alpha CF\\))  [33] узагальнених показників Херста між послідовними моментними порядками є більш надійним показником розподілу узагальнених показників Херста.\n\nmeasure_label = r'$\\alpha CF$'\nfile_name = f\"mfdfa_alphaCF_name={symbol}_ret={ret_type}_order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_ \\\n           wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          alphaCF, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr='crimson')\n\n\n\n\n\n\n\nРис. 7.32: Динаміка індексу сирої нафти WTI та кумулятивний індексу інкрементів узагальнених показників Херста \\(\\alpha CF\\)\n\n\n\n\n\nПредставлений кумулятивний індекс дещо відрізняється від \\(hFI\\), але за логікою приблизно однаковий: події з найвищим ступенем мультифрактальності характеризуються вищою амплітудою \\(\\alpha CF\\). Представлений показник виділяє ті самі кризи, що й попередній, але динаміка цього показника більш виразна, що робить його більш надійним для ідентифікації періодів самоорганізації системи.\n\n\n7.2.9 Інтегральна мультифрактальна теплоємність \\(C(q)\\)\nЗагальний ступінь мультифрактальності, інтегральну мультифрактальну питому теплоємність (\\(C_{area}\\)), можна виразити через рівняння (7.17):\n\\[\nC_{area} = \\int C(q)dq.\n\\tag{7.17}\\]\nРозрахуємо і проаналізуємо динаміку цього показника.\n\nmeasure_label = r'$C_{area}$'\nfile_name = f\"mfdfa_C_q_area_name={symbol}_ret={ret_type}_order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_ \\\n           wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          C_q_area_wind, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr='darkslateblue')\n\n\n\n\n\n\n\nРис. 7.33: Динаміка індексу сирої нафти WTI та інтегральної мультифрактальної теплоємності \\(C_{area}\\)\n\n\n\n\n\nЗ представленого рисунку видно, що динаміка інтегральної теплоємності дуже подібна до ширини спектра мультифрактальності. Тобто, \\(C_{area}\\) є показником складності, що вказує на ступінь самоорганізованості фінансового фазового переходу. Видно, що фінансові крахи представляють доволі трендостійку динаміку, що є наслідком цілеспрямованих та колективних дій трейдерів на ринку.\n\n\n7.2.10 Хаусдорфова розмірність (\\(D_{0}\\))\nЯк уже зазначалось, \\(D_{0}\\) представляє верхню межу змін розмірностей фрактальних підмножин атрактора системи. Інформацію про статистичні властивості системи він не містить і не представляє особливої цінності.\n\nmeasure_label = r'$D_{0}$'\nfile_name = f\"mfdfa_D_0_area_name={symbol}_ret={ret_type}_order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_ \\\n           wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          D_0, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr='darkred')\n\n\n\n\n\n\n\nРис. 7.34: Динаміка індексу сирої нафти WTI та хаусдорфової розмірності цього часового сигналу \\(D_{0}\\)\n\n\n\n\n\n\n\n7.2.11 Інформаційна розмірність (\\(D_{1}\\))\nІнформаційна розмірність тісно пов’язана з інформаційною ентропією Шеннона. Чим вище значення \\(D_1\\), тим швидше збільшується ентропія, що є показником того, наскільки мало ми знаємо про поточний стан системи. Зі зменшенням \\(D_1\\) ентропія знижується, що, своєю чергою, свідчить про збільшення асиметрії в просторі, зменшення складності та розширення нашого розуміння поточного стану системи. Це також означає, що для опису можливих конфігурацій системи нам буде потрібно менше інформації.\n\nmeasure_label = r'$D_{1}$'\nfile_name = f\"mfdfa_D_1_area_name={symbol}_ret={ret_type}_order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_ \\\n           wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          D_1, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr='darkred')\n\n\n\n\n\n\n\nРис. 7.35: Динаміка індексу сирої нафти WTI та інформаційної розмірності цього часового сигналу \\(D_{1}\\)\n\n\n\n\n\nНа Рис. 7.35 видно, що інформаційна розмірність характеризується спадом під час крахових подій. Це вказує на зростання ступеня впорядкованості системи та колективної атракції агентів ринку до конкретної області фазового простору досліджуваної системи.\n\n\n7.2.12 Кореляційна розмірність (\\(D_{2}\\))\nКореляційну розмірність, аналогічно інформаційній розмірності, можна подати як тангенс кута нахилу лінії регресії, побудованої в логарифмічному масштабі, щодо залежності кореляційного інтеграла \\(C(\\varepsilon)\\) від \\(\\varepsilon\\). Подібно до \\(D_1\\), кореляційна розмірність також визначає, як швидко змінюється значення кореляційного інтеграла.\n\nmeasure_label = r'$D_{2}$'\nfile_name = f\"mfdfa_D_2_area_name={symbol}_ret={ret_type}_order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_\\\n           wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          D_2, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr='darkred')\n\n\n\n\n\n\n\nРис. 7.36: Динаміка індексу сирої нафти WTI та кореляційної розмірності цього часового сигналу \\(D_{2}\\)\n\n\n\n\n\nКореляційна розмірність на Рис. 7.36 характеризується зростанням у передкризовий період та спадом під час кризи. Це говорить про те, що більшість агентів ринку починає орієнтуватися на один конкретний вектор розвитку системи.\n\n\n7.2.13 Кривизна лівого (\\(\\Delta D_{L}\\)) та правого (\\(\\Delta D_{R}\\)) хвостів розподілу узагальнених фрактальних розмірностей\nОхарактеризувати ступінь цієї складності можна за кривизною окремо правого та лівого хвостів узагальнених фрактальних розмірностей. Праву сторону (\\(\\Delta D_{R}\\)) можна визначити як\n\\[\n\\Delta D_{R} = D_0 - D_{q_{max}}.\n\\tag{7.18}\\]\nІ чим більшим буде значення цієї міри, тим сильнішим буде ступінь впливу елементів із найбільшою концентрацією (щільністю, амплітудою флуктуацій) на загальну складність системи.\nКривизна лівого хвоста кривої узагальнених фрактальних розмірностей (\\(\\Delta D_{L}\\)):\n\\[\n\\Delta D_{L} = D_{q_{min}} - D_{0}.\n\\tag{7.19}\\]\nЦей показник буде говорити нам про те, наскільки сильним є вплив найменш концентрованих елементів на складність системи.\n\nfig, ax = plt.subplots(1, 1)\n\nax2 = ax.twinx()\nax3 = ax.twinx()\n\nax2.spines.right.set_position((\"axes\", 1.03))\nax3.spines.right.set_position((\"axes\", 1.12))\n\np1, = ax.plot(time_ser.index[window:length:tstep], \n              time_ser.values[window:length:tstep], \n              \"b-\", label=fr\"{ylabel}\")\np2, = ax2.plot(time_ser.index[window:length:tstep], \n               D_left, color=\"g\", label=r\"$\\Delta D_{L}$\")\np3, = ax3.plot(time_ser.index[window:length:tstep], \n               D_right, color=\"r\", label=r\"$\\Delta D_{R}$\")\n\n\nax.set_xlabel(xlabel)\nax.set_ylabel(f\"{ylabel}\")\n\nax.yaxis.label.set_color(p1.get_color())\nax2.yaxis.label.set_color(p2.get_color())\nax3.yaxis.label.set_color(p3.get_color())\n\ntkw = dict(size=4, width=1.5)\nax.tick_params(axis='y', colors=p1.get_color(), **tkw)\nax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\nax3.tick_params(axis='y', colors=p3.get_color(), **tkw)\nax.tick_params(axis='x', **tkw)\n\nax3.legend(handles=[p1, p2, p3])\n\nplt.savefig(f\"mfdfa_delta_D_left_right_name={symbol}_ret={ret_type}_order={order}_qmin={q_min}_qmax={q_max}_qinc={q_step}_ \\\n           wind={window}_step={tstep}_windbeg={win_beg}_winden={win_end}.jpg\")\nplt.show();\n\n\n\n\n\n\n\nРис. 7.37: Індекс сирої нафти WTI та кривизна лівого і правого хвостів спектра узагальнених фрактальних розмірностей\n\n\n\n\n\nНа рисунку (Рис. 7.37) видно, що спостерігаються етапи, при який два показники можуть вести себе як асинхронно, так і синхронно. Для 1992 року видно, що у передкризовий етап спостерігалась короткочасна домінація великих флуктуацій, на що і вказує \\(\\Delta D_{R}\\). Під час краху спостерігалась домінація малоконцентрованих елементів, що демонструє зростання \\(\\Delta D_{L}\\). Для 2001 року бачимо зростання впливу правого хвоста і зменшення впливу лівого. Для краху 2008 року видно зростання впливу висококонцентрованих елементів напередодні кризи. Після цього домінують малоконцентровані елементи. Бачимо зростання впливу малоконцентрованих елементів під час кризи 2016 року, і закономірне зменшення участі великоконцентрованих елементів. Пандемія 2020-2021 років характеризувалась активною участю як малоконцентрованих елементів, так і великоконцентрованих, на що вказує зростання обох показників кривизни хвостів узагальнених розмірностей.\n\n\n7.2.14 Дво- та тривимірна візуалізація показників мультифрактальності\nПопередньо ми аналізували залежності \\(h(q)\\), \\(\\tau(q)\\), \\(D(q)\\), \\(C(q)\\) та \\(f(\\alpha)\\) для всього часового ряду. Тепер, скориставшись процедурою ковзного вікна, ми можемо подивитися на їх зміну з плином часу. Скористаємось часовим рядом цін на нафту.\nПерш за все оголосимо функції для побудови двовимірних графіків:\n\ndef plot_2d(X, Y, Z, subtitle_jpg, subtitle_fig, ylabel, barlabel, cmap, lims):\n\n    fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n\n    cp = ax.contourf(X, Y, Z, alpha=0.8, cmap=cmap)\n    plt.colorbar(cp, ax=ax, extend='both', label=barlabel)\n\n    ax.set_xlim((time_ser.index[window:length:tstep][0], \n                 time_ser.index[window:length:tstep][-1]))\n    ax.set_ylim((np.min(lims), np.max(lims)))\n\n    ax.set_xlabel(xlabel, fontsize=22)\n    ax.set_ylabel(ylabel, fontsize=22)\n\n    ax.set_title(subtitle_fig, pad=10, fontsize=22)\n\n    ax.tick_params(axis='both', which='major', pad=10, labelsize=22)\n\n    fig.tight_layout()\n\n    plt.savefig(f\"mfdfa_{subtitle_jpg}_name={symbol}_ret={ret_type}_order={order}_ \\\n                qmin={q_min}_qmax={q_max}_qinc={q_step}_windbeg={win_beg}_winden={win_end}.jpg\", \n                bbox_inches=\"tight\")\n    plt.show(); \n\nта тривимірних:\n\ndef plot_3d(X, Y, Z, subtitle_jpg, ylabel, zlabel, cmap):\n\n    fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n\n    surf = ax.plot_surface(X, Y, Z, cmap=cmap, rstride=2, cstride=2, linewidth=0)\n\n    ax.set_xlabel(xlabel, fontsize=22, labelpad=15)\n    ax.set_ylabel(ylabel, fontsize=22, labelpad=15)\n    ax.set_zlabel(zlabel, fontsize=22, labelpad=15)\n    ax.tick_params(axis='both', which='major', labelsize=18, pad=5)\n\n    fig.colorbar(surf, shrink=0.5, aspect=10, location='right', pad=0.1)\n\n    fig.tight_layout()\n\n    plt.savefig(f\"mfdfa_{subtitle_jpg}_name={symbol}_ret={ret_type}_order={order}_ \\\n                qmin={q_min}_qmax={q_max}_qinc={q_step}_windbeg={win_beg}_ \\\n                winden={win_end}.jpg\", bbox_inches=\"tight\")\n    \n    plt.show();\n\nПісля оголошення необхідний функцій можна приступати до візуалізації.\n\n7.2.14.1 Динаміка \\(h(q)\\) з ходом часу в дво- та тривимірному просторах\n\nX, Y = np.meshgrid(time_ser.index[window:length:tstep], nq)\nZ = np.array(h_q).T\n\nplot_2d(X, Y, Z, \n        subtitle_jpg='contour_h(q)', \n        subtitle_fig=fr\"Теплова діаграма $h(q)$\", \n        ylabel=r\"$q$\", \n        barlabel=r\"$h(q)$\",\n        cmap='jet',\n        lims=nq)\n\n\n\n\n\n\n\nРис. 7.38: Двовимірна контурна діаграма динаміки узагальненого показника Херста \\(h(q)\\), що змінюється з плином часу\n\n\n\n\n\n\nX, Y = np.meshgrid(np.arange(window, length, tstep), nq)\nZ = np.array(h_q).T\n\nplot_3d(X, Y, Z, \n        subtitle_jpg='3d_h(q)', \n        ylabel=r\"$q$\", \n        zlabel=r\"$h(q)$\",\n        cmap='jet')\n\n\n\n\n\n\n\nРис. 7.39: Тривимірна діаграма динаміки узагальненого показника Херста \\(h(q)\\), що змінюється з плином часу\n\n\n\n\n\nНа представлених рисунках (Рис. 7.38 та Рис. 7.39) видно, що узагальнений показник Херста характеризується значним ростом саме в період криз. Особливо високим \\(h(q)\\) предстає для \\(q &lt; 0\\), що говорить про значну персистентність малих флуктуацій у періоди турбулентності. Найвищим ступінь нелінійності в даному випадку представляють кризи 1992, 2008-2009, 2015-2016 та 2020-2021 років, що підтверджується й попередніми індикаторами.\n\n\n7.2.14.2 Динаміка \\(\\tau(q)\\) з ходом часу в дво- та тривимірному просторах\n\nX, Y = np.meshgrid(time_ser.index[window:length:tstep], nq)\nZ = np.array(tau_q).T\n\nplot_2d(X, Y, Z, \n        subtitle_jpg='contour_tau(q)', \n        subtitle_fig=fr\"Теплова діаграма $\\tau(q)$\", \n        ylabel=r\"$q$\", \n        barlabel=r\"$\\tau(q)$\",\n        cmap='viridis',\n        lims=nq)\n\n\n\n\n\n\n\nРис. 7.40: Двовимірна контурна діаграма динаміки показника \\(\\tau(q)\\), що змінюється з плином часу\n\n\n\n\n\n\nX, Y = np.meshgrid(np.arange(window, length, tstep), nq)\nZ = np.array(tau_q).T\n\nplot_3d(X, Y, Z, \n        subtitle_jpg='3d_tau(q)', \n        ylabel=r\"$q$\", \n        zlabel=r\"$\\tau(q)$\",\n        cmap='viridis')\n\n\n\n\n\n\n\nРис. 7.41: Тривимірна діаграма динаміки показника \\(\\tau(q)\\), що змінюється з плином часу\n\n\n\n\n\nЯк видно з представлених рисунків (Рис. 7.40 та Рис. 7.41), \\(\\tau(q)\\) стає більш нелінійним для всіх значень \\(q\\). На кінцях хвостів цього індикатора можна помітити значні впадини, що можуть слугувати індикаторами крахових подій, але в порівнянні з тим же показником Херста даний індикатор є менш виразним.\n\n\n7.2.14.3 Динаміка \\(D(q)\\) з ходом часу в дво- та тривимірному просторах\n\nX, Y = np.meshgrid(time_ser.index[window:length:tstep], nq)\nZ = np.array(D_q).T\n\nplot_2d(X, Y, Z, \n        subtitle_jpg='contour_D(q)', \n        subtitle_fig=fr\"Теплова діаграма $D(q)$\", \n        ylabel=r\"$q$\", \n        barlabel=r\"$D(q)$\",\n        cmap='magma',\n        lims=nq)\n\n\n\n\n\n\n\nРис. 7.42: Двовимірна контурна діаграма динаміки узагальненої фрактальної розмірності \\(D(q)\\), що змінюється з плином часу\n\n\n\n\n\n\nX, Y = np.meshgrid(np.arange(window, length, tstep), nq)\nZ = np.array(D_q).T\n\nplot_3d(X, Y, Z, \n        subtitle_jpg='3d_D(q)', \n        ylabel=r\"$q$\", \n        zlabel=r\"$D(q)$\",\n        cmap='magma')\n\n\n\n\n\n\n\nРис. 7.43: Тривимірна діаграма динаміки узагальненої фрактальної розмірності \\(D(q)\\), що змінюється з плином часу\n\n\n\n\n\nДво- та тривимірні представлення узагальненої фрактальної розмірності показують, що \\(D(q)\\) зростає під час кризових подій. Узагальнена фрактальна розмірність також представляє найбільш індикативну динаміку для негативних значень \\(q\\), хоча для позитивних \\(q\\) також спостерігаються незначні коливання.\n\n\n7.2.14.4 Динаміка \\(C(q)\\) в дво- та тривимірному просторах\n\nX, Y = np.meshgrid(time_ser.index[window:length:tstep], nq)\nZ = np.array(C_q).T\n\nplot_2d(X, Y, Z, \n        subtitle_jpg='contour_C(q)', \n        subtitle_fig=fr\"Теплова діаграма $C(q)$\", \n        ylabel=r\"$q$\", \n        barlabel=r\"$C(q)$\",\n        cmap='hot',\n        lims=nq)\n\n\n\n\n\n\n\nРис. 7.44: Двовимірна контурна діаграма динаміки мультифрактальної теплоємності \\(C(q)\\), що змінюється з плином часу\n\n\n\n\n\n\nX, Y = np.meshgrid(np.arange(window, length, tstep), nq)\nZ = np.array(C_q).T\n\nplot_3d(X, Y, Z, \n        subtitle_jpg='3d_C(q)', \n        ylabel=r\"$q$\", \n        zlabel=r\"$C(q)$\",\n        cmap='hot')\n\n\n\n\n\n\n\nРис. 7.45: Тривимірна контурна діаграма динаміки мультифрактальної теплоємності \\(C(q)\\), що змінюється з плином часу\n\n\n\n\n\nНа даних рисунках (Рис. 7.44 та Рис. 7.45) під час кризових подій спостерігаються стрибки мультифрактальної теплоємності, що вказує аналогії фізичних фазових переходів та кризових подій. Можна бачити, що при різних ринкових режимах \\(C(q)\\) може бути симетричною, демонструючи рівномірний вплив на динаміку ринку як висококонцентрованих елементів, так і низькоконцентрованих. Також \\(C(q)\\) може зміщуватись як у ліву сторону, так і вправу, що говорить про мінливість ринку та впливовість різних початкових умов на його структуризацію.\n\n\n7.2.14.5 Динаміка \\(f(\\alpha)\\) з ходом часу в дво- та тривимірному просторах\n\nX = time_ser.index[window:length:tstep].values\nX = np.expand_dims(X, axis=1)\nX = np.repeat(a=X, repeats=nq.shape[0], axis=1)\n\nY = np.array(alpha)\nZ = np.array(mfSpect)\n\nplot_2d(X, Y, Z, \n        subtitle_jpg='contour_f(alpha)', \n        subtitle_fig=fr\"Теплова діаграма $f(\\alpha)$\", \n        ylabel=r\"$\\alpha$\", \n        barlabel=r\"$f(\\alpha)$\",\n        cmap='hsv',\n        lims=alpha)\n\n\n\n\n\n\n\nРис. 7.46: Двовимірна контурна діаграма динаміки мультифрактального спектра \\(f(\\alpha)\\), що змінюється з плином часу\n\n\n\n\n\n\nX = np.arange(window, length, tstep)\nX = np.expand_dims(X, axis=1)\nX = np.repeat(a=X, repeats=nq.shape[0], axis=1)\n\nY = np.array(alpha)\nZ = np.array(mfSpect)\n\nplot_3d(X, Y, Z, \n        subtitle_jpg='3d_f(alpha)', \n        ylabel=r\"$\\alpha$\", \n        zlabel=r\"$f(\\alpha)$\",\n        cmap='hsv')\n\n\n\n\n\n\n\nРис. 7.47: Тривимірна діаграма динаміки мультифрактального спектра \\(f(\\alpha)\\), що змінюється з плином часу\n\n\n\n\n\nЯк видно з останніх рисунків (Рис. 7.46 та Рис. 7.47), ширина спектра мультифрактальності змінюється у формі з плином часу, і стає ширшою під час кризових подій, що підтверджувалось таким індикатором як, наприклад, \\(\\Delta\\alpha\\). Видно, що у передкризові періоди зростає лівостороння асиметрія, що характеризує флуктуації значної амплітуди коливань. Самі кризи представляють зміщення \\(f(\\alpha)\\) у праву сторону, що вказує на домінацію флуктуацій з малою амплітудою. У будь-якому разі, зростання ширини спектра є індикатором зростання ступеня самоорганізованості елементів, що залучені до досліджуваної системи. Тобто, як \\(f(\\alpha)\\), так і попередні індикатори можна рекомендувати в якості індикаторів або індикаторів-передвісників кризових подій. У подальших лабораторних роботах цікаво буде розглянути різновиди MF-DFA, що, наприклад, враховують мультифрактальні крос-кореляції  [34].",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Лабораторна робота № 7</span>"
    ]
  },
  {
    "objectID": "lab_7.html#завдання-для-виконання",
    "href": "lab_7.html#завдання-для-виконання",
    "title": "7  Лабораторна робота № 7",
    "section": "7.3 Завдання для виконання",
    "text": "7.3 Завдання для виконання\n\nВибрати із запропонованої бази даних варіант завдання\nПровести повний аналіз ряду за допомогою методу MF-DFA\nДати інтерпретацію отриманим результатам",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Лабораторна робота № 7</span>"
    ]
  },
  {
    "objectID": "lab_7.html#контрольні-запитання",
    "href": "lab_7.html#контрольні-запитання",
    "title": "7  Лабораторна робота № 7",
    "section": "7.4 Контрольні запитання",
    "text": "7.4 Контрольні запитання\n\nВ чому переваги мультифрактальних характеристик у порівнянні з монофрактальними?\nНа що вказує мультифрактальність часового ряду?\nЯким чином поводять себе різні характеристики, якщо ряд містить кризу?\n\n\n\n\n\n[1] T. C. Halsey, M. H. Jensen, L. P. Kadanoff, I. Procaccia, and B. I. Shraiman, Fractal Measures and Their Singularities: The Characterization of Strange Sets, Phys. Rev. A 33, 1141 (1986).\n\n\n[2] U. Frisch and G. Parisi, Turbulence and Predictability of Geophysical Flows and Climate Dynamics, in Proceedings of the International School of Physics“enrico Fermi,\" Course LXXXVIII, Varenna, 1983 (North-Holland, New York, 1985).\n\n\n[3] P. Grassberger, Generalized Dimensions of Strange Attractors, Physics Letters A 97, 227 (1983).\n\n\n[4] T. C. Halsey, M. H. Jensen, L. P. Kadanoff, I. Procaccia, and B. I. Shraiman, Fractal Measures and Their Singularities: The Characterization of Strange Sets, Nuclear Physics B - Proceedings Supplements 2, 501 (1987).\n\n\n[5] J. W. Kantelhardt, E. Koscielny-Bunde, H. H. A. Rego, S. Havlin, and A. Bunde, Detecting Long-Range Correlations with Detrended Fluctuation Analysis, Physica A: Statistical Mechanics and Its Applications 295, 441 (2001).\n\n\n[6] J. W. Kantelhardt, Fractal and Multifractal Time Series, in Mathematics of Complexity and Dynamical Systems, edited by R. A. Meyers (Springer New York, New York, NY, 2011), pp. 463–487.\n\n\n[7] J. W. Kantelhardt, S. A. Zschiegner, E. Koscielny-Bunde, S. Havlin, A. Bunde, and H. E. Stanley, Multifractal Detrended Fluctuation Analysis of Nonstationary Time Series, Physica A: Statistical Mechanics and Its Applications 316, 87 (2002).\n\n\n[8] S. Dutta, Multifractal Properties of ECG Patterns of Patients Suffering from Congestive Heart Failure, Journal of Statistical Mechanics: Theory and Experiment 2010, P12021 (2010).\n\n\n[9] E. Maiorino, L. Livi, A. Giuliani, A. Sadeghian, and A. Rizzi, Multifractal Characterization of Protein Contact Networks, Physica A: Statistical Mechanics and Its Applications 428, 302 (2015).\n\n\n[10] P. H. Figueirêdo, E. Nogueira, M. A. Moret, and S. Coutinho, Multifractal Analysis of Polyalanines Time Series, Physica A: Statistical Mechanics and Its Applications 389, 2090 (2010).\n\n\n[11] G. R. Jafari, P. Pedram, and L. Hedayatifar, Erratum: Long-Range Correlation and Multifractality in Bach’s Inventions Pitches, Journal of Statistical Mechanics: Theory and Experiment 2012, E03001 (2012).\n\n\n[12] Z.-Q. Jiang, W.-J. Xie, W.-X. Zhou, and D. Sornette, Multifractal Analysis of Financial Markets: A Review, Reports on Progress in Physics 82, 125901 (2019).\n\n\n[13] L. Telesca, V. Lapenna, and M. Macchiato, Multifractal Fluctuations in Earthquake-Related Geoelectrical Signals, New Journal of Physics 7, 214 (2005).\n\n\n[14] E. G. Yee Leung and Z. Yu, Temporal Scaling Behavior of Avian Influenza a (H5N1): The Multifractal Detrended Fluctuation Analysis, Annals of the Association of American Geographers 101, 1221 (2011).\n\n\n[15] F. Liao and Y.-K. Jan, Using Multifractal Detrended Fluctuation Analysis to Assess Sacral Skin Blood Flow Oscillations in People with Spinal Cord Injury, The Journal of Rehabilitation Research and Development 48, 787 (2011).\n\n\n[16] L. Telesca, V. Lapenna, and M. Macchiato, Multifractal Fluctuations in Seismic Interspike Series, Physica A: Statistical Mechanics and Its Applications 354, 629 (2005).\n\n\n[17] M. S. Movahed, F. Ghasemi, S. Rahvar, and M. R. R. Tabar, Long-Range Correlation in Cosmic Microwave Background Radiation, Phys. Rev. E 84, 021103 (2011).\n\n\n[18] P. Mali, S. Sarkar, S. Ghosh, A. Mukhopadhyay, and G. Singh, Multifractal Detrended Fluctuation Analysis of Particle Density Fluctuations in High-Energy Nuclear Collisions, Physica A: Statistical Mechanics and Its Applications 424, 25 (2015).\n\n\n[19] I. T. Pedron, Correlation and Multifractality in Climatological Time Series, Journal of Physics: Conference Series 246, 012034 (2010).\n\n\n[20] R. Rak, S. Drożdż, J. Kwapień, and P. Oświȩcimka, Detrended Cross-Correlations Between Returns, Volatility, Trading Activity, and Volume Traded for the Stock Market Companies, Europhysics Letters 112, 48001 (2015).\n\n\n[21] M. Wątorek, S. Drożdż, J. Kwapień, L. Minati, P. Oświęcimka, and M. Stanuszek, Multiscale Characteristics of the Emerging Global Cryptocurrency Market, Physics Reports 901, 1 (2021).\n\n\n[22] C.-K. Peng, S. Havlin, H. E. Stanley, and A. L. Goldberger, Quantification of Scaling Exponents and Crossover Phenomena in Nonstationary Heartbeat Time Series, Chaos: An Interdisciplinary Journal of Nonlinear Science 5, 82 (1995).\n\n\n[23] E. Canessa, Multifractality in Time Series, Journal of Physics A: Mathematical and General 33, 3637 (2000).\n\n\n[24] A. Kasprzak, R. Kutner, J. Perelló, and J. Masoliver, Higher-Order Phase Transitions on Financial Markets, The European Physical Journal B: Condensed Matter and Complex Systems 76, 513 (2010).\n\n\n[25] M. Dai, C. Zhang, and D. Zhang, Multifractal and Singularity Analysis of Highway Volume Data, Physica A: Statistical Mechanics and Its Applications 407, 332 (2014).\n\n\n[26] M. Dai, J. Hou, and D. Ye, Multifractal Detrended Fluctuation Analysis Based on Fractal Fitting: The Long-Range Correlation Detection Method for Highway Volume Data, Physica A: Statistical Mechanics and Its Applications 444, 722 (2016).\n\n\n[27] X. Sun, H. Chen, Z. Wu, and Y. Yuan, Multifractal Analysis of Hang Seng Index in Hong Kong Stock Market, Physica A: Statistical Mechanics and Its Applications 291, 553 (2001).\n\n\n[28] E. A. Ihlen, Introduction to Multifractal Detrended Fluctuation Analysis in Matlab, Frontiers in Physiology 3, (2012).\n\n\n[29] P. Oświȩcimka, L. Livi, and S. Drożdż, Right-Side-Stretched Multifractal Spectra Indicate Small-Worldness in Networks, Communications in Nonlinear Science and Numerical Simulation 57, 231 (2018).\n\n\n[30] S. Drożdż and P. Oświȩcimka, Detecting and Interpreting Distortions in Hierarchical Organization of Complex Time Series, Phys. Rev. E 91, 030902 (2015).\n\n\n[31] S. Drożdż, R. Kowalski, P. Oświȩcimka, R. Rak, and R. Gȩbarowski, Dynamical Variety of Shapes in Financial Multifractality, Complexity 2018, 1 (2018).\n\n\n[32] A. Orozco-Duque, D. Novak, V. Kremen, and J. Bustamante, Multifractal Analysis for Grading Complex Fractionated Electrograms in Atrial Fibrillation, Physiological Measurement 36, 2269 (2015).\n\n\n[33] A. Faini, G. Parati, and P. Castiglioni, Multiscale Assessment of the Degree of Multifractality for Physiological Time Series, Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences 379, 20200254 (2021).\n\n\n[34] A. Bielinskyi, V. Soloviev, V. Solovieva, A. Matviychuk, and S. Semerikov, The Analysis of Multifractal Cross-Correlation Connectedness Between Bitcoin and the Stock Market, in Information Technology for Education, Science, and Technics, edited by E. Faure, O. Danchenko, M. Bondarenko, Y. Tryus, C. Bazilo, and G. Zaspa (Springer Nature Switzerland, Cham, 2023), pp. 323–345.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Лабораторна робота № 7</span>"
    ]
  },
  {
    "objectID": "lab_8.html",
    "href": "lab_8.html",
    "title": "8  Лабораторна робота № 8",
    "section": "",
    "text": "8.1 Теоретичні відомості\nВивчення статистичних властивостей матриць з незалежними випадковими елементами — випадкових матриць — має багату історію, що починається з ядерної фізики  [1–7], де проблема з’явилася більше 50 років тому при дослідженні енергетичних рівнів складних ядер. Теорія випадкової матриці (ТВМ) була розвинена в цьому контексті Вігнером (Wigner), Дайсоном (Dyson), Метою (Mehta) та іншими для пояснення статистики рівнів енергії складних квантових систем. Дослідники постулювали, що функція Гамільтона, яка описує важкі ядра, може бути задана матрицею \\(H\\) з незалежними випадковими елементами \\(H_{ij}\\), отриманими з розподілу імовірності. Відштовхуючись від цього припущення було зроблено низку вражаючих передбачень, які було підтверджено експериментально. Для складних квантових систем результати на основі ТВМ представляють середнє за всіма можливими взаємодіями. Відхилення від універсальних передбачень ТВМ відображують системну специфіку, невипадкові властивості системи, забезпечуючи ключові підходи до розуміння базової взаємодії системи. Недавні дослідження, що використовували методи аналізу ТВМ до аналізу властивостей матриці взаємних кореляцій \\(C\\) для економічних систем, показують, що близько 98% власних значень матриці \\(C\\) співпадають зі значеннями, отримуваними з використанням ТВМ, таким чином пропонуючи задовільний рівень хаотичності у вимірюваних крос-кореляціях. Також було знайдено, що існують відхилення від передбачень за допомогою ТВМ у близько 2% найбільших власних значень. Ці результати викликають наступні питання:\nШляхом комп’ютерного моделювання виявлено, що найбільше власне значення матриці \\(C\\) представляє вплив ринку в цілому. Аналіз змісту власних значень, що відхиляються від ТВМ, показує існування взаємних кореляцій між акціями того ж самого типу діяльності, найбільш капіталізованими акціями, і акціями фірм, що мають бізнес у певному географічному секторі (локалізовані територіально). Обчислюючи скалярний добуток власних векторів від одного періоду часу до наступного, можна побачити, що власні вектори, що відхиляються, мають різні ступені стабільності в часі, визначеному кількісно величиною скалярного добутку. Найбільші два-три власних вектори стійкі протягом тривалих періодів часу, у той час як для іншої частини власних векторів, що відхиляються, стабільність у часі зменшується як тільки відповідні власні значення наближаються до верхньої межі ТВМ.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Лабораторна робота № 8</span>"
    ]
  },
  {
    "objectID": "lab_8.html#теоретичні-відомості",
    "href": "lab_8.html#теоретичні-відомості",
    "title": "8  Лабораторна робота № 8",
    "section": "",
    "text": "Яка можлива інтерпретація для відхилень від ТВМ?\nЩо можна сказати про структуру C з цих результатів?\nЯке практичне значення отриманих результатів?\n\n\n\n8.1.1 Знаходження коефіцієнтів матриці крос-кореляцій\nВизначення кореляцій між різними акціями — тема, цікава тема не лише з точки зору розуміння економіки як складної динамічної системи, але також і прагматично, зокрема, з точки зору розміщення активів і оцінки портфельного ризику. Ми будемо аналізувати взаємні кореляції між акціями, застосовуючи поняття і методи теорії випадкових матриць, що використовуються в контексті складних квантових систем, де точний характер взаємодій між підодиницями невідомий.\nДля визначення кількісно кореляцій спочатку обчислюється зміна цін (прибутковості) акції \\(i=1,...,N\\) за час \\(\\Delta t\\),\n\\[\nG_{i}(t) = \\ln S_i(t+\\Delta t) - \\ln S_i(t),\n\\tag{8.1}\\]\nде \\(S_i(t)\\) позначає ціну акції \\(i\\). Оскільки різні ціни мають різні рівні змінюванності (стандартні відхилення), визначатимемо стандартизовану прибутковість\n\\[\ng_i(t) \\equiv \\left[ G_i(t) - \\left\\langle G_i \\right\\rangle \\right] \\big/ \\sigma_i,\n\\tag{8.2}\\]\nде \\(\\sigma_i \\equiv \\sqrt{\\left\\langle G_{i}^{2} \\right\\rangle - \\left\\langle G_i \\right\\rangle^{2}}\\) — стандартне відхилення \\(G_i\\), а \\(\\left\\langle...\\right\\rangle\\) позначає середнє значення за досліджуваний період часу. Тоді знаходження матриці кореляцій \\(C\\) зводиться до обчислення формули:\n\\[\nC_{ij} \\equiv \\left\\langle g_i(t)g_j(t) \\right\\rangle.\n\\tag{8.3}\\]\nЗа побудовою, елементи \\(C_{ij}\\) обмежені областю \\(−1 \\leq C_{ij} \\leq 1\\), де \\(C_{ij} = 1\\) відповідає повним кореляціям, \\(C_{ij} = -1\\) — повним антикореляціям, і \\(C_{ij} = 0\\) свідчить про некорельованність пар акцій.\nТруднощі в аналізі важливості та значення коефіцієнтів крос-кореляції \\(C_{ij}\\) виникають внаслідок кількох причин, що полягають в наступному:\n\nринкові умови з часом змінюються і взаємна кореляція, що існує між будь-якою парою акцій, може бути не постійною (нестаціонарною);\nскінчена довжина досліджуваного ряду, доступного для оцінювання взаємних кореляцій, додає так званий “шум вимірювання” — чим коротший досліджуваний ряд — тим менш точними будуть отримувані значення.\n\nЯким же чином можна виділяти з \\(C_{ij}\\) ті акції, що залишилися корельованими на розглядуваному періоді часу? Щоб відповісти на це питання, перевіримо статистику \\(C\\) у порівнянні із так званою “нульовою гіпотезою” випадкової кореляційної матриці — матриці кореляцій, побудованої із взаємно некорельованих часових рядів. Якщо властивості \\(C\\) відповідають властивостям для випадкової матриці кореляцій, тоді можна говорити про те, що значення емпірично вимірюваних властивостей \\(C\\) випадкові. Навпаки, відхилення властивостей \\(C\\) від таких же властивостей для випадкової кореляційної матриці передає інформацію про “справжні” кореляції. Отже, нашою метою є порівняння властивостей \\(C\\) з такими ж властивостями випадкової матриці кореляцій і розділ властивостей \\(C\\) на дві групи: (a) частина \\(C\\), що відповідає властивостям випадкової кореляційної матриці (“шум”) і (b) частина \\(C\\), що відхиляється (“інформація”).\n\n\n8.1.2 Розподіл власних значень\nДля отримання інформації про взаємні кореляції \\(C\\) необхідно порівняти властивості \\(C\\) з такими ж властивостями випадкової матриці крос-кореляцій  [8]. У матричній нотації така матриця може бути виражена як\n\\[\nC = \\frac{1}{L} GG^{T},\n\\tag{8.4}\\]\nде \\(G\\) — матриця розміру \\(N \\times L\\) з елементами \\(g_{im}=g_i(m\\Delta t), i=1,...,N; m=0,...,L-1\\) і \\(G^{T}\\) позначає транспонування \\(G\\). Розглянемо випадкову кореляційну матрицю\n\\[\nR = \\frac{1}{L} AA^{T},\n\\tag{8.5}\\]\nде \\(A\\) — матриця розміру \\(N \\times L\\), що містить \\(N\\) часових рядів із \\(L\\) випадковими елементів \\(a_{im}\\) з нульовим середнім і одиничним відхиленням, що означають взаємну некорельованість. За побудовою \\(R\\) належить до типу матриць, які часто називають матрицями Вішарта у багатовимірній статистиці  [9].\nСтатистичні властивості випадкових матриць \\(R\\) відомі  [10,11]. Зокрема, у наближенні \\(N \\to \\infty\\), \\(L \\to \\infty\\), такому, що \\(Q \\equiv L/N(&gt;1)\\) фіксоване, показано аналітично  [11], що функція розподілу щільності імовірності \\(P_{rm}(\\lambda)\\) власних значень \\(\\lambda\\) випадкової матриці кореляції \\(R\\) визначається як\n\\[\nP_{rm}(\\lambda) = \\frac{Q}{2\\pi}\\frac{\\sqrt{(\\lambda_{+} - \\lambda)(\\lambda - \\lambda_{-})}}{\\lambda},\n\\tag{8.6}\\]\nіз \\(\\lambda\\) в межах границь \\(\\lambda_{-} \\leq \\lambda_{i} \\leq \\lambda_{+}\\), де \\(\\lambda_{-}\\) і \\(\\lambda_{+}\\) — найменше та найбільше власні значення \\(R\\), які можна визначити аналітично як\n\\[\n\\lambda_{\\pm} = 1 + 1/Q \\pm 2\\sqrt{1/Q}.\n\\tag{8.7}\\]\nВираз (8.6) є точним для випадку розподілених за Гаусом матричних елементів \\(a_{im}\\).\nПорівняємо розподіл власних значень \\(P(\\lambda)\\) для \\(C\\) з \\(P_{rm}(\\lambda)\\). Для цього обчислимо власні значення \\(\\lambda_i\\) матриці \\(C\\), причому \\(\\lambda_i\\) впорядкуємо за зростанням (\\(\\lambda_{i+1} &gt; \\lambda_{i}\\)). При дослідженнях зверніть увагу на присутність чіткої “великої частини” власних значень, що спадають у межах границь \\([\\lambda_{-}, \\lambda_{+}]\\) для \\(P_{rm}(\\lambda)\\). Також зверніть увагу на відхилення для деяких найбільших і найменших власних значень отриманих за допомогою ТВМ.\nОскільки (8.6) є таким, що строго відповідає лише для \\(L \\to \\infty\\) і \\(N \\to \\infty\\), необхідно перевірити також відхилення від ідеального випадку, оскільки робота проводиться завжди зі скінченими рядами. При дослідженнях виявляється, що для кількох найбільших (найменших) власних значень ефект впливу скінчених величин \\(L\\) і \\(N\\) відсутній.\n\n\n8.1.3 Розподіл власних векторів\nВідхилення \\(P(\\lambda)\\) від передбачення ТВМ \\(P_{rm}(\\lambda)\\) свідчить про те, що ці відхилення також повинні відображатися в статистиці відповідних компонент власного вектора  [8]. Відповідно, у даній лабораторній ми будемо аналізувати розподіл компоненти власного вектора. Розподіл компонент \\(\\left\\{ u_{l}^{k}; l=1,...,N \\right\\}\\) власного вектора \\(u^k\\) випадкової кореляційної матриці \\(R\\) має відповідати розподілу Гауса з нульовим середнім та одиничною дисперсією:\n\\[\n\\rho_{rm}(u) = \\frac{1}{\\sqrt{2\\pi}}\\exp\\left( -u^{2} \\big/ 2 \\right).\n\\]\n\n\n8.1.4 Обернене відношення участі\nВивчивши інтерпретацію найбільшого власного значення, що значно відхиляється від результатів ТВМ, зосередимось на власних значеннях, що залишаються. Відхилення розподілу компонентів власного вектора \\(u_k\\) від ТВМ Гаусового передбачення більш явне, коли відстань від верхньої границі ТВМ \\(\\lambda_k - \\lambda_{+}\\) збільшується. Оскільки близькість до \\(\\lambda_{+}\\) збільшує ефекти хаотичності, визначаємо кількість компонентів, що беруть значну участь у кожному власному векторі, що, у свою чергу, відображає ступінь відхилення від ТВМ для розподілу компонентів власного вектора. Для цього використовується поняття оберненого відношення участі (ОВУ)  [12–14], що часто застосовується в теорії локалізації. ОВУ власного вектора \\(u_k\\) визначається як\n\\[\nI^{k} \\equiv \\sum_{l=1}^{N}\\left[ u_{l}^{k} \\right]^4,\n\\tag{8.8}\\]\nде \\(u_{l}^{k}\\), \\(l=1,...,N\\) — компоненти власного вектора \\(u^{k}\\). Значення \\(I^{k}\\) може бути проілюстровано двома граничними випадками:\n\nвектор з ідентичними компонентами \\(u_{l}^{k} \\equiv 1 \\big/ \\sqrt{N}\\) має \\(I^{k} = 1 \\big/ N\\);\n\nвектор з одним компонентом \\(u_{1}^{k}=1\\) і нульовими іншими має \\(I^{k}=1\\).\n\nТаким чином, ОВУ визначає кількість даних з числа компонентів власного вектора, що значно впливають на ринок, заданий системою часових рядів. Наявність векторів з великими значеннями \\(I^{k}\\) також виникає в теорії локалізації Андерсона. У контексті теорії локалізації часто знаходять “випадкову смугу матриць”, що містять узагальнені стани з маленьким \\(I^{k}\\) в більшій частині спектра власних значень, тоді як основні стани локалізовані і мають великі \\(I^{k}\\). Виявлення локалізованих станів для маленьких і великих власних значень матриці крос-кореляцій \\(C\\) нагадує про локалізацію Андерсона і припускає, що \\(C\\) може мати випадкову зону матричної структури.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Лабораторна робота № 8</span>"
    ]
  },
  {
    "objectID": "lab_8.html#хід-роботи",
    "href": "lab_8.html#хід-роботи",
    "title": "8  Лабораторна робота № 8",
    "section": "8.2 Хід роботи",
    "text": "8.2 Хід роботи\nІмпортуємо необхідні бібліотеки:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport matplotlib.ticker as mticker\nimport pandas as pd\nimport yfinance as yf\nimport scienceplots\nimport requests\nfrom tqdm import tqdm\n\n%matplotlib inline\n\nВизначаємо стиль рисунків:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    'axes.labelsize': 22,\n    'legend.fontsize': 22, \n    'xtick.labelsize': 22,\n    'ytick.labelsize': 22,\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\nВиконуємо парсинг та фільтрацію заголовків акцій компаній:\n\nheaders = {\n    'authority': 'api.nasdaq.com',\n    'accept': 'application/json, text/plain, */*',\n    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/87.0.4280.141 Safari/537.36',\n    'origin': 'https://www.nasdaq.com',\n    'sec-fetch-site': 'same-site',\n    'sec-fetch-mode': 'cors',\n    'sec-fetch-dest': 'empty',\n    'referer': 'https://www.nasdaq.com/',\n    'accept-language': 'en-US,en;q=0.9',\n}\n\nparams = (\n    ('tableonly', 'true'),\n    ('limit', '25'),\n    ('offset', '0'),\n    ('download', 'true'),\n)\n\nr = requests.get('https://api.nasdaq.com/api/screener/stocks', headers=headers, params=params)\ndata = r.json()['data']\ndf = pd.DataFrame(data['rows'], columns=data['headers'])\ndf = df.dropna(subset={'marketCap'})\ndf = df[~df['symbol'].str.contains(\"\\/|\\.|\\^\")]\ndf.head()\n\n\n\n\n\n\n\n\n\nsymbol\nname\nlastsale\nnetchange\npctchange\nmarketCap\ncountry\nipoyear\nvolume\nsector\nindustry\nurl\n\n\n\n\n0\nA\nAgilent Technologies Inc. Common Stock\n$137.01\n4.46\n3.365%\n40149370256.00\nUnited States\n1999\n2536839\nIndustrials\nBiotechnology: Laboratory Analytical Instruments\n/market-activity/stocks/a\n\n\n1\nAA\nAlcoa Corporation Common Stock\n$26.32\n-0.38\n-1.423%\n4725992617.00\nUnited States\n2016\n3747970\nIndustrials\nAluminum\n/market-activity/stocks/aa\n\n\n2\nAACG\nATA Creativity Global American Depositary Shares\n$1.24\n0.005\n0.405%\n39211491.00\nChina\n2008\n8363\nReal Estate\nOther Consumer Services\n/market-activity/stocks/aacg\n\n\n3\nAACI\nArmada Acquisition Corp. I Common Stock\n$10.99\n0.00\n0.00%\n0.00\nUnited States\n2021\n5\nFinance\nBlank Checks\n/market-activity/stocks/aaci\n\n\n4\nAACIW\nArmada Acquisition Corp. I Warrant\n$0.03\n0.00\n0.00%\n0.00\nUnited States\n2021\n102\nFinance\nBlank Checks\n/market-activity/stocks/aaciw\n\n\n\n\n\n\n\n\nФільтруємо та сортуємо заголовків акцій за їх капіталізацією:\n\ndef cust_filter(mkt_cap):\n    if 'M' in mkt_cap:\n        return float(mkt_cap[1:-1])\n    elif 'B' in mkt_cap:\n        return float(mkt_cap[1:-1]) * 1000\n    elif mkt_cap == '':\n        return 0.0\n    else:\n        return float(mkt_cap[1:]) / 1e6\n    \ndf['marketCap'] = df['marketCap'].apply(cust_filter)\ndf = df.sort_values('marketCap', ascending=False)\ndf.head()\n\n\n\n\n\n\n\n\n\nsymbol\nname\nlastsale\nnetchange\npctchange\nmarketCap\ncountry\nipoyear\nvolume\nsector\nindustry\nurl\n\n\n\n\n4733\nNVDA\nNVIDIA Corporation Common Stock\n$776.63\n-10.38\n-1.319%\n941575.00000\nUnited States\n1999\n39191401\nTechnology\nSemiconductors\n/market-activity/stocks/nvda\n\n\n16\nAAPL\nApple Inc. Common Stock\n$181.42\n-1.21\n-0.663%\n801466.05102\nUnited States\n1980\n48892881\nTechnology\nComputer Manufacturing\n/market-activity/stocks/aapl\n\n\n401\nAMZN\nAmazon.com Inc. Common Stock\n$173.16\n-0.38\n-0.219%\n798678.94435\nUnited States\n1997\n28092221\nConsumer Discretionary\nCatalog/Specialty Distribution\n/market-activity/stocks/amzn\n\n\n2945\nGOOG\nAlphabet Inc. Class C Capital Stock\n$137.43\n-2.67\n-1.906%\n708667.19000\nUnited States\n2004\n30590285\nTechnology\nComputer Software: Programming Data Processing\n/market-activity/stocks/goog\n\n\n2946\nGOOGL\nAlphabet Inc. Class A Common Stock\n$136.38\n-2.50\n-1.80%\n695612.54000\nUnited States\n2004\n37275794\nTechnology\nComputer Software: Programming Data Processing\n/market-activity/stocks/googl\n\n\n\n\n\n\n\n\nВизначаємо найпередовіші акцій за їх капіталізацією:\n\ntop = 200\ntickers_list = df.iloc[:top]['symbol'].tolist()\ntickers_list[:10]\n\n['NVDA',\n 'AAPL',\n 'AMZN',\n 'GOOG',\n 'GOOGL',\n 'META',\n 'WFC',\n 'CSCO',\n 'TMUS',\n 'BABA']\n\n\nЗчитуємо дані з Yahoo Finance згідно створенного списку акцій:\n\nstart = \"2001-12-31\"\nend = \"2024-02-29\"\ndata_init = yf.download(tickers_list, start, end)[\"Adj Close\"]\n\nxlabel = 'time, days'    # підпис по вісі Ох \n\nДалі нам потребується обрати фінансовий індекс для порівняння з розрахованими індикаторами. Далі надається список усіх доступних для нас акцій:\n\nylabel = 'AAPL'             # підпис по вісі Оу           \n\ndata_init.columns.values\n\narray(['AAPL', 'ACN', 'ADBE', 'AEE', 'AIG', 'AMAT', 'AMD', 'AMGN', 'AMX',\n       'AMZN', 'ANSS', 'ASML', 'AXP', 'BA', 'BAC', 'BALL', 'BDX', 'BHP',\n       'BKNG', 'BKR', 'BLK', 'BP', 'CAT', 'CHKP', 'CLX', 'CMCSA', 'CME',\n       'CMI', 'COO', 'COP', 'COST', 'CP', 'CSCO', 'CTRA', 'CTSH', 'CUK',\n       'CVX', 'DD', 'DHI', 'DHR', 'DUK', 'ED', 'ELV', 'ENTG', 'EPD',\n       'ERIE', 'ETN', 'F', 'FIS', 'FMX', 'GE', 'GS', 'HBAN', 'HD', 'HDB',\n       'HIG', 'HON', 'HPQ', 'HSBC', 'HUBB', 'IBM', 'IBN', 'IFF', 'INTC',\n       'INTU', 'ISRG', 'ITUB', 'ITW', 'J', 'JNJ', 'JPM', 'K', 'KO', 'LIN',\n       'LLY', 'LOW', 'LRCX', 'LYG', 'MCD', 'MCK', 'MCO', 'MDLZ', 'MDT',\n       'MKL', 'MRK', 'MRVL', 'MS', 'MSFT', 'MU', 'MUFG', 'NEE', 'NFLX',\n       'NKE', 'NOC', 'NVDA', 'NVO', 'NVS', 'PBR', 'PEP', 'PFE', 'PFG',\n       'PG', 'PGR', 'PLD', 'PPL', 'PRU', 'PSA', 'QCOM', 'REGN', 'RS',\n       'RSG', 'RTO', 'RTX', 'RY', 'RYAAY', 'SAP', 'SCHW', 'SLB', 'SNY',\n       'SPG', 'SPGI', 'STX', 'SYK', 'T', 'TECK', 'TGT', 'TJX', 'TM',\n       'TMO', 'TSM', 'TSN', 'TTE', 'TXN', 'TYL', 'UBS', 'UL', 'UMC',\n       'UNH', 'UNP', 'UPS', 'VIV', 'VRSN', 'VRTX', 'VZ', 'WAT', 'WDC',\n       'WFC', 'WMT', 'XOM', 'YUM'], dtype=object)\n\n\n\nperc = 5.0 \nmin_count =  int(((100-perc)/100)*data_init.shape[0] + 1)\ndata_init = data_init.dropna(axis=1, thresh=min_count)\ndata_init\n\n\n\n\n\n\n\n\nTicker\nAAPL\nACN\nADBE\nAEE\nAIG\nAMAT\nAMD\nAMGN\nAMX\nAMZN\n...\nVIV\nVRSN\nVRTX\nVZ\nWAT\nWDC\nWFC\nWMT\nXOM\nYUM\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2001-12-31\n0.331080\n19.321152\n15.452659\n16.077808\n979.726562\n14.758854\n15.860000\n40.573265\n2.202186\n0.541000\n...\n2.570263\n32.301708\n24.590000\n14.652889\n38.750000\n5.033542\n11.653322\n12.636704\n19.206497\n6.171408\n\n\n2002-01-02\n0.352246\n18.811560\n15.845819\n16.248848\n971.706177\n15.336703\n16.389999\n40.544498\n2.196534\n0.548000\n...\n2.618942\n32.318691\n24.540001\n14.970889\n38.750000\n4.929181\n11.645280\n12.746495\n19.353119\n6.019630\n\n\n2002-01-03\n0.356478\n18.223026\n16.462933\n16.188034\n969.609009\n16.742659\n19.370001\n39.063633\n2.263233\n0.595000\n...\n2.700724\n31.707302\n24.170000\n15.467963\n37.990002\n4.929181\n11.688172\n12.731126\n19.382442\n6.146321\n\n\n2002-01-04\n0.358142\n19.880970\n17.866358\n16.077808\n959.983887\n16.643278\n20.000000\n39.983784\n2.266624\n0.612500\n...\n2.706564\n30.832680\n23.950001\n15.625419\n38.889999\n5.178046\n11.717662\n12.647694\n19.548605\n6.384649\n\n\n2002-01-07\n0.346199\n18.983822\n17.995750\n16.146219\n947.644775\n16.466614\n19.980000\n40.034107\n2.254189\n0.617000\n...\n2.706564\n28.913593\n23.790001\n15.529710\n38.740002\n5.507194\n11.736426\n12.601573\n19.377556\n6.503810\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2024-02-22\n184.369995\n371.309998\n537.570007\n70.809998\n70.879997\n199.729996\n181.860001\n285.179993\n18.959999\n174.580002\n...\n11.260000\n194.270004\n426.779999\n40.730000\n330.549988\n55.700001\n53.369999\n58.470001\n104.760002\n137.330002\n\n\n2024-02-23\n182.520004\n377.320007\n553.440002\n71.599998\n71.320000\n197.160004\n176.520004\n289.179993\n19.240000\n174.990005\n...\n11.050000\n196.559998\n430.109985\n40.660000\n331.250000\n56.060001\n53.860001\n58.520000\n103.839996\n138.320007\n\n\n2024-02-26\n181.160004\n377.480011\n560.479980\n70.830002\n71.320000\n203.550003\n176.009995\n286.369995\n19.309999\n174.729996\n...\n10.950000\n193.020004\n433.480011\n39.689999\n327.029999\n57.080002\n54.130001\n59.599998\n104.250000\n138.059998\n\n\n2024-02-27\n182.630005\n377.910004\n552.489990\n71.320000\n71.949997\n202.860001\n178.000000\n278.489990\n19.330000\n173.539993\n...\n11.350000\n193.559998\n430.920013\n39.930000\n332.649994\n57.480000\n54.810001\n59.590000\n104.029999\n137.039993\n\n\n2024-02-28\n181.419998\n378.029999\n551.820007\n71.489998\n72.430000\n197.539993\n176.539993\n277.459991\n18.920000\n173.160004\n...\n11.200000\n194.410004\n426.970001\n40.099998\n337.679993\n57.459999\n54.720001\n59.619999\n104.320000\n138.600006\n\n\n\n\n5578 rows × 150 columns\n\n\n\n\n\ndata_init = data_init.dropna(axis=0)\ndata_init\n\n\n\n\n\n\n\n\nTicker\nAAPL\nACN\nADBE\nAEE\nAIG\nAMAT\nAMD\nAMGN\nAMX\nAMZN\n...\nVIV\nVRSN\nVRTX\nVZ\nWAT\nWDC\nWFC\nWMT\nXOM\nYUM\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2002-12-11\n0.234175\n12.847271\n12.928527\n16.445744\n762.468323\n10.725008\n7.720000\n33.981182\n1.655536\n1.106500\n...\n1.886119\n7.769732\n16.600000\n12.396795\n24.420000\n5.892537\n12.804420\n11.462061\n17.542360\n5.632037\n\n\n2002-12-12\n0.229640\n12.732435\n12.893639\n16.667768\n743.536743\n10.570430\n7.750000\n36.267204\n1.664619\n1.115000\n...\n1.900076\n7.803698\n16.629999\n12.316601\n24.540001\n5.772116\n12.708458\n11.325406\n17.472225\n5.837752\n\n\n2002-12-13\n0.223593\n12.646309\n12.853766\n16.934195\n730.049072\n9.981552\n7.320000\n36.173748\n1.635097\n1.109000\n...\n1.898082\n7.481021\n16.170000\n12.412822\n20.150000\n5.459026\n12.612490\n11.140245\n17.552378\n5.832731\n\n\n2002-12-16\n0.224500\n12.775500\n13.237535\n16.651621\n758.508423\n10.644040\n7.470000\n36.310326\n1.677110\n1.125500\n...\n2.003752\n7.769732\n16.790001\n12.810550\n20.959999\n5.659726\n12.886674\n11.448839\n17.963118\n5.862836\n\n\n2002-12-17\n0.227977\n12.976462\n13.377088\n16.732355\n749.476013\n10.202384\n7.340000\n37.108273\n1.665755\n1.123000\n...\n2.029672\n7.888614\n16.950001\n12.560370\n20.950001\n5.563389\n12.837327\n11.228419\n17.792807\n5.820190\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2024-02-22\n184.369995\n371.309998\n537.570007\n70.809998\n70.879997\n199.729996\n181.860001\n285.179993\n18.959999\n174.580002\n...\n11.260000\n194.270004\n426.779999\n40.730000\n330.549988\n55.700001\n53.369999\n58.470001\n104.760002\n137.330002\n\n\n2024-02-23\n182.520004\n377.320007\n553.440002\n71.599998\n71.320000\n197.160004\n176.520004\n289.179993\n19.240000\n174.990005\n...\n11.050000\n196.559998\n430.109985\n40.660000\n331.250000\n56.060001\n53.860001\n58.520000\n103.839996\n138.320007\n\n\n2024-02-26\n181.160004\n377.480011\n560.479980\n70.830002\n71.320000\n203.550003\n176.009995\n286.369995\n19.309999\n174.729996\n...\n10.950000\n193.020004\n433.480011\n39.689999\n327.029999\n57.080002\n54.130001\n59.599998\n104.250000\n138.059998\n\n\n2024-02-27\n182.630005\n377.910004\n552.489990\n71.320000\n71.949997\n202.860001\n178.000000\n278.489990\n19.330000\n173.539993\n...\n11.350000\n193.559998\n430.920013\n39.930000\n332.649994\n57.480000\n54.810001\n59.590000\n104.029999\n137.039993\n\n\n2024-02-28\n181.419998\n378.029999\n551.820007\n71.489998\n72.430000\n197.539993\n176.539993\n277.459991\n18.920000\n173.160004\n...\n11.200000\n194.410004\n426.970001\n40.099998\n337.679993\n57.459999\n54.720001\n59.619999\n104.320000\n138.600006\n\n\n\n\n5339 rows × 150 columns\n\n\n\n\n\ndata = data_init.T\ndata\n\n\n\n\n\n\n\n\nDate\n2002-12-11\n2002-12-12\n2002-12-13\n2002-12-16\n2002-12-17\n2002-12-18\n2002-12-19\n2002-12-20\n2002-12-23\n2002-12-24\n...\n2024-02-14\n2024-02-15\n2024-02-16\n2024-02-20\n2024-02-21\n2024-02-22\n2024-02-23\n2024-02-26\n2024-02-27\n2024-02-28\n\n\nTicker\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAAPL\n0.234175\n0.229640\n0.223593\n0.224500\n0.227977\n0.220267\n0.214673\n0.213766\n0.219058\n0.217093\n...\n184.149994\n183.860001\n182.309998\n181.559998\n182.320007\n184.369995\n182.520004\n181.160004\n182.630005\n181.419998\n\n\nACN\n12.847271\n12.732435\n12.646309\n12.775500\n12.976462\n12.911866\n12.588892\n12.308977\n12.344866\n12.538650\n...\n370.980011\n371.510010\n369.480011\n362.950012\n360.910004\n371.309998\n377.320007\n377.480011\n377.910004\n378.029999\n\n\nADBE\n12.928527\n12.893639\n12.853766\n13.237535\n13.377088\n12.973382\n12.729166\n12.828846\n13.232551\n13.073062\n...\n604.659973\n590.440002\n546.659973\n541.909973\n538.520020\n537.570007\n553.440002\n560.479980\n552.489990\n551.820007\n\n\nAEE\n16.445744\n16.667768\n16.934195\n16.651621\n16.732355\n16.760612\n16.784834\n16.813091\n16.526482\n16.522438\n...\n68.610001\n69.540001\n69.510002\n69.870003\n71.400002\n70.809998\n71.599998\n70.830002\n71.320000\n71.489998\n\n\nAIG\n762.468323\n743.536743\n730.049072\n758.508423\n749.476013\n730.544006\n718.789185\n730.915588\n735.122437\n733.637695\n...\n69.169998\n70.300003\n70.040001\n68.129997\n68.680000\n70.879997\n71.320000\n71.320000\n71.949997\n72.430000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\nWDC\n5.892537\n5.772116\n5.459026\n5.659726\n5.563389\n5.202130\n5.194100\n5.410857\n5.178046\n5.418886\n...\n56.080002\n56.009998\n54.000000\n54.130001\n53.389999\n55.700001\n56.060001\n57.080002\n57.480000\n57.459999\n\n\nWFC\n12.804420\n12.708458\n12.612490\n12.886674\n12.837327\n12.686520\n12.659102\n12.905866\n12.903131\n12.864743\n...\n48.529999\n52.040001\n51.910000\n51.770000\n52.660000\n53.369999\n53.860001\n54.130001\n54.810001\n54.720001\n\n\nWMT\n11.462061\n11.325406\n11.140245\n11.448839\n11.228419\n11.121353\n11.072788\n11.211862\n10.946960\n10.971246\n...\n56.200001\n56.430000\n56.786667\n58.619999\n57.900002\n58.470001\n58.520000\n59.599998\n59.590000\n59.619999\n\n\nXOM\n17.542360\n17.472225\n17.552378\n17.963118\n17.792807\n17.802835\n17.607470\n17.882977\n17.908020\n17.742720\n...\n100.839996\n103.730003\n103.730003\n102.750000\n104.849998\n104.760002\n103.839996\n104.250000\n104.029999\n104.320000\n\n\nYUM\n5.632037\n5.837752\n5.832731\n5.862836\n5.820190\n5.980748\n5.867856\n6.000816\n5.887925\n5.958169\n...\n133.342682\n134.725769\n133.880005\n133.949997\n135.229996\n137.330002\n138.320007\n138.059998\n137.039993\n138.600006\n\n\n\n\n150 rows × 5339 columns\n\n\n\n\n\n8.2.1 Знаходження коефіцієнтів матриці крос-кореляцій\nПерш за все нам потребується процедура для перетворення ряду до потрібного нам виду. Для цього визначимо функцію transformation(). З її допомогою для всього ряду та у віконній процедурі будемо рахувати прибутковості та всі необхідні індикатори.\n\ndef transformation(signal, ret_type):\n\n    for_rmt = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        for_rmt.values\n    elif ret_type == 2:\n        for_rmt = for_rmt.diff(axis=1).iloc[:,1:]\n    elif ret_type == 3:\n        for_rmt = for_rmt.pct_change(axis=1).iloc[:,1:]\n    elif ret_type == 4:\n        for_rmt = for_rmt.pct_change(axis=1).iloc[:,1:]\n        for_rmt = for_rmt.values\n        for_rmt -= np.mean(for_rmt, axis=1, keepdims=True)\n        for_rmt /= np.std(for_rmt, axis=1, keepdims=True)\n    elif ret_type == 5: \n        for_rmt = for_rmt.pct_change(axis=1).iloc[:,1:]\n        for_rmt = for_rmt.values\n        for_rmt -= np.mean(for_rmt, axis=1, keepdims=True)\n        for_rmt /= np.std(for_rmt, axis=1, keepdims=True)\n        for_rmt = np.abs(for_rmt)\n    elif ret_type == 6:\n        for_rmt = for_rmt.values\n        for_rmt -= np.mean(for_rmt, axis=1, keepdims=True)\n        for_rmt /= np.std(for_rmt, axis=1, keepdims=True)\n\n    return for_rmt\n\n\ntype = 4\n\nlog_ret = transformation(data, type)\n\n\nN, T = log_ret.shape\n\nБудуємо матрицю кореляцій для матриці прибутковостей наших активів:\n\ndef calc_cross_corr(data):\n    C = (1/T)*np.dot(data, data.T)\n    di = np.diag_indices(N)\n    ccoef = np.ma.asarray(C)\n    ccoef[di] = np.ma.masked\n    ccoef_flat = ccoef.compressed()\n\n    return C, ccoef_flat\n\n\nC, ccoef_flat = calc_cross_corr(log_ret)\n\nі матрицю кореляцій для перемішаної матриці прибутковостей:\n\nnp.random.seed(1234)\nrandom_stocks = np.random.normal(size=(N,T))\n\nR, ccoef_flat_rand = calc_cross_corr(random_stocks)\n\nВиводимо результат:\n\nfig = plt.figure(figsize=(12, 8))\ngs = gridspec.GridSpec(2, 2)\n\nax1 = fig.add_subplot(gs[0, 0])\nim1 = ax1.imshow(C, cmap='hot', interpolation='nearest')\nax1.invert_yaxis()\nfig.colorbar(im1, ax=ax1)\n\nax2 = fig.add_subplot(gs[0, 1])\nim2 = ax2.imshow(R, cmap='hot', interpolation='nearest')\nax2.invert_yaxis()\nfig.colorbar(im2, ax=ax2)\n\nax3 = fig.add_subplot(gs[1, :])\nax3.hist(ccoef_flat, bins='auto', density=True, label=r'$P^{init}(C_{ij})$')\nax3.hist(ccoef_flat_rand, bins='auto', density=True, label=r'$P^{rand}(C_{ij})$')\nax3.set_xlabel(r'$C_{ij}$')\nax3.set_ylabel(r'$P(C_{ij})$')\nax3.legend()\n\nfig.align_labels()\nplt.show();\n\n\n\n\n\n\n\nРис. 8.1: Карти полів кореляцій вихідної і перемішаної матриць. У нижній частині рисунку представлено порівняння розподілів значень коефіцієнтів кореляції вихідної та перемішаної матриць\n\n\n\n\n\nНа Рис. 8.1 видно, що розподіл коефіцієнтів кореляції для вихідної матриці значно відхиляється розподілу для випадкової матриці. Можна помітити, що \\(P^{init}(C_{ij})\\) представляється доволі розтягненим в діапазоні \\(C_{ij}\\in [0.1, 0.8]\\), що вказує на те, що достатньо багато досліджуваних активів досить сильно відрізняють один від одного. Можна виокремити активи, які характеризуються досить значним ступенем лінійної залежності по відношенню до більності активів на протязі значного часу існування, а є такі активи, які проявляють лінійну залежність лише в конкретних ринкових умовах. Тим не менш, увесь ринок характеризується досить значним ступенем детермінованості.\n\n\n8.2.2 Розподіл власних значень та векторів\n\ndef calc_lambd_eig(C):\n    return np.linalg.eig(C)\n\n\nlambdas, u = calc_lambd_eig(C)\nlambdas_rand, u_rand = calc_lambd_eig(R)\n\nQ = T/N\n\nlambda_plus = 1 + 1/Q + 2*np.sqrt(1/Q)\nlambda_minus = 1 - 1/Q - 2*np.sqrt(1/Q)\n\nprint(\"Верхня границя розподілу власних значень, що прогнозується ТВМ: \", lambda_plus)\nprint(\"Нижня границя розподілу власних значень, що прогнозується ТВМ: \", lambda_minus)\n\nВерхня границя розподілу власних значень, що прогнозується ТВМ:  1.3633639630325702\nНижня границя розподілу власних значень, що прогнозується ТВМ:  0.6366360369674295\n\n\nГенеруємо 1000 випадкових значень в діапазоні від \\(\\lambda_{-}\\) до \\(\\lambda_{+}\\), і генеруємо \\(P_{rm}(\\lambda)\\).\n\nlambda_random = np.linspace(lambda_minus, lambda_plus, 1000)\nP_rm = (Q/(2*np.pi))*np.sqrt((lambda_plus-lambda_random)*(lambda_random-lambda_minus))/lambda_random\n\nВиводимо результат:\n\nfig, ax = plt.subplots(2, 1, figsize=(13, 10))\nax[0].hist(lambdas, bins=50, density=True, color=\"skyblue\", label='$P^{init}_{\\it{empiric}}$')\nax[0].hist(lambdas_rand, bins='auto', density=True, color=\"red\", label='$P^{rand}_{\\it{empiric}}$')\nax[0].set_xlabel('$\\lambda_{i}$')\nax[0].set_ylabel('$P(\\lambda_{i})$')\nax[0].text(20, 0.5, '$\\lambda_{max}=$'+f'{lambdas.max():.2f}', ha='center', va='center')\nax[0].set_yscale('log')\nax[0].legend()\n\nax[1].hist(lambdas, bins='auto', density=True, color=\"skyblue\", label='$P^{init}_{\\it{empiric}}$')\nax[1].hist(lambdas_rand, bins='auto', density=True, color=\"red\", label='$P^{rand}_{\\it{empiric}}$')\nax[1].set_xlabel('$\\lambda_{i}$')\nax[1].set_ylabel('$P(\\lambda_{i})$')\nax[1].set_xlim(lambda_minus-3*np.std(lambdas_rand), lambda_plus+3*np.std(lambdas_rand))\nax[1].scatter(lambda_random, P_rm, label='$P_{\\it{RMT}}$', color='green')\nax[1].legend()\n\nfig.tight_layout()\nplt.show();\n\n\n\n\n\n\n\nРис. 8.2: Щільність розподілу ймовірностей для матриці прибутковостей фінансових активів \\(P_{empiric}^{init}\\) та випадкової матриці \\(P_{empiric}^{rand}\\). Другий рисунок представляє щільність розподілу, що прогнозується ТВМ \\((P_{RMT})\\)\n\n\n\n\n\nНа Рис. 8.2 видно, що розподіл \\(\\lambda_i\\) значно відхиляється від тієї частки, що відповідає випадковій матриці крос-корреляцій (\\(P^{rand}_{empiric}\\)). Для вихідного розподілу видно, що \\(\\lambda_{max}\\) значно відхиляється від передбачення ТВМ. Для \\(P^{init}_{empiric}\\) знаходяться і такі власні значення, що менші за ті, що знаходяться на нижній границі випадкової матриці. Загалом, така закономірність вказує на те, що на ринку є один або декілька індексів, що регулюють динаміку всього ринку.\nНа Рис. 8.3 представлено щільність розподілу ймовірностей компонент власних векторів вихідної матриці прибутковостей і випадкової матриці при \\(\\lambda_{1}\\), \\(\\lambda_{30}\\) та \\(\\lambda_{70}\\).\n\nfig, ax = plt.subplots(3, 1, figsize=(13, 10), sharex=True, sharey=True)\nax[0].hist(u[:, 0], bins=50, density=True, label=r'$\\rho(u)_{empiric}^{init}$')\nax[0].hist(u_rand[:, 0], bins=50, density=True, alpha=0.7, label=r'$\\rho(u)_{empiric}^{rand}$')\nax[0].set_xlabel('$u_{1}$')\nax[0].set_ylabel('$P(u)$')\nax[0].set_title('Компоненти власного вектора при $ \\lambda_{1} $')\nax[0].legend()\nax[0].set_yscale('log')\n\nax[1].hist(u[:, 30], bins=50, density=True, label=r'$\\rho(u)_{empiric}^{init}$')\nax[1].hist(u_rand[:, 30], bins=50, density=True, alpha=0.7, label=r'$\\rho(u)_{empiric}^{rand}$')\nax[1].set_xlabel('$u_{30}$')\nax[1].set_ylabel('$P(u)$')\nax[1].set_title('Компоненти власного вектора при $ \\lambda_{30} $')\nax[1].legend()\nax[1].set_yscale('log')\n\nax[2].hist(u[:, 70], bins=50, density=True, label=r'$\\rho(u)_{empiric}^{init}$')\nax[2].hist(u_rand[:, 70], bins=50, density=True, alpha=0.7, label=r'$\\rho(u)_{empiric}^{rand}$')\nax[2].set_xlabel('$u_{70}$')\nax[2].set_ylabel('$P(u)$')\nax[2].set_title('Компоненти власного вектора при $ \\lambda_{70} $')\nax[2].legend()\nax[2].set_yscale('log')\n\nfig.tight_layout()\nplt.show();\n\n\n\n\n\n\n\nРис. 8.3: Щільність розподілу ймовірностей компонент власних векторів вихідної матриці прибутковостей і випадкової матриці при \\(\\lambda_1\\), \\(\\lambda_{30}\\) та \\(\\lambda_{70}\\)\n\n\n\n\n\nНа Рис. 8.3 видно, що компоненти власного вектора при найбільшому власному значень значно відхиляються від передбачення ТВМ, що вказує на значний ступінь впливовості індексу, який відповідає \\(\\lambda_{1}\\). При \\(\\lambda_{30}\\) та \\(\\lambda_{70}\\) розподіл компонент власних векторів вихідних значень починає збігатися до розподілу випадкових значень, що говорить про незначний вплив індексів, яким відповідають ці власні значення. Оскільки розподіл компонент їх векторів близький до ТВМ, можна сказати, що вони вносять найбільший шум у динаміку ринку.\nНа Рис. 8.4 представлено ОВУ для вихідної матриці крос-корреляцій та випадкової.\n\n\n8.2.3 Обернене відношення участі\n\ndef calc_ipr(u):\n    return np.sum(u**4, axis=0)\n\n\nIPR = calc_ipr(u)\nIPR_rand = calc_ipr(u_rand)\n\n\nfig, ax = plt.subplots(1, 1)\n\nax.scatter(lambdas, IPR, color='green', label=r'$IPR_{init}$', s=10**2, marker='x')\nax.scatter(lambdas_rand, IPR_rand, color='red', label=r'$IPR_{rand}$', s=10**2, marker='x')\nax.set_xlabel(r'$\\lambda_{i}$')\nax.set_ylabel(r'$IPR$')\nax.set_xscale('log')\nax.legend()\n\nfig.tight_layout()\nplt.show(); \n\n\n\n\n\n\n\nРис. 8.4: ОВУ для вихідної та випадкової матриць крос-корреляцій\n\n\n\n\n\nНа Рис. 8.4 видно, що ОВУ для, наприклад, випадкової матриці концентрується в межах \\(\\lambda_i \\approx 10^0\\). Для даних, що значно відхиляються від випадкових, ОВУ має довгі хвости, які виходять далеко за межі передбачень ТВМ. Для набільшого власного вектора ОВУ локалізується далеко в правому хвості розподілу. Також видно, що деякі власні значення мають \\(IRP \\approx 0.25\\). Це говорить про те, що компоненти деяких векторів розподілені асиметрично, що може вказувати на деяких ступінь впливу фондових індексів, яким властиві дані вектори.\n\n\n8.2.4 Коефіцієнт поглинання\nЦікаво буде буде порівняти максимальне власне значення \\(\\lambda_{max}\\) з середнім значенням коефіцієнта кореляції і так званим коефіцієнтом поглинання (absorption ratio, AR), який є кумулятивною мірою ризику:\n\\[\nAR = \\sum_{k=1}^{n}\\lambda_k \\bigg/ \\sum_{k=1}^{N}\\lambda_k.\n\\tag{8.9}\\]\n\\(AR\\) вказує, яку частину загальної варіації описують \\(n\\) із загальної кількості \\(N\\) власних значень.\nЩоб вирішити, який власний вектор можна відкинути для розрахунків у чисельнику формули вище, не втрачаючи занадто багато інформації, нам потрібно перевірити відповідні власні значення: власні вектори з найменшими власними значеннями є найменш інформативними, тому їх і можна відсікти.\nЗагальний підхід полягає в упорядкуванні власних значень від найвищого до найнижчого.\nПісля сортування власних значень постає питання: “яку кількість найбільш інформативних власних компонент треба вибрати для розрахунку коефіцієнту поглинання?”. Корисною мірою є так звана “врахована (пояснена) дисперсія”, яка може бути обчислена за власними значеннями. Пояснена дисперсія говорить нам, яку частину інформації (дисперсії) можна віднести до кожної із головних компонент.\nВизначимо слідуючу функцію для коефіцієнту поглинання:\n\ndef ar(lambdas, proc):\n    \n    # сортуємо власні значення у спадному порядку\n    sorted_lambdas = np.sort(lambdas)[::-1]\n\n    # розраховуємо кумулятивну варіацію \n    cumulative_variance = np.cumsum(sorted_lambdas) / np.sum(sorted_lambdas)\n\n    # знаходимо індекс, де кумулятивна варіація перетинає \"proc\"\n    index_percent = np.argmax(cumulative_variance &gt;= proc) + 1\n\n    # виділяємо верхні власні значення, які описують встановлений відсоток даних\n    selected_lambdas = sorted_lambdas[:index_percent]\n\n    # повертаємо коефіцієнт поглинання та кількість значень, що пояснюють обраний відсоток варіації\n    return np.sum(selected_lambdas)/np.sum(sorted_lambdas), len(selected_lambdas)\n\nРозглянемо, як варіюється коефіцієнт поглинання при різних значеннях поясненої варіації власними значеннями матриці кореляцій. На Рис. 8.5 представлено залежність коефіцієнта поглинання від різних значень поясненої варіації власними значеннями матриць кореляції вихідних прибутковостей та випадкових. Також представлено залежність кількості поясненої варіації від кількості власних значень, що пояснюють такий відсоток.\n\nproc_variance = np.linspace(0, 0.99, 100)\nar_init = np.array([ar(lambdas, proc)[0] for proc in proc_variance])\nar_rand = np.array([ar(lambdas_rand, proc)[0] for proc in proc_variance])\n\nlambdas_init_cnt = np.array([ar(lambdas, proc)[1] for proc in proc_variance])\nlambdas_rand_cnt = np.array([ar(lambdas_rand, proc)[1] for proc in proc_variance])\n\nfig, ax = plt.subplots(1, 2, figsize=(13, 8))\n\nax[0].plot(proc_variance, ar_init, color='green', label=r'$AR_{init}$')\nax[0].plot(proc_variance, ar_rand, color='red', label=r'$AR_{rand}$')\nax[0].set_xlabel('Відсоток варіації\\n(a)')\nax[0].set_ylabel('$AR$')\nax[0].legend()\n\nax[1].plot(proc_variance, lambdas_init_cnt, color='green', label=r'$\\#\\lambda_{i}^{init}$')\nax[1].plot(proc_variance, lambdas_rand_cnt, color='red', label=r'$\\#\\lambda_{i}^{rand}$')\nax[1].set_xlabel('Відсоток варіації\\n(b)')\nax[1].set_ylabel(r'Кі-ть $\\lambda_i$')\nax[1].legend()\n\nfig.tight_layout()\nplt.show();\n\n\n\n\n\n\n\nРис. 8.5: Залежність коефіцієнта поглинання від різних значень поясненої варіації власними значеннями матриць кореляції вихідних прибутковостей та випадкових (a). Залежність кількості поясненої варіації від кількості власних значень, що пояснюють такий відсоток (b)\n\n\n\n\n\nЯк ми можемо бачити на Рис. 8.5, коефіцієнт поглинання для вихідної матриці кореляцій залишається сталим при перших 40% варіації. На рисунку справа видно, що лише одного власного значення достатньо для опису майже половини всього ринку. Для випадкової матриці кількість власних значень необхідних для опису системи зростає прямо пропоційно відсотку варіації. Отже, для нашої задачі при розрахунку коефіцієнту поглинання достатньо буде взяти до 40% варіації при врахуванні кількості необхідних власних значень у чисельнику (8.9).\n\n\n8.2.5 Віконна процедура\nПерш ніж розпочинати розрахунки визначимо функцію для побудови графіків. Вона буде виводити один із часових рядів досліджуваної бази акцій компаній та розраховані індикатори. Ми будемо виводити як двовимірні, так і тривимірні графіки.\n\ndef plot_2d(time_ser_index, \n            time_ser_values,\n            y1_values,  \n            time_ser_label, \n            y1_label, \n            x_label,\n            file_name, \n            clr=\"magenta\"):\n\n    fig, ax = plt.subplots(1, 1)\n\n    ax2 = ax.twinx()\n\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(time_ser_index, \n                time_ser_values, \n                \"b-\", \n                label=fr\"{time_ser_label}\")\n    p2, = ax2.plot(time_ser_index, \n                y1_values, color=clr, label=y1_label)\n\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(f\"{y1_label}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=4, width=1.5)\n\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n    ax.tick_params(axis='x', **tkw)\n\n\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(file_name + \".jpg\")\n        \n    plt.show();\n\n\nwindow = 250        # розмір вікна\ntstep = 1           # крок вікна\nret_type = 4        # вид ряду: \n                    # 1 - вихідний, \n                    # 2 - детрендований (різниця між теп. значенням та попереднім)\n                    # 3 - прибутковості звичайні, \n                    # 4 - стандартизовані прибутковості, \n                    # 5 - абсолютні значення (волатильності)\n                    # 6 - стандартизований ряд\n\nlength = data.shape[1]\nnum_bins = 50\n\n\nmean_corr_init = []\nmean_corr_rand = []\n\nC_vals_init = [] \nC_vals_rand = []\n\nC_hist_vals_init = []\nC_hist_vals_rand = []\n\nu_hist_vals_init = []\nu_hist_vals_rand = []\n\nlambdas_vals_init = []\nlambdas_vals_rand = []\n\nlambdas_vals_max_init = []\nlambdas_vals_max_rand = []\n\nlambdas_hist_vals_init = []\nlambdas_hist_vals_rand = []\n\nipr_vals_init = []\nipr_vals_rand = []\n\nabsorb_vals_init = []\nabsorb_vals_rand = []\n\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    # відбираємо фрагменти\n    fragm = data.iloc[:, i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n\n    # генеруємо випадковий фрагмент\n    fragm_random = np.random.normal(size=(fragm.shape[0], fragm.shape[1]))\n\n    # розраховуємо крос-кореляції\n    C_fragm, ccoef_flat_fragm = calc_cross_corr(fragm)\n    C_fragm_random, ccoef_flat_fragm_random = calc_cross_corr(fragm_random)\n\n    # розраховуємо гістограму крос-кореляцій\n    C_fragm_hist = np.histogram(ccoef_flat_fragm, bins=num_bins, density=True)[0]\n    C_fragm_hist_rand = np.histogram(ccoef_flat_fragm_random, bins=num_bins, density=True)[0]\n\n    # розрахунок середнього значення коефіцієнта кореляції\n    mean_correlation = np.mean(ccoef_flat_fragm)\n    mean_correlation_random = np.mean(ccoef_flat_fragm_random)\n\n    # розрахунок власних значень та векторів\n    lambdas_fragm, u_fragm = calc_lambd_eig(C_fragm)\n    lambdas_fragm_random, u_fragm_random = calc_lambd_eig(C_fragm_random)\n\n    # розраховуємо гістограму власних значень\n    lambdas_fragm_hist = np.histogram(lambdas_fragm, bins=num_bins, density=True)[0]\n    u_fragm_hist_init = np.histogram(u_fragm, bins=num_bins, density=True)[0]\n    lambdas_fragm_hist_rand = np.histogram(lambdas_fragm_random, bins=num_bins, density=True)[0]\n    u_fragm_hist_rand = np.histogram(u_fragm_random, bins=num_bins, density=True)[0]\n\n    # отримання максимального власного значення\n    lambdas_fragm_max = lambdas_fragm.max()\n    lambdas_fragm_max_random = lambdas_fragm_random.max()\n\n    # отримання оберненого відношення участі\n    ipr_fragm = calc_ipr(u_fragm)\n    ipr_random = calc_ipr(u_fragm_random)\n\n    # розрахунок коефіцієнта поглинання \n    absorb_init = ar(lambdas_fragm, 0.05)[0]\n    absorb_rand = ar(lambdas_fragm_random, 0.05)[0]\n\n\n    # додаємо індикатори до масивів\n    mean_corr_init.append(mean_correlation)\n    mean_corr_rand.append(mean_correlation_random)\n\n    C_vals_init.append(ccoef_flat_fragm)\n    C_vals_rand.append(ccoef_flat_fragm_random)\n\n    C_hist_vals_init.append(C_fragm_hist)\n    C_hist_vals_rand.append(C_fragm_hist_rand)\n    lambdas_hist_vals_init.append(lambdas_fragm_hist)\n    lambdas_hist_vals_rand.append(lambdas_fragm_hist_rand)\n    u_hist_vals_init.append(u_fragm_hist_init)\n    u_hist_vals_rand.append(u_fragm_hist_rand)\n\n    lambdas_vals_init.append(lambdas_fragm)\n    lambdas_vals_rand.append(lambdas_fragm_random)\n\n    lambdas_vals_max_init.append(lambdas_fragm_max)\n    lambdas_vals_max_rand.append(lambdas_fragm_max_random)\n\n    ipr_vals_init.append(ipr_fragm)\n    ipr_vals_rand.append(ipr_random)\n\n    absorb_vals_init.append(absorb_init)\n    absorb_vals_rand.append(absorb_rand)\n\n100%|██████████| 5089/5089 [04:04&lt;00:00, 20.79it/s]\n\n\n\nind_names = ['avg_corr_init', 'lambda_max', 'ar']\n\nindicators = [mean_corr_init, lambdas_vals_max_init, absorb_vals_init]\n\nmeasure_labels = [r'$\\langle C \\rangle_{init}$', r'$\\lambda_{max}$', r'$AR$']\n\nfor i in range(len(ind_names)):\n    name = f\"RMT_{ind_names[i]}_symbol={ylabel}_wind={window}_step={tstep}_seriestype={ret_type}\"\n    np.savetxt(name + \".txt\", indicators[i])\n\n\n8.2.5.1 Середній коефіцієнт крос-кореляцій\n\nfile_name = f\"avg_corr_symbol={ylabel}_wind={window}_step={tstep}_seriestype={ret_type}\"\n\nplot_2d(data_init.loc[:, ylabel].index[window:length:tstep],\n        data_init.loc[:, ylabel].values[window:length:tstep],\n        mean_corr_init, \n        ylabel,\n        measure_labels[0],\n        xlabel,\n        file_name,\n        clr=\"magenta\")\n\n\n\n\n\n\n\nРис. 8.6: Динаміка індексу цін акцій компанії Apple та середнього показника крос-кореляцій прибутковостей досліджуваних акцій\n\n\n\n\n\nНа Рис. 8.6 видно, що середній ринковий ступінь крос-корреляцій зростає у (перед-)кризові періоди, що вказує на зростання самоорганізованості ринку. Усі індекси синхронізовано починають “відповідати” на зовнішні чинники, які змушують трейдерів колективно все відкуповувати або розпродавати.\nНа Рис. 8.7 представлений той самий показник, але у тривимірному просторі.\n\nY = np.linspace(-1, 1, num_bins)\n\nX = np.arange(window, length, tstep)\nX = np.expand_dims(X, axis=1)\nX = np.repeat(a=X, repeats=Y.shape[0], axis=1)\n\nZ = np.array(C_hist_vals_init)\n\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize=(8, 6))\n\nsurf = ax.plot_surface(X, Y, Z, cmap='hot', rstride=2, cstride=2, linewidth=0)\n\nax.set_xlabel(xlabel, fontsize=22, labelpad=15)\nax.set_ylabel(r\"$C_{ij}$\", fontsize=22, labelpad=15)\nax.set_zlabel(r\"$P(C)$\", fontsize=22, labelpad=15)\nax.tick_params(axis='both', which='major', labelsize=18, pad=5)\n\nax.view_init(60, 140)\n\nfig.tight_layout()\n\nplt.savefig(f\"RMT_3D_P(C)_wind={window}_step={tstep}_seriestype={ret_type}.jpg\", bbox_inches=\"tight\")\n\nplt.show();\n\n\n\n\n\n\n\nРис. 8.7: Віконна функція щільності розподілу коефіцієнтів кореляції вихідної матриці фінансових активів\n\n\n\n\n\n\n\n8.2.5.2 Максимальне значення \\(\\lambda\\)\nНа наступному рисунку (Рис. 8.8) представлено порівняльну динаміку індексу цін акцій компанії Apple та показника \\(\\lambda_{max}\\).\n\nfile_name = f\"lambda_max_symbol={ylabel}_wind={window}_step={tstep}_seriestype={ret_type}\"\n\nplot_2d(data_init.loc[:, ylabel].index[window:length:tstep],\n        data_init.loc[:, ylabel].values[window:length:tstep],\n        lambdas_vals_max_init, \n        ylabel,\n        measure_labels[1],\n        xlabel,\n        file_name,\n        clr=\"red\")\n\n\n\n\n\n\n\nРис. 8.8: Динаміка індексу цін акцій компанії Apple та показника \\(\\lambda_{max}\\)\n\n\n\n\n\nРис. 8.8 показує, що \\(\\lambda_{max}\\) у схожий спосіб із \\(\\langle C \\rangle_{init}\\). Із динаміки даного показника можна зробити висновок, що серед усіх індексів найбільш впливовим є індекс, якому характерно \\(\\lambda_{max}\\). Усі інші індекси, власне значення яких знаходиться в межах теоретичного передбачення ТВМ, вносять лише шумову інформацію в загальну динаміку ринку і реагують на всі інші збурення на фондовому ринку, слідуючи закономірностям найбільш капіталізованих індексів.\n\n\n8.2.5.3 Обернене відношення участі\nНа Рис. 8.9 представлено тривимірну віконну динаміка показника оберненого відношення участі\n\ndef log_tick_formatter(val, pos=None):\n    return f\"$10^{{{int(val)}}}$\"  \n\nY = np.array(lambdas_vals_init)\n\nX = np.arange(window, length, tstep)\nX = np.expand_dims(X, axis=1)\nX = np.repeat(a=X, repeats=Y.shape[1], axis=1)\n\nZ = np.array(ipr_vals_init)\n\nfig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"}, figsize=(8, 6))\n\nsurf = ax.plot_surface(X, np.log(Y), Z, cmap='magma', rstride=2, cstride=2, linewidth=0.5)\n\nax.set_xlabel(xlabel, fontsize=22, labelpad=15)\nax.set_ylabel(r\"$\\lambda_i$\", fontsize=22, labelpad=15)\nax.set_zlabel(r\"$IPR$\", fontsize=22, labelpad=15)\nax.tick_params(axis='both', which='major', labelsize=18, pad=5)\n\nax.yaxis.set_major_formatter(mticker.FuncFormatter(log_tick_formatter))\nax.yaxis.set_major_locator(mticker.MaxNLocator(integer=True))\n\nfig.tight_layout()\n\nplt.savefig(f\"RMT_3D_IPR_wind={window}_step={tstep}_seriestype={ret_type}.jpg\", bbox_inches=\"tight\")\n\nplt.show();\n\n\n\n\n\n\n\nРис. 8.9: Тривимірна віконна динаміка показника оберненого відношення участі\n\n\n\n\n\nОВУ на Рис. 8.9 характеризується значним зростанням у кризові періоди, у той час як для періодів релаксації цей показник залишається на рівні нуля.\n\n\n8.2.5.4 Коефіцієнт поглинання\nНа Рис. 8.10 представлено порівняльну динаміку індексу цін акцій компаній Apple та коефіцієнту поглинання.\n\nfile_name = f\"ar_symbol={ylabel}_wind={window}_step={tstep}_seriestype={ret_type}\"\n\nplot_2d(data_init.loc[:, ylabel].index[window:length:tstep],\n        data_init.loc[:, ylabel].values[window:length:tstep],\n        absorb_vals_init, \n        ylabel,\n        measure_labels[2],\n        xlabel,\n        file_name,\n        clr=\"black\")\n\n\n\n\n\n\n\nРис. 8.10: Динаміка індексу цін акцій компаній Apple та коефіцієнту поглинання\n\n\n\n\n\nНа Рис. 8.10 видно, що \\(AR\\) зростає під час крахів 2008, 2015-2016, 2021 і 2023 року. Це говорить про те, що в кризові періоди зростає активність одного або декілька індексів, які стають рушієм для усього фондового ринку.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Лабораторна робота № 8</span>"
    ]
  },
  {
    "objectID": "lab_8.html#висновок",
    "href": "lab_8.html#висновок",
    "title": "8  Лабораторна робота № 8",
    "section": "8.3 Висновок",
    "text": "8.3 Висновок\nТаким чином, при наявності сукупності часових рядів, що є даними діяльності економічних об’єктів однієї області, можна провести дослідження стосовно структури вказаної області та взаємодії об’єктів всередині неї. Дослідження проводяться на основі теорії випадкових матриць, що дозволяє отримувати інформацію шляхом аналізу матриці крос-кореляцій, побудованої для сукупної бази економічних об’єктів.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Лабораторна робота № 8</span>"
    ]
  },
  {
    "objectID": "lab_8.html#завдання-для-самостійної-роботи",
    "href": "lab_8.html#завдання-для-самостійної-роботи",
    "title": "8  Лабораторна робота № 8",
    "section": "8.4 Завдання для самостійної роботи",
    "text": "8.4 Завдання для самостійної роботи\nОберіть певний фондовий індекс і проведіть дослідження крахових подій для ринку, що він представляє за допомогою теорії випадкових матриць.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Лабораторна робота № 8</span>"
    ]
  },
  {
    "objectID": "lab_8.html#контрольні-запитання",
    "href": "lab_8.html#контрольні-запитання",
    "title": "8  Лабораторна робота № 8",
    "section": "8.5 Контрольні запитання",
    "text": "8.5 Контрольні запитання\n\nПоясніть основну ідею теорії випадкових матриць\nПро що свідчить відмінність кореляційних і спектральних властивостей матриці даних і випадкової?\nДослідіть, як змінюється розподіл власних значень у випадках:\n\n\\(\\lambda_{-} &lt; \\lambda &lt; \\lambda_{+}\\);\n\\(\\lambda &gt; \\lambda_{+}\\);\n\\(\\lambda &lt; \\lambda_{-}\\).\n\nПорівняйте кольорову карту поля взаємних кореляцій випадкової матриці і заданої. Зробіть висновки\n\n\n\n\n\n[1] E. P. Wigner, On the Statistical Distribution of the Widths and Spacings of Nuclear Resonance Levels, Mathematical Proceedings of the Cambridge Philosophical Society 47, 790 (1951).\n\n\n[2] E. P. Wigner, On a Class of Analytic Functions from the Quantum Theory of Collisions, Annals of Mathematics 53, 36 (1951).\n\n\n[3] F. J. Dyson, Statistical Theory of the Energy Levels of Complex Systems. I, Journal of Mathematical Physics 3, 140 (2004).\n\n\n[4] F. J. Dyson and M. L. Mehta, Statistical Theory of the Energy Levels of Complex Systems. IV, Journal of Mathematical Physics 4, 701 (2004).\n\n\n[5] M. L. Mehta and F. J. Dyson, Statistical Theory of the Energy Levels of Complex Systems. V, Journal of Mathematical Physics 4, 713 (2004).\n\n\n[6] M. L. Mehta, Random Matrices (Academic Press, 1991).\n\n\n[7] T. A. Brody, J. Flores, J. B. French, P. A. Mello, A. Pandey, and S. S. M. Wong, Random-Matrix Physics: Spectrum and Strength Fluctuations, Rev. Mod. Phys. 53, 385 (1981).\n\n\n[8] L. Laloux, P. Cizeau, J.-P. Bouchaud, and M. Potters, Noise Dressing of Financial Correlation Matrices, Phys. Rev. Lett. 83, 1467 (1999).\n\n\n[9] Aspects of Multivariate Statistical Theory (Wiley, New York, 1982).\n\n\n[10] F. J. Dyson, Distribution of Eigenvalues for a Class of Real Symmetric Matrices, Revista Mexicana de Fisica 20, 231 (1971).\n\n\n[11] A. M. Sengupta and P. P. Mitra, Distributions of Singular Values for Some Random Matrices, Phys. Rev. E 60, 3389 (1999).\n\n\n[12] V. Plerou, P. Gopikrishnan, B. Rosenow, L. A. Nunes Amaral, and H. E. Stanley, Universal and Nonuniversal Properties of Cross Correlations in Financial Time Series, Phys. Rev. Lett. 83, 1471 (1999).\n\n\n[13] T. Guhr, A. Müller–Groeling, and H. A. Weidenmüller, Random-Matrix Theories in Quantum Physics: Common Concepts, Physics Reports 299, 189 (1998).\n\n\n[14] Y. V. Fyodorov and A. D. Mirlin, Analytical Derivation of the Scaling Law for the Inverse Participation Ratio in Quasi-One-Dimensional Disordered Systems, Phys. Rev. Lett. 69, 1093 (1992).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Лабораторна робота № 8</span>"
    ]
  },
  {
    "objectID": "lab_9.html",
    "href": "lab_9.html",
    "title": "9  Лабораторна робота № 9",
    "section": "",
    "text": "9.1 Теоретичні відомості\nЗдавалося б, випадкові коливання у складних системах часто демонструють різний рівень складності та хаотичності. В умовах обмеженості даних стає важко визначити межі їх передбачуваності. Аналіз таких систем, процесів, що визначають їх динаміку, теорія хаосу розглядалися в різних галузях, таких як економіка, фінанси, фізика та ін. Що стосується аналізу, наприклад, динаміки Біткоїна, то знання про його абсолютно випадкові і, водночас, детерміновані процеси потенційно можуть пояснити флуктуації часових рядів різної природи. Протягом багатьох років теорія хаосу надавала підходи до вивчення деяких цікавих властивостей часових рядів. Найбільш поширеними є: кореляційна розмірність, BDS тест, ентропія Колмогорова, показники Ляпунова тощо.\nПродемонструємо, яким чином показники Ляпунова дають можливість дослідити режими хаотичної та детермінованої поведінки.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Лабораторна робота № 9</span>"
    ]
  },
  {
    "objectID": "lab_9.html#теоретичні-відомості",
    "href": "lab_9.html#теоретичні-відомості",
    "title": "9  Лабораторна робота № 9",
    "section": "",
    "text": "9.1.1 Показники Ляпунова\nЕволюція системи демонструє чутливу залежність від початкових умов. Це означає, що спочатку близькі траєкторії, які розвиваються, можуть швидко відхилятися одна від одної і мати абсолютно різні результати. Відповідно, при малих невизначеностях, які надзвичайно швидко посилюються, довгострокові прогнози виявляються неможливими. З іншого боку, в системі з точками тяжіння або стабільними точками відстань між ними асимптотично зменшується з часом або з кількістю точок, які мають тенденцію до зближення  [1].\nЩоб представити ідею більш точно, розглянемо дві послідовні траєкторії — \\(x(t)\\) та наближчого сусіда цієї траєкторії з невеликим зміщенням, \\(x(t) + \\delta(t)\\), де \\(\\delta(t)\\) представляє собою мале відхилення в часі \\(t\\), як показано на Рис. 9.1.\n\n\n\n\n\n\nРис. 9.1: Розбіжність двох початково близьких траєкторій  [2]\n\n\n\nКоли динаміка двох початково близьких траєкторій порушується певною подією відстань між ними може зростати експоненційно  [3]:\n\\[\n\\| \\delta(t) \\| \\approx \\| \\delta(0) \\| \\exp(\\lambda t),\n\\tag{9.1}\\]\nде \\(\\lambda\\) позначає показник Ляпунова (ПЛ); \\(\\delta(t)\\) — відстань між точкою що розглядається та її наближчим сусідом після \\(t\\) ітерацій; \\(\\delta(0)\\) — початкова відстань між точкою, що розглядається та її найближчим сусідом у початковий момент часу (\\(t=0\\)).\nПЛ є мірою швидкості експоненціальної розбіжності близьких один до одного траєкторій у фазовому просторі динамічної системи. Іншими словами, ПЛ показує, наскільки швидко зближуються або розходяться траєкторії, які починаються близько одна від одної, вимірюючи ступінь хаосу в системі.\nУ тих випадках, коли наша система \\(n\\)-вимірна, ми маємо стільки ж ПЛ. Для їх визначення розглянемо еволюцію нескінченно малої сфери, що зазнала збурень за різними осями. Визначивши величину збурення по вісі \\(i\\) як \\(\\delta_i(t)\\), отримаємо \\(n\\) показників Ляпунова, що мають вид\n\\[\n\\| \\delta_i(t) \\| \\approx \\| \\delta_i(0) \\| \\exp(\\lambda_i t), \\; \\text{для} \\; i=1,...,n.\n\\tag{9.2}\\]\nДля визначення того, чи є рух періодичним або хаотичним, особливо для великих \\(t\\), рекомендується розглядати внесок системи в найбільший показник Ляпунова (НПЛ), оскільки діаметр \\(n\\)-розмірного еліпсоїда починає залежати від нього  [2]. Саме НПЛ використовується для кількісної оцінки передбачуваності систем, оскільки експоненціальна розбіжність означає, що в системі, де початкове збурення було нескінченно малим, починаються втрати передбачуванності. Однак слід зазначити, що інші показники також містять важливу інформацію щодо стійкості системи, в тому числі про напрямки збіжності та розбіжності траєкторій  [4].\nІснування принаймні одного позитивного ПЛ зазвичай розглядається як сильний індикатор хаосу. Позитивний ПЛ означає, що початково близькі траєкторії у фазовому просторі, чутливі до початкових умов і розходяться експоненціально швидко. Негативний ПЛ відповідає випадкам, коли траєкторії залишаються близькими одна до одної, але це не обов’язково означає стабільність, і ми повинні дослідити нашу систему більш детально. Нульові або дуже близькі до нуля показники вказують на те, що збурення, практично не впливають на еволюцію траєкторій динамічної системи.\nУ зв’язку з великою зацікавленістю в ПЛ, з’являється все більше інструментів для розрахунку. На жаль, досі не отримано загальноприйнятого та універсального методу оцінки всього спектру показників Ляпунова за значеннями часового ряду. Одні з найбільш поширених і популярних алгоритмів були застосовані Вольфом та ін.  [5], Сано і Савадою  [6], а пізніше вдосконалені Екманом  [7], Розенштейном  [8], Парліцом  [9], Бальцержаком тощо  [10].\n\n9.1.1.1 Метод Екмана\nПо-перше, згідно з підходом Екмана та ін.  [7], ми повинні реконструювати динаміку атрактора з часового ряду \\(\\{x(i) \\,|\\, i=1,...N \\}\\) з розмірністю вкладень \\(d_E\\), і після цього побудувати \\(d_E\\)-вимірну орбіту, що представляє часову еволюцію\n\\[\n\\vec{X}(i) = \\left[ x(i), x(i+1), ..., x(i+(d_E - 1)) \\right], \\; \\text{для} \\; i=1,...,N-d_E+1.\n\\]\nДалі, ми маємо визначити найближчі до \\(\\vec{X}(i)\\) траєкторії:\n\\[\n\\| \\vec{X}(i) - \\vec{X}(j) \\| = \\max_{0\\leq \\alpha \\leq d_E-1} \\left| x(i+\\alpha) - x(j+\\alpha) \\right|.   \n\\tag{9.3}\\]\nСортуємо \\(x(i)\\) так, щоб \\(x(\\Pi(1)) \\leq x(\\Pi(2)) \\leq ... \\leq x(\\Pi(N))\\) і зберігаємо перестановку \\(\\Pi\\) та її зворотню версію \\(\\Pi^-1\\). Далі намагаємось знайти сусідів \\(x(i)\\), переглядаючи \\(k=\\Pi^{-1}(i)\\) і скануємо \\(x(\\Pi(s))\\) при \\(s=k+1, k+2,...\\) і \\(k-1, k-2,...\\) до тих пір, до поки не виконається умова \\(x(\\Pi(𝑠)) - x(i) &gt; r\\). Для вибраної розмірності вкладення \\(d_E &gt; 1\\) вибираємо вибираємо значення \\(s\\) за умови\n\\[\n\\left| x(\\Pi(s) + \\alpha) - x(j + \\alpha) \\right| \\leq r, \\; \\text{для} \\; \\alpha=0,1,...,d_E-1.\n\\]\nПісля реконструкції систем до розмірності \\(d_E\\) потрібно визначити матрицю \\(M_i\\) розмірності \\(d_E \\times d_E\\), яка описуватиме часову еволюцію векторів, із оточення траєкторії \\(\\vec{X}(i)\\), і те, як вони відображаються на стан \\(\\vec{X}(i+1)\\). Матриця \\(M_i\\) отримується шляхом пошуку сусідів\n\\[\nM_i(\\vec{X}(i) - \\vec{X}(j)) \\approx \\vec{X}(i+1) - \\vec{X}(j+1).\n\\tag{9.4}\\]\nВектори \\(\\vec{X}(i)-\\vec{X}(j)\\) можуть і не покривати \\(\\mathbb{R}^{d_E}\\). У цьому випадку така невизначеність може призвести до хибних показників, які можуть зіпсувати аналіз. Для подолання таких перешкод проекція траєкторій визначається на підпросторі розмірності \\(d_M \\leq d_E\\). Таким чином, простір, на якому відбувається динаміка, відповідає локальній розмірності \\(d_M\\), а \\(d_E\\) має бути дещо більшим за \\(d_M\\), щоб уникнути наявності хибних сусідів  [11,12]. Звідси випливає, що траєкторія \\(\\vec{X}(i)\\) асоціюється з \\(d_M\\)-вимірним вектором\n\\[\n\\vec{X}(i) = \\left[ x(i), x(i+\\tau), ..., x(i+(d_M - 1)\\tau) \\right] = \\left[ x(i), x(i+\\tau), ..., x(i+d_E - 1) \\right],\n\\tag{9.5}\\]\nде \\(\\tau=(d_E-1)/(d_M-1)\\). Коли \\(\\tau&gt;1\\), умова (9.4) замінюється наступним виразом:\n\\[\nM_i(\\vec{X}(i) - \\vec{X}(j)) \\approx \\vec{X}(i+\\tau) - \\vec{X}(j+\\tau).\n\\tag{9.6}\\]\nМатриця \\(M_i\\) визначається методом найменших квадратів. Останнім кроком є QR декомпозиція для знаходження ортогональних матриць \\(Q_i\\) і верхніх трикутних матриць \\(R_i\\) при яких\n\\[\nM_{1+i\\tau}Q_i = Q_{i+1}R_{i+1}, \\; \\text{для} \\; i=0,1,2,... .\n\\]\nЯк було запропоновано Екманом  [7,13], знаючи \\(K\\) кількість точок на атракторі, діагональні власні значення матриці \\(R_i\\) та крок дискретизації \\(\\Delta t\\), можна визначити наступне рівняння для знаходження \\(k\\)-го ПЛ:\n\\[\n\\lambda_k = \\frac{1}{\\Delta t}\\frac{1}{\\tau}\\frac{1}{K}\\sum_{i=0}^{K-1}\\ln{(R_i)_{kk}}.\n\\]\n\n\n9.1.1.2 Метод Розенштейна\nАлгоритм Розенштейна  [8] використовує метод реконструкції вкладень із часовою затримкою, який передає найважливіші особливості багатовимірного атрактора в один одновимірний часовий ряд деякого скінченного розміру \\(N\\). Для часового ряду кожен вектор \\(\\vec{X}(i)\\) буде представлений подібно до вектора (9.5) із розмірністю вкладень \\(d_E\\) і часовою затримкою \\(\\tau\\). Потім на відновленій траєкторії ми ініціалізуємо пошук у просторі станів найближчого сусіда \\(\\vec{X}(j)\\) для траєкторії \\(\\vec{X}(i)\\):\n\\[\n\\delta_i(0) = \\min_{\\vec{X}(i)} \\| \\vec{X}(i) - \\vec{X}(j) \\|, \\; \\text{для} \\; \\left| i-j \\right| &gt; \\text{середній період},\n\\]\nде \\(\\| \\cdot \\|\\) — це Евклідова норма, \\(\\vec{X}(j)\\) — найближча сусідня траєкторія, \\(\\vec{X}(i)\\) — розглядувана траєкторія.\nЗ (9.1) ми вже знаємо, що відстань між станами \\(\\vec{X}(i)\\) та \\(\\vec{X}(j)\\) зростає з часом відповідно до степеневого закону, де \\(\\lambda\\) є хорошим наближенням СПЛ. Для подальших оцінок розглянемо логарифм відстані на траєкторії \\(\\ln{\\delta_i(k)} \\approx \\lambda(k\\cdot \\Delta t) + \\ln{c_i}\\), де \\(\\delta_i(k)\\) — відстань між \\(i\\)-ою парою найближчих сусідів, визначених рівнянням (9.6) через \\(k\\) часових кроків, \\(c_i\\) — початкова відстань між ними, а \\(\\Delta t\\) — часовий інтервал між вимірюваннями (період дискретизації часового ряду).\nПодальший результат цього алгоритму представляє функцію від часу\n\\[\ny(k, \\Delta t) = \\frac{1}{\\Delta t}\\frac{1}{M}\\sum_{i=1}^{M}\\ln{\\delta_i(k)},\n\\]\nде \\(M=N-(d_E-1)\\tau\\) — розмір реконструйованого часового ряду, а \\(\\delta_i(k)\\) — \\(i\\)-та лінія, нахил котрої приблизно рівний СПЛ. Тоді пропонується обчислювати СПЛ як кут нахилу найбільш лінійної ділянки. Знаходження такої ділянки виявляється нетривіальною задачею. Незважаючи на цю проблему, метод Розенштейна є простим для реалізації та обчислення.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Лабораторна робота № 9</span>"
    ]
  },
  {
    "objectID": "lab_9.html#хід-роботи",
    "href": "lab_9.html#хід-роботи",
    "title": "9  Лабораторна робота № 9",
    "section": "9.2 Хід роботи",
    "text": "9.2 Хід роботи\nРозглянемо, як можна використовувати зазначені підходи для розрахунку відповідних хаос-динамічних індикаторів. Спочатку імпортуємо необхідні бібліотеки.\n\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport neurokit2 as nk\nimport yfinance as yf\nimport pandas as pd\nimport scienceplots\nfrom tqdm import tqdm\n\n%matplotlib inline\n\nДалі виконаємо налаштування формату виведення рисунків.\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    'axes.labelsize': 22,\n    'legend.fontsize': 22, \n    'xtick.labelsize': 22,\n    'ytick.labelsize': 22,\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\nРозглянемо значення фондового індексу Доу Джонса за весь період, що представляє Yahoo! Finance. Початкову та кінцеві дати зазначати не будемо.\n\nsymbol = '^DJI'                       # Символ індексу\n\ndata = yf.download(symbol)            # вивантажуємо дані\ntime_ser = data['Adj Close'].copy()   # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'                 # підпис по вісі Ох \nylabel = symbol                       # підпис по вісі Оу\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nУвага\n\n\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance, а із власного файлу. Зрозуміло, що й аналіз результатів, і висновки залежать від того, з яким рядом ми працюємо\n\n\n\n\nsymbol = 'sMpa11'                  # Символ індексу\n\npath = \"databases\\sMpa11.txt\"      # шлях по якому здійснюється зчитування файлу\ndata = pd.read_csv(path,           # зчитування даних \n                   names=[symbol])\ntime_ser = data[symbol].copy()     # копіюємо значення кривої \n                                   # \"напруга-видовження\" до окремої змінної\n\nxlabel = r'$\\varepsilon$'          # підпис по вісі Ох \nylabel = symbol                    # підпис по вісі Оу\n\n\nВиводимо досліджуваний ряд:\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\n\n\n\nРис. 9.2: Динаміка щоденних значень фондового індексу Доу Джонса\n\n\n\n\n\nВизначимо функцію transformation() для виконання перетворення ряду до прибутковостей або стандартизованих значень:\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\nВизначимо функцію для побудови парних графіків:\n\ndef plot_pair(x_values, \n              y1_values,\n              y2_values,  \n              y1_label, \n              y2_label,\n              x_label, \n              file_name, clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(x_values, \n                  y1_values, \n                  \"b-\", label=fr\"{y1_label}\")\n    p2, = ax2.plot(x_values,\n                   y2_values, \n                   color=clr, \n                   label=y2_label)\n\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(f\"{y1_label}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=4, width=1.5)\n    ax.tick_params(rotation=45, axis='x', **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n\n\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(file_name + \".jpg\")\n        \n    plt.show();\n\n\n9.2.1 Обчислення показників Ляпунова із використанням віконної процедури\nДля подальших розрахунків використовуватимемо бібліотеку neurokit2. Ключовою функцією для отримання відповідних показників є complexity_lyapunov(). Вона надає доступ до розрахунків згідно з наступними алгоритмами:\n\nРозенштейна та ін. (1993);\nМаковскі — це спеціальна модифікація алгоритму Розенштейна, що використовує процедуру \\(k\\)-вимірного дерева для більш ефективного обчислення найближчих сусідів. Крім того, СПЛ обчислюється як нахил до точки зміни швидкості розбіжності (точки, де вона вирівнюється), що робить його більш стійким до параметра довжини траєкторії;\nЕкман та ін. (1986).\n\nРозглянемо її синтаксис більш детально:\ncomplexity_lyapunov(signal, delay=1, dimension=2, method='rosenstein1993', separation='auto', **kwargs)\nПараметри:\n\nsignal (Union[list, np.array, pd.Series]) — сигнал;\ndelay (int) — часова затримка (часто позначається \\(\\tau\\) іноді називають запізненням);\ndimension (int) — розмірність вкладень (\\(m\\), іноді позначається як \\(d\\) або порядок). Якщо метод має значення \"eckmann1986\", рекомендується використовувати більші значення розмірності;\nmethod (str) — метод, який визначає алгоритм обчислення ПЛ. Може бути \"rosenstein1993\", \"makowski\" або \"eckmann1986\";\nlen_trajectory (int) — застосовується, якщо метод \"rosenstein1993\". Кількість точок даних, в яких простежуються сусідні траєкторії;\nmatrix_dim (int) — застосовується, якщо метод \"eckmann1986\". Відповідає кількості ПЛ, які потрібно повернути;\nmin_neighbors (int, str) — застосовується, якщо метод \"eckmann1986\". Мінімальна кількість сусідів. Якщо \"default\", використовується min(2 * matrix_dim, matrix_dim + 4);\nkwargs (необов’язково) — інші аргументи, які передаються до signal_psd() для обчислення мінімального часового розділення двох сусідів.\n\nПовертає:\n\nlle (float) — оцінка СПЛ, якщо метод \"rosenstein1993\", і масив ПЛ, якщо \"eckmann1986\";\ninfo (dict) — словник, що містить додаткову інформацію щодо параметрів, які використовуються для обчислення СПЛ.\n\nПеред розрахунками виконаємо оновлення бібліотеки neurokit2:\n\n!pip install --upgrade neurokit2 \n\n\n9.2.1.1 Обчислення старшого показника Ляпунова на основі методу Розенштейна\nСпочатку виконаємо розрахунки для всього ряду індексу Доу Джонса:\n\nsignal = time_ser.copy()\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\ntime_ser_ret = transformation(signal, ret_type) \n\nДалі визначимо наступні параметри:\n\nd_E = 3                      # розмірність вкладень\ntau = 10                     # часові затримка\napproach_lyap = \"makowski\"   # метод для розрахунку старшого показника\nmax_len = \"auto\"             # встановлюємо максимальну довжину траєкторії у 10 разів більшу за затримку\nsep = \"auto\"                 # оцінка середнього періоду як величину, обернену до середньої частоти спектра потужності \n\nі візуалізуємо результат:\n\nlle, _ = nk.complexity_lyapunov(signal=time_ser_ret, \n                                method=approach_lyap, \n                                dimension=d_E, \n                                delay=tau,\n                                max_length=max_len,\n                                separation=sep, \n                                show=True)\n\n\n\n\n\n\n\nРис. 9.3: Діаграма розбіжності траєкторій реконструйованого фазового простору індексу Доу Джонса, що представляє розрахований СПЛ\n\n\n\n\n\nНа Рис. 9.3 показано типовий графік (суцільна крива) залежності середньої розбіжності траєкторій від часу \\(\\Delta t\\); помаранчева лінія має нахил, що дорівнює теоретичному значенню \\(\\lambda_{max}\\). Коротка синя ділянка до переходу через червону пунктирну лінію використовується для вилучення найбільшого показника Ляпунова. Як ми можемо бачити, крива змінюється при більших часових періодах, оскільки система обмежена у фазовому просторі і середня дивергенція не може перевищувати “довжину” атрактора. Отриманий показник Ляпунова вказує на те, що індекс Доу Джонса знаходиться на межі між хаосом та стабільність, тобто індекс дивергенції динаміки ряду врівноважується конвергенцією.\nЯк ми вже мали змогу переконатись, складні системи мінливі і система з плином часу може проявляти як конвергенцію чи дивергенцію, так і повну незмінність у часі.\nДалі розглянемо динаміку досліджуваної системи з часом у рамках процедури ковзного вікна. Визначимо наступні параметри:\n\nwindow = 500            # ширина вікна\ntstep = 1               # часовий крок вікна \nlength = len(time_ser)  # довжина самого ряду\nret_type = 1            # вид ряду: \n                        # 1 - вихідний, \n                        # 2 - детрендований (різниця між теп. значенням та попереднім)\n                        # 3 - прибутковості звичайні, \n                        # 4 - стандартизовані прибутковості, \n                        # 5 - абсолютні значення (волатильності)\n                        # 6 - стандартизований ряд\n\nd_E = 3                      # розмірність вкладень\ntau = 1                      # часові затримка\napproach_lyap = \"makowski\"   # метод для розрахунку старшого показника: rosenstein1993, makowski \nmax_len = \"auto\"             # встановлюємо максимальну довжину траєкторії у 10 разів більшу за затримку: auto\nsep = \"auto\"                 # оцінка середнього періоду як величину, обернену до середньої частоти спектра потужності\n\nLLE = []                # масив для збереження СПЛ\n\nТепер можна приступати до віконної процедури:\n\nfor i in tqdm(range(0,length-window,tstep)):  # фрагменти довжиною window  \n                                              # з кроком tstep\n\n    fragm = time_ser.iloc[i:i+window].copy()  # відбираємо фрагмент\n\n    fragm = transformation(fragm, ret_type)   # виконуємо процедуру \n                                              # трансформації ряду\n    \n    lle, _ = nk.complexity_lyapunov(signal=fragm, \n                                method=approach_lyap, \n                                dimension=d_E, \n                                delay=tau,\n                                max_length=max_len,\n                                separation=sep, \n                                show=False)\n    \n    LLE.append(lle)\n\n100%|██████████| 7599/7599 [00:31&lt;00:00, 239.01it/s]\n\n\nЗберігаємо отримані результати в текстовому файлі:\n\nname = f\"LLE_name={symbol}_window={window}_step={tstep}_rettype={ret_type}_\\\n    d_E={d_E}_tau={tau}_approach={approach_lyap}_max_len={max_len}_separation={sep}.txt\"\n\nnp.savetxt(name, LLE)\n\nВизначаємо параметри для збереження рисунків:\n\n# позначення показника Ляпунова в легенді рисунку \nlabel_lyap = r'$\\lambda_{max}$'  \n\n# назва рисунку\nfile_name = f\"LLE_name={symbol}_window={window}_step={tstep}_rettype={ret_type}_\\\n    d_E={d_E}_tau={tau}_approach={approach_lyap}_max_len={max_len}_separation={sep}\"\n\n# колір показника\ncolor = 'red'  \n\nта виводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          LLE, \n          ylabel, \n          label_lyap,\n          xlabel,\n          file_name,\n          color)\n\n\n\n\n\n\n\nРис. 9.4: Динаміка індексу Доу Джонса та старшого показника Ляпунова\n\n\n\n\n\nБачимо (Рис. 9.4), що СПЛ починає спадати в кризові та передкризові стани, що вказує на зростання корельованності досліджуваної динаміки. У момент кризи СПЛ починає зростати, що вказує на зростання дивергенції в кризові періоди.\n\n\n9.2.1.2 Обчислення показників Ляпунова на основі методу Екмана\nВизначимо наступні параметри:\n\nwindow = 500            # ширина вікна\ntstep = 1               # часовий крок вікна \nlength = len(time_ser)  # довжина самого ряду\nret_type = 1            # вид ряду: \n                        # 1 - вихідний, \n                        # 2 - детрендований (різниця між теп. значенням та попереднім)\n                        # 3 - прибутковості звичайні, \n                        # 4 - стандартизовані прибутковості, \n                        # 5 - абсолютні значення (волатильності)\n                        # 6 - стандартизований ряд\n\nd_E = 4                         # розмірність вкладень вихідного простору (кількість показників)\nd_M = 3                         # розмірність вкладень підпростору \n\napproach_lyap = \"eckmann1986\"   # метод для розрахунку старшого показника\nsep = \"auto\"                    # оцінка середнього періоду як величину, обернену до середньої частоти спектра потужності\nmin_neighb = \"default\"          # min(2 * matrix_dim, matrix_dim + 4)        \n\nLE = []                         # масив для збереження ПЛ  \n\nТепер переходимо до розрахунків:\n\nfor i in tqdm(range(0,length-window,tstep)):  # фрагменти довжиною window  \n                                              # з кроком tstep\n\n    fragm = time_ser.iloc[i:i+window].copy()  # відбираємо фрагмент\n\n    fragm = transformation(fragm, ret_type)   # виконуємо процедуру \n                                              # трансформації ряду\n    \n    le, _ = nk.complexity_lyapunov(signal=fragm, \n                                method=approach_lyap, \n                                dimension=d_E,\n                                matrix_dim=d_M, \n                                min_neighbors=min_neighb,\n                                separation=sep, \n                                show=False)\n\n    LE.append(le)\n\n100%|██████████| 7599/7599 [09:33&lt;00:00, 13.25it/s]\n\n\nЗберігаємо отримані результати в текстових файлах:\n\nLE = np.array(LE)\n\nfor i in range(d_E):\n    np.savetxt(f\"LE number={i+1}_name={symbol}_window={window}_step={tstep}_rettype={ret_type}_\\\n    d_E={d_E}_d_M={d_M}_approach={approach_lyap}_min_neighbors={min_neighb}_separation={sep}.txt\", LE[i])\n\nВізуалізуємо отримані результати:\n\nfig, ax = plt.subplots(LE.shape[1]+1, 1, figsize=(10, 10), sharex=True)\n\nax[0].plot(time_ser.index[window:length:tstep], time_ser.values[window:length:tstep], label=symbol)\nax[0].set_ylabel(symbol)\nax[0].legend()\n\nfor i in range(1, LE.shape[1]+1):\n    ax[i].plot(time_ser.index[window:length:tstep], LE[:, i-1], color='red', label=fr'$\\lambda_{i}$')\n    ax[i].set_ylabel(fr\"$\\lambda_{i}$\")\n    ax[i].legend()\n\nax[-1].set_xlabel(xlabel)\nfig.subplots_adjust(hspace=0)\n\nplt.savefig(f\"LE name={symbol}_window={window}_step={tstep}_rettype={ret_type}_\\\n    #d_E={d_E}_d_M={d_M}_approach={approach_lyap}_min_neighbors={min_neighb}_separation={sep}.jpg\")\nplt.show();\n\n\n\n\n\n\n\nРис. 9.5: Динаміка індексу Доу Джонса та спектра показників Ляпунова\n\n\n\n\n\nЯк показано на Рис. 9.5, спектр показників Ляпунова реагує особливим чином на кризові події фондового ринку. Видно, що, по-перше, \\(\\lambda\\) спадає в передкризові періоди та зростає під час кризи. Особливо характерною є дана динаміка перед кризами 1997, 2001, 2008, 2011, 2015, 2020 років. У передкризові періоди спостерігається конвергенція траєкторій у фазовому просторі системі, що говорить про зростання її впорядкованості. Сам кризовий та посткризовий періоди характеризуються дивергенцією, тобто розбіжністю траєкторій системи. По-друге, видно, що, спускаючись від 1-го до 4-го показника Ляпунова, ми поступово втрачаємо інформацію про динаміку системи. Тобто, перші найбільші показники представляються в даному випадку найбільш інформативними. Можливо, у даному випадку, має сенс розглядати лише старший ПЛ.\nНа Рис. 9.6 представлено порівняльну динаміку індексу Доу-Джонса та старшого показника Ляпунова на основі методу Екмана.\nЗберігаємо показник у текстовому файлі:\n\nname = f\"LE Eckman name={symbol}_window={window}_step={tstep}_rettype={ret_type}_\\\n    #d_E={d_E}_d_M={d_M}_approach={approach_lyap}_min_neighbors={min_neighb}_separation={sep}.txt\"\n\nnp.savetxt(name, LE[:, 0])\n\nВизначаємо параметри для збереження рисунків:\n\n# позначення показника Ляпунова в легенді рисунку \nlabel_lyap = r'$\\lambda_{max}$'  \n\n# назва рисунку\nfile_name = f\"LE Eckman name={symbol}_window={window}_step={tstep}_rettype={ret_type}_\\\n    #d_E={d_E}_d_M={d_M}_approach={approach_lyap}_min_neighbors={min_neighb}_separation={sep}\"\n\n# колір показника\ncolor = 'red'  \n\nВізуалізуємо старший показник Ляпунова:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          LE[:, 0], \n          ylabel, \n          label_lyap,\n          xlabel,\n          file_name,\n          color)\n\n\n\n\n\n\n\nРис. 9.6: Динаміка індексу Доу Джонса та старшого показника Ляпунова отриманого за допомогою методу Екмана\n\n\n\n\n\nТеорія хаосу та її інструментарій залишаються величезним викликом для дослідників різних галузей науки. У світі показників Ляпунова зберігається зростаючий інтерес до їх визначення, чисельних методів та застосування до різних складних систем. Підсумовуючи, СПЛ дозволяє встановити:\n\nобласть чутливості до початкових умов;\nобласть хаосу;\nобласть стабільності.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Лабораторна робота № 9</span>"
    ]
  },
  {
    "objectID": "lab_9.html#завдання-для-самостійної-роботи",
    "href": "lab_9.html#завдання-для-самостійної-роботи",
    "title": "9  Лабораторна робота № 9",
    "section": "9.3 Завдання для самостійної роботи",
    "text": "9.3 Завдання для самостійної роботи\n\nОберіть часовий варіант згідно вашого варіанту\nВизначіть період прогнозованості досліджуваної системи за діаграмою розбіжності фазових траєкторій\nПроаналізуйте динаміку старшого показника Ляпунова та його спектру за описаними в лабораторній роботі алгоритмами",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Лабораторна робота № 9</span>"
    ]
  },
  {
    "objectID": "lab_9.html#контрольні-запитання",
    "href": "lab_9.html#контрольні-запитання",
    "title": "9  Лабораторна робота № 9",
    "section": "9.4 Контрольні запитання",
    "text": "9.4 Контрольні запитання\n\nПоясніть, які відмінності хаотичного ряду динаміки від інших процесів.\nЯким чином наявність або відсутність хаотичності може впливати на можливість прогнозування розвитку певного процесу?\n\n\n\n\n\n[1] C. J. Gavilán-Moreno and G. Espinosa-Paredes, Using Largest Lyapunov Exponent to Confirm the Intrinsic Stability of Boiling Water Reactors, Nuclear Engineering and Technology 48, 434 (2016).\n\n\n[2] A. Prieto-Guerrero and G. Espinosa-Paredes, Dynamics of BWRs and Mathematical Models, in Linear and Non-Linear Stability Analysis in Boiling Water Reactors, edited by A. Prieto-Guerrero and G. Espinosa-Paredes (Woodhead Publishing, 2019), pp. 193–268.\n\n\n[3] V. N. Soloviev, A. Bielinskyi, O. Serdyuk, V. Solovieva, and S. Semerikov, Lyapunov Exponents as Indicators of the Stock Market Crashes, in Proceedings of the 16th International Conference on ICT in Education, Research and Industrial Applications. Integration, Harmonization and Knowledge Transfer. Volume II: Workshops, Kharkiv, Ukraine, October 06-10, 2020, edited by O. Sokolov, G. Zholtkevych, V. Yakovyna, Y. Tarasich, V. Kharchenko, V. Kobets, O. Burov, S. Semerikov, and H. Kravtsov, Vol. 2732 (CEUR-WS.org, 2020), pp. 455–470.\n\n\n[4] D. Nychka, S. Ellner, A. R. Gallant, and D. McCaffrey, Finding Chaos in Noisy Systems, Journal of the Royal Statistical Society. Series B (Methodological) 54, 399 (1992).\n\n\n[5] A. Wolf, J. B. Swift, H. L. Swinney, and J. A. Vastano, Determining Lyapunov Exponents from a Time Series, Physica D: Nonlinear Phenomena 16, 285 (1985).\n\n\n[6] M. Sano and Y. Sawada, Measurement of the Lyapunov Spectrum from a Chaotic Time Series, Phys. Rev. Lett. 55, 1082 (1985).\n\n\n[7] J.-P. Eckmann, S. O. Kamphorst, D. Ruelle, and S. Ciliberto, Liapunov Exponents from Time Series, Phys. Rev. A 34, 4971 (1986).\n\n\n[8] M. T. Rosenstein, J. J. Collins, and C. J. De Luca, A Practical Method for Calculating Largest Lyapunov Exponents from Small Data Sets, Physica D: Nonlinear Phenomena 65, 117 (1993).\n\n\n[9] U. Parlitz, Identification of True and Spurious Lyapunov Exponents from Time Series, International Journal of Bifurcation and Chaos 02, 155 (1992).\n\n\n[10] M. Balcerzak, D. Pikunov, and A. Dabrowski, The Fastest, Simplified Method of Lyapunov Exponents Spectrum Estimation for Continuous-Time Dynamical Systems, Nonlinear Dynamics 94, 3053 (2018).\n\n\n[11] H. Kantz and T. Schreiber, Nonlinear Time Series Analysis (Cambridge University Press, 2004).\n\n\n[12] H. D. I. Abarbanel, R. Brown, J. J. Sidorowich, and L. Sh. Tsimring, The Analysis of Observed Chaotic Data in Physical Systems, Rev. Mod. Phys. 65, 1331 (1993).\n\n\n[13] J.-P. Eckmann and D. Ruelle, Ergodic Theory of Chaos and Strange Attractors, Rev. Mod. Phys. 57, 617 (1985).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Лабораторна робота № 9</span>"
    ]
  },
  {
    "objectID": "lab_10.html",
    "href": "lab_10.html",
    "title": "10  Лабораторна робота № 10",
    "section": "",
    "text": "10.1 Теоретичні відомості",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Лабораторна робота № 10</span>"
    ]
  },
  {
    "objectID": "lab_10.html#теоретичні-відомості",
    "href": "lab_10.html#теоретичні-відомості",
    "title": "10  Лабораторна робота № 10",
    "section": "",
    "text": "10.1.1 Неекстенсивна термодинаміка і кризи фінансово-економічних систем\nВеликий виклик теорії складності, що лежить в основі сучасної наукової парадигми, бере початок ще із старих та таких важливих проблем, як: стріла часу, існування простого та фундаментального фізичного рівня для єдиного опису макроскопічного та мікроскопічного рівнів, взаємозв’язок між спостерігачем та досліджуваним об’єктом, і т. д. Загалом, що стосується теорії складності та кожного нового рівня реальності, потрібні нові концепції та нові класифікації.\nЗокрема, теорія складності включає: хаотичну динаміку в просторі станів, далеку від рівноважних фазових переходів, довготривалі кореляції, самоорганізацію та мультимасштабність, фрактальні процеси в просторі і часі та інші значущі явища  [1]. Теорія складності розглядається як третя наукова революція минулого століття (після теорії відносності та квантової теорії). Однак теорія складності ще далека від своєї академічної зрілості. У цьому напрямку вагомий внесок щодо питання “що таке складність” можна знайти в книзі Г. Ніколісa та І. Пригожина  [2]. Як правило, ми можемо узагальнити основну концепцію теорії складності наступним чином:\n\nТеорія складності — це узагальнення статистичної фізики для критичних станів термодинамічної рівноваги та для далеких від рівноваги процесів.\nСкладність — це поширення динаміки на нелінійність і дивну динаміку.\nТакож, згідно Іллі Пригожину, теорія складності пов’язана з динамікою кореляцій замість динаміки траєкторій або хвильових функцій.\n\nЗгідно з теорією складності, різні фізичні явища, що відбуваються в розподілених фізичних системах, таких як космічна плазма, рідини або тверді тіла, хімія, біологія, екосистеми, динаміка ДНК, соціально-економічні чи інформаційні системи, мережі можна описати і зрозуміти подібним чином. Цей опис базується на принципі максимізації ентропії. Також згідно з теорією складності, вказані системи є цілісно стійкими дисипативними структурами, що утворюються загальним природним процесом, спрямованим на максимізацію ентропії. З точки зору складності, немає суттєвої диференціації між групою галактик, зірками, тваринами, квітами або елементарними частинками, оскільки скрізь ми маємо відкриті, динамічні та самоорганізовані системи і всюди природа працює з метою максимізації ентропії.\nПід час дослідження складних фізичних систем та явищ, зокрема, самоорганізаційних і фрактальних структур, субдифузії, турбулентності, хімічних реакцій, а також різних економічних, соціальних і біологічних систем розподіл Гіббса не забезпечує узгодження із спостережуваними явищами. Як виявляється у багатьох дослідженнях, для таких систем характерні степеневі розподіли  [3]. Вони не отримуються з принципу максимуму ентропії Гіббса-Шеннона, на якому ґрунтується як рівноважна, так і нерівноважна статистична термодинаміка  [4–6]. Це спричинило численні спроби побудови узагальненої статистики, яка б забезпечила степеневу асимптотику функції розподілу. Таку узагальнену статистику можна будувати на основі кількох ентропій. Серед них важливе місце посідає ентропія Тсалліса (Tsallis).\nДослідження в області механіки неекстенсивних (неадитивних) систем стали останнім часом предметом значного інтересу в зв’язку з проявами неаддитивних властивостей в аномальних фізичних явищах. Це пояснюється як новизною виникаючих тут загальнотеоретичних проблем, так і важливістю практичних застосувань (див. бібліографію, представлену на сайті, яка постійно оновлюється). Початок систематичного вивчення в цьому напрямку пов’язаний з роботою К. Тсалліса, в якій автором була введена параметрична формула статистичної \\(q\\)-ентропії, залежної від деякого дійсного числа \\(q\\) (так званого параметра деформації) і неадитивної для сукупності незалежних складних систем. Теорія неекстенсивних систем, заснована на ентропії Тсалліса, в даний час інтенсивно розвивається. Ці роботи стали значним кроком у розвитку теоретико-інформаційного підходу і при розробці принципів неекстенсивної статистичної механіки та рівноважної термодинаміки відкритих систем. При цьому важливо відзначити, що діапазон застосування цих та багатьох інших неекстенсивних параметричних ентропій в даний час постійно розширюється, охоплюючи різні напрямки в науці, такі як космологія і космогонія, теорія плазми, квантова механіка і статистика, нелінійна динаміка і фрактали, геофізика, біомедицина і багато інших.\nЕкономічну динаміку з фізичної точки зору можна розглядати як просторово розподілену динаміку та пов’язану із загальною категорією нелінійних розподілених систем. Аналіз економічних часових рядів демонструє складну та хаотичну динаміку у фазовому просторі. Теорема Такенса (за допомогою методу затримок) дозволяє реконструювати топологічний еквівалент до вихідного фазового простору, який зберігає основні геометричні та динамічні властивості, такі як ступені свободи, фрактальна розмірність, мультифрактальність, показники Ляпунова, матриця прогнозування тощо. Реконструйований фазовий простір може бути використаний для оцінки всіх вищезазначених величин, а також фазових переходів, статистичної поведінки, генерування ентропії тощо. Крім того, фазовий простір може мати мультифрактальні властивості та характеристики переривчастої турбулентності, які вказують на існування дальніх взаємодій у просторі та часі, а також мультимасштабну взаємодію.\nЦі характеристики також вказують на існування дробової динаміки у фазовому просторі, яку можна описати за допомогою дробово-диференціальних рівнянь Фоккера-Планка та аномальних дифузійних рівнянь. Рішеннями цих рівнянь є дробові просторово-часові функції та негаусові функції розподілу, які належать до категорії розподілів Леві та розподілів Тсалліса. Нерівноважні стаціонарні стани економічної динаміки походять від процесів сильної самоорганізації, що відповідає локальним максимумам ентропії Тсалліса, тоді як зміни параметрів управління економічної системи можуть спричинити фазовий перехід та зміщення економічної динаміки до нової стійкої рівноваги, стійкого стану з максимальною ентропією Тсалліса. Цей фазовий перехід призводить до мультифрактальної зміни у формуванні фазового простору та до зміни феноменології економічної системи. Нарешті, статистику динаміки в мультифрактальному фазовому просторі можна описати за допомогою степеневих функцій розподілу Тсалліса з “важкими” хвостами, які можуть бути використані для вдосконалення методів прогнозування.\nВ останні роки статистична механіка розширила своє початкове призначення: застосування статистики до великих систем, стани яких регулюються якимись гамільтоновими функціоналами  [3]. Їх здатність пов’язувати мікроскопічні стани окремих складових системи з макроскопічними властивостями сьогодні використовується повсюдно  [@ 4]. Безумовно, найважливішим із цих зв’язків все-таки є визначення термодинамічних властивостей через відповідність між поняттям ентропії, спочатку введеним Рудольфом Клаузіусом в 1865 р., та кількістю дозволених мікроскопічних станів, введеним Людвігом Больцманом близько 1877 р. коли він вивчав підхід до рівноваги ідеального газу  [6]. Цей зв’язок можна виразити як\n\\[\nS = k\\ln{W},\n\\tag{10.1}\\]\nде \\(k\\) — позитивна константа, а \\(W\\) — кількість мікростанів, сумісних з макроскопічним станом ізольованої системи. Це рівняння, відоме як принцип Больцмана, є одним із наріжних каменів стандартної статистичної механіки. Коли система не ізольована, а замість цього контактує з деяким великим резервуаром, можна модифікувати рівняння (10.1) і отримати ентропію Больцмана-Гіббса (БГ-BG):\n\\[\nS_{BG} = -k\\sum_{i=1}^{W}p_i\\ln{p_i},\n\\tag{10.2}\\]\nде \\(p_i\\) — ймовірність мікроскопічної конфігурації \\(i\\)  [3]. Статистична механіка BG все ще ґрунтується на таких гіпотезах, як молекулярний хаос  [2] та ергодичність  [7]. Незважаючи на відсутність фактичного фундаментального виведення, статистика BG, безсумнівно, мала успіх у вивченні систем, в яких домінують короткі просторово-часові взаємодії. Отже, цілком можливо, що інші фізичні ентропії, крім BG, можуть бути визначені для належного опису аномальних систем, для яких спрощена гіпотеза про ергодичність та/або незалежність не виконується. Натхненний такими концепціями в 1988 р. Константіно Тсалліс (К. Tsallis) запропонував узагальнення статистичної механіки BG, яка охоплює системи, що порушують ергодичність, системи, мікроскопічні конфігурації яких не можна вважати незалежними. Це узагальнення базується на неадитивних ентропіях, \\(S_q\\), що характеризується індексом \\(q\\) і призводить до неекстенсивної статистики\n\\[\nS_q = -k \\left( 1-\\sum_{i=1}^{W}p_{i}^{q} \\right) \\Bigg/ (1-q),\n\\tag{10.3}\\]\n\\(p_i\\) — ймовірності, пов’язані з мікроскопічними конфігураціями, \\(W\\) — їх загальне число, \\(q\\) — дійсне число, і \\(k\\) — постійна Больцмана. Значення \\(q\\) є мірою неекстенсивності системи. При цьому, \\(q=1\\) відповідає стандартній статистиці BG. Вираз (10.3) модифікує \\(S_{BG}\\) (\\(\\lim q\\to 1, S_q = S_{BG}\\)), як основу можливого узагальнення статистичної механіки BG  [8,9]. Значення ентропійного індексу \\(q\\) для конкретної системи повинно визначатися апріорі з мікроскопічної динаміки.\nЗ часу своєї появи ентропія Тсалліса (10.3) стала джерелом кількох важливих результатів як у фундаментальній, так і в прикладній фізиці, а також в інших наукових областях, таких як біологія, хімія, економіка, геофізика та медицина  [10].\n\n\n10.1.2 Неекстенсивна ентропія і триплет Тсалліса\nСистеми, що характеризуються статистичною механікою Больцмана-Гіббса, мають такі характеристики: (i) їх функції розподілу для енергій пропорційні експоненціальній функції; (ii) вони мають сильну чутливість до початкових умов, яка з часом зростає в геометричній прогресії (хаос), характеризуючись позитивним максимальним показником Ляпунова; (iii) їх релаксація відбувається експоненційно з певним часом релаксації. Іншими словами, ці три способи поведінки описуються експоненціальними функціями (тобто \\(q=1\\)). Однак встановлено, що для систем, які можна вивчати в рамках неекстенсивної статистичної механіки, функція щільності ймовірності енергії (пов’язана зі стаціонарністю або рівновагою), чутливість до початкових умов та релаксація описуються трьома ентропійними індексами \\(q_{stat}, q_{sens}, q_{rel}\\), які отримали назву триплета Тсалліса, або \\(q\\)-триплета Тсалліса  [4,11].\nНеекстенсивна статистична теорія математично базується на нелінійному рівнянні\n\\[\n\\frac{dy}{dx} = y^{q},\n\\tag{10.4}\\]\nрозв’язком якого є \\(q\\)-експоненціальна функція\n\\[\n\\exp_q(x) = \\begin{cases}\n    \\left( 1+(1-q)x \\right)^{1 / (1-q)}, & \\text{якщо} \\; 1+(1-q)x &gt; 0,\\\\\n    0, & \\text{якщо} \\; 1+(1-q)x \\leq 0.\n\\end{cases}\n\\tag{10.5}\\]\nДля \\(q\\to1\\) \\(q\\)-Гаусіан відповідає звичайному розподілу Гауса.\nРозв’язок рівняння (10.4) можна реалізувати трьома різними способами, включеними до \\(q\\)-триплету Тсалліса: (\\(q_{sens}, q_{stat}, q_{rel}\\)). Ці величини характеризують три фізичні процеси, які узагальнені тут, тоді як значення \\(q\\)-триплету характеризують атракторний набір динаміки у фазовому просторі динаміки, і вони можуть змінюватися, коли динаміка системи притягується до іншого набору атракторів.\nДля неекстенсивної системи величина \\(q\\)-індексу залежить від оцінюваних властивостей динаміки і фазового простору системи. Для динамічних систем оцінюється \\(q\\)–триплет, що відображає три властивості системи (Рис. 10.1). Індекс \\(q_{stat}\\) оцінюється на основі рівноважної моделі рангового розподілу з використанням методів нелінійного оцінювання  [12]. Цей індекс є параметром області атракції системи. Індекс \\(q_{sens}\\) відображає чутливість системи до початкових умов та виробництво ентропії і визначається за мультифрактальним спектром  [13]. Релаксаційний індекс \\(q_{rel}\\) знаходять на основі автокореляції і характеристик дифузійних процесів  [14].\n\n\n\n\n\n\nРис. 10.1: Часова періодизація періодів виробництва \\(q\\)-ентропії. Перший період відповідає виробництву ентропії через параметр \\(q_{sens}\\) \\(q\\)-триплету Тсалліса. Другий період відповідає певному процесу релаксації через параметр \\(q_{rel}\\). Система виявляє коливання через параметр \\(q_{stat}\\) \\(q\\)-триплету Тсалліса\n\n\n\n\n10.1.2.1 Стаціонарність \\(q=q_{stat}\\)\nЗначення \\(q\\) для стаціонарного стану оцінюють із функції розподілу прибутковостей, що в свою чергу отримується шляхом підгонки \\(q\\)-Гаусіана:\n\\[\nP_q(\\beta, x) = \\left( \\sqrt{\\beta} \\bigg/ C_q \\right)\\exp(-\\beta rx^2)\n\\tag{10.6}\\]\nдля емпірично побудованої гістограми \\(\\{ p(x_i)\\;|\\;i=1,...,N \\}\\) та різних значень \\(\\beta\\), що підбираються шляхом мінімізації \\(\\sum_i \\left[ P_{q_{stat}}(\\beta, x_i) - p(x_i) \\right]^2\\). В залежності від значення \\(q\\), \\(C_q\\) може приймати наступні види:\n\\[\nC_q = \\begin{cases}\n    2\\sqrt{\\pi}\\,\\Gamma\\left( \\frac{1}{1-q} \\right) \\Big/ (3-q)\\sqrt{1-q}\\,\\Gamma\\left( \\frac{3-q}{2(1-q)} \\right), & \\text{якщо} \\; -\\infty&lt;q&lt;1,\\\\\n    \\sqrt{\\pi}, & \\text{якщо} \\; q=1,\\\\\n    \\sqrt{\\pi}\\,\\Gamma\\left( \\frac{3-q}{2(q-1)} \\right) \\Big/ \\sqrt{q-1}\\,\\Gamma\\left( \\frac{1}{q-1} \\right), & \\text{якщо} \\; 1&lt;q&lt;3.\n\\end{cases}\n\\tag{10.7}\\]\nДля оцінки динаміки значення \\(q\\) будується графік залежності \\(\\ln_q[p(x)]\\) від \\(x^2\\) для вибраного інтервалу \\(q\\) (наприклад, від 1 до 5), що забезпечує найкраще лінійне наближення (оцінюється за максимальним коефіцієнтом детермінації \\(R^2\\))  [15]. Зрозуміло, що значення \\(p(x)\\) стають помітно негаусівськими вздовж хвостів, і замість цього можуть бути описані степеневим законом.\n\n\n10.1.2.2 Релаксація \\(q=q_{rel}\\)\nВідповідне \\(q\\)-значення для релаксаційного процесу знаходиться з коефіцієнта автокореляції:\n\\[\nC(\\tau) = \\sum_{t}|g_{t+\\tau}|\\cdot|g_t| \\bigg/ \\sum_{t}|g_t|^2.\n\\tag{10.8}\\]\nДля статистики BG така кореляція має спадати експоненціально. Той самий алгоритм, що й для \\(q_{stat}\\), необхідно проробити на графіку залежності \\(\\ln_{q}[C(\\tau)]\\) від \\(\\tau\\) щоб визначити, який набір \\(q\\) найкраще лінеаризує емпіричні дані.\n\n\n10.1.2.3 Чутливість до початкових умов \\(q=q_{sens}\\)\nВиробництво ентропії пов’язане із загальним характером атракторної множини. Цей атрактор може бути описаний мультифрактальністю, а також чутливістю до початкових умов. Чутливість до початкових умов можна виразити як:\n\\[\n\\frac{d\\xi}{dt} = \\lambda_1\\xi + (\\lambda_q - \\lambda_1)\\xi^q,\n\\tag{10.9}\\]\nде \\(\\xi\\) — відхилення траєкторії у фазовому просторі: \\(\\xi \\equiv \\lim_{\\delta \\to 0} \\left[ \\delta(t)/\\delta(0) \\right]\\), і \\(\\delta(t)\\) — це відстань між сусідніми траєкторіями через час \\(t\\). Розв’язок рівняння (10.9) може бути представлений у вигляді:\n\\[\n\\xi = \\left[ 1 - (\\lambda_{q_{sens}} \\big/ \\lambda_{1}) + (\\lambda_{q_{sens}} \\big/ \\lambda_{1}) \\exp{\\left( \\left( 1-q_{sens} \\right)\\lambda_{1}t \\right)} \\right]^{1 / (1-q_{sens})}.\n\\tag{10.10}\\]\nСпочатку було висловлено гіпотезу, а згодом доведено для часових рядів неекстенсивних систем різної природи, що має місце таке співвідношення  [16]:\n\\[\n1 \\big/ (1-q_{sens}) = 1 \\big/ \\alpha_{min} - 1 \\big/ \\alpha_{max},\n\\tag{10.11}\\]\nде \\(\\alpha_{min}\\) та \\(\\alpha_{max}\\) — відповідно мінімальні та максимальні значення \\(\\alpha\\) відповідного мультифрактального спектру \\(f(\\alpha)\\).\nСпектр мультифрактальності, в свою чергу, випливає з процедури мультифрактального аналізу детрендованих флуктуацій (МФ-АДФ), що дозволяє розрахувати показник Херста для різних часових масштабів.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Лабораторна робота № 10</span>"
    ]
  },
  {
    "objectID": "lab_10.html#хід-роботи",
    "href": "lab_10.html#хід-роботи",
    "title": "10  Лабораторна робота № 10",
    "section": "10.2 Хід роботи",
    "text": "10.2 Хід роботи\nРозглянемо як можна застосовувати зазначені показники в якості індикаторів кризових станів.\nСпочатку імпортуємо необхідні бібліотеки:\n\nimport matplotlib.pyplot as plt \nimport numpy as np\nimport yfinance as yf\nimport pandas as pd\nimport scienceplots\nimport neurokit2 as nk\nimport fathon\nimport scipy\nimport statsmodels.api as sm\nfrom fathon import fathonUtils as fu\nfrom scipy.stats import norm\nfrom scipy.special import gamma\nfrom scipy.optimize import curve_fit\nfrom tqdm import tqdm\n\n%matplotlib inline\n\nта визначимо необхідні функції для подальшої роботи:\n\n# q-експоненціальна функція\ndef np_exp_q(x, q=1):\n    if q==1:\n        return np.exp(x)\n    else:\n        return (1+(1-q)*x)**(1/(1-q))\n\n# q-логарифм\ndef np_log_q(x, q=1):\n    if q==1:\n        return np.log(x)\n    else: \n        return x**(1-q)-1/(1-q)\n\n# значення для обчислення q-гаусіана\ndef C_q(q=1.0):\n    if q==1:\n        return np.sqrt(np.pi)\n    elif q&lt;1:\n        return 2*np.sqrt(np.pi)*gamma(1/(1-q))/(3-q)*np.sqrt(1-q)*gamma((3-q)/(2*(1-q)))\n    elif q&gt;1:\n        return (np.sqrt(np.pi)*gamma((3-q))/(2*(q-1)))/(np.sqrt(q-1)*gamma(1/(q-1)))\n\n# функція щільності q-гаусіана для обчислення q_stat\ndef G_q(r, beta, q):\n    return np.sqrt(beta)/C_q(q) * np_exp_q(-beta*r, q)\n\n\n# функція автокореляцій для обчислення q_rel\ndef acf(x, maxlag):\n\n    n = len(x)\n    a = (x - x.mean()) / (x.std() * n)\n    b = (x - x.mean()) / x.std()\n\n    cor = np.correlate(a, b, mode=\"full\")\n    acf = cor[n:n+maxlag+1]\n    lags = np.arange(maxlag + 1)\n\n    return acf, lags\n\n# рункція релаксацій для обчислення q_rel\ndef rel_func(x, q, tau):\n    return np_exp_q(-x/tau, q)\n\n\n# функція для обчислення прибутковостей ряду чи його стандартизації\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\n\n# функція для побудови парних графіків\ndef plot_pair(x_values, \n              y1_values,\n              y2_values,  \n              y1_label, \n              y2_label,\n              x_label, \n              file_name, clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(x_values, \n                  y1_values, \n                  \"b-\", label=fr\"{y1_label}\")\n    p2, = ax2.plot(x_values,\n                   y2_values, \n                   color=clr, \n                   label=y2_label)\n\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(f\"{y1_label}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n\n\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(file_name + \".jpg\")\n        \n    plt.show();\n\nДалі виконаємо налаштування формату виведення рисунків:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    'axes.labelsize': 22,\n    'legend.fontsize': 22, \n    'xtick.labelsize': 22,\n    'ytick.labelsize': 22,\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\nУ цій роботі розглянемо динаміку неекстенсивних показників на прикладі фондового індексу S&P 500, але дивитимемось на ряд, починаючи з 2016 року. Для отримання значень індексу скористаємось бібліотекою yfinance.\n\nsymbol = '^DJI'          # Символ індексу\nstart = \"2016-01-01\"     # Дата початку зчитування даних\nend = \"2023-12-31\"       # Дата закінчення зчитування даних\n\ndata = yf.download(symbol, start, end)  # вивантажуємо дані\ntime_ser = data['Adj Close'].copy()     # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'    # підпис по вісі Ох \nylabel = symbol          # підпис по вісі Оу\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nУвага\n\n\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance, а із власного файлу. Зрозуміло, що й аналіз результатів, і висновки залежать від того, з яким рядом ми працюємо\n\n\n\n\nsymbol = 'sMpa11'                  # Символ індексу\n\npath = \"databases\\sMpa11.txt\"      # шлях по якому здійснюється зчитування файлу\ndata = pd.read_csv(path,           # зчитування даних \n                   names=[symbol])\ntime_ser = data[symbol].copy()     # копіюємо значення кривої \n                                   # \"напруга-видовження\" до окремої змінної\n\nxlabel = r'$\\varepsilon$'          # підпис по вісі Ох \nylabel = symbol                    # підпис по вісі Оу\n\n\nВиводимо досліджуваний ряд:\n\nfig, ax = plt.subplots(1, 1)               # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\n\n\n\nРис. 10.2: Динаміка щоденних змін фондового індексу Доу Джонса\n\n\n\n\n\n\n10.2.1 Розрахунок показника \\(q_{stat}\\)\n\n10.2.1.1 Побудова \\(q\\)-гаусіана для всього ряду\n\nq_stat_time_ser = time_ser.copy()\nret_type = 4 # визначення типу ряду для його перетворення\nq_stat_time_ser = transformation(q_stat_time_ser, ret_type)\n\nhist, bin_edg = np.histogram(q_stat_time_ser, bins=250, density=True)\n\nmu, std = norm.fit(q_stat_time_ser)\nx = np.linspace(q_stat_time_ser.min(), q_stat_time_ser.max(), len(bin_edg[1:]))\np = norm.pdf(x, mu, std)\n\nxval = bin_edg[1:]**2\nyval = hist\n\npopt, pcov = curve_fit(G_q, xdata=xval, ydata=yval, bounds=([0.0, 0.0], [np.inf, 3.0]))\n\n\nfig, ax = plt.subplots(1, 1)\nax.plot(bin_edg[1:], hist, 'o', label=r\"$P_{емпіричний}$\")\nax.plot(x, p, 'o', label=\"Гаус\")\nax.plot(x, G_q(x**2, popt[0], popt[1]), label=r\"$q$-Гаусіан\")\nax.set_yscale('log')\nax.set_xlabel(\"x\")\nax.set_ylabel(r\"$\\log{P(\\beta, x)}$\")\n\nplt.legend()\nplt.show(); \n\n\n\n\n\n\n\nРис. 10.3: Функція розподілу нормалізованих прибутковостей для Доу Джонса в порівнянні з Гаусіан та \\(q\\)-гаусіаном\n\n\n\n\n\n\n\n10.2.1.2 Розрахунок \\(q_{stat}\\) у віконній процедурі\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nlength = len(time_ser)\n\nq_stats = []\n\n\nfor i in tqdm(range(0,length-window,tstep)):\n\n    fragm = time_ser.iloc[i:i+window].copy()  # відбираємо фрагмент\n\n    fragm = transformation(fragm, ret_type)   # виконуємо процедуру \n                                              # трансформації ряду\n\n    hist_fragm, bin_edg_fragm = np.histogram(fragm, bins=100, density=True)\n\n    xval = bin_edg_fragm[1:]**2\n    yval = hist_fragm\n\n    popt, pcov = curve_fit(G_q, xdata=xval, ydata=yval, bounds=([0.01, 1.0], [np.inf, 5.0]))\n    q_stat = popt[1]\n\n    \n    q_stats.append(q_stat)\n\n100%|██████████| 1762/1762 [00:10&lt;00:00, 163.19it/s]\n\n\nЗберігаємо отримані результати в текстовому файлі:\n\nname = f\"q_stat_name={symbol}_window={window}_step={tstep}_rettype={ret_type}.txt\"\n\nnp.savetxt(name, q_stats)\n\nВизначаємо параметри для збереження рисунків:\n\n# позначення показника q_stat в легенді рисунку \nlabel_q_stat = r'$q_{stat}$'  \n\n# назва рисунку\nfile_name = f\"q_stat_name={symbol}_window={window}_step={tstep}_rettype={ret_type}\"\n\n# колір показника\ncolor = 'brown'  \n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          q_stats, \n          ylabel, \n          label_q_stat,\n          xlabel,\n          file_name,\n          color)\n\n\n\n\n\n\n\nРис. 10.4: Порівняльна динаміка коливань ціни індексу Доу Джонса та показника \\(q_{stat}\\)\n\n\n\n\n\nЯк ми можемо бачити на Рис. 10.4, показник \\(q_{stat}\\) зростає під час крахових явищ на фондовому ринку. Це вказує на значне зростання ступеня впливу важких хвостів у розподілі прибутковостей досліджуваного індексу.\n\n\n\n10.2.2 Розрахунок показника \\(q_{rel}\\)\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nmax_lag = 100\n\nlength = len(time_ser)\n\nq_rels = []\n\n\nfor i in tqdm(range(0,length-window,tstep)):\n\n    fragm = time_ser.iloc[i:i+window].copy()  # відбираємо фрагмент\n\n    fragm = transformation(fragm, ret_type)   # виконуємо процедуру \n                                              # трансформації ряду\n\n    autocor, lags = acf(x=fragm, maxlag=max_lag)\n    lags = lags\n    autocor = autocor\n    \n    popt, pcov = curve_fit(rel_func, xdata=lags[1:], ydata=autocor[1:], bounds=(1, [np.inf, 10]))\n    q_rel = popt[0]\n    \n    q_rels.append(q_rel)\n\n100%|██████████| 1762/1762 [00:08&lt;00:00, 216.61it/s]\n\n\nЗберігаємо отримані результати в текстовому файлі:\n\nname = f\"q_rel_name={symbol}_window={window}_step={tstep}_rettype={ret_type}_maxlag={max_lag}.txt\"\n\nnp.savetxt(name, q_rels)\n\nВизначаємо параметри для збереження рисунків:\n\n# позначення показника q_rel в легенді рисунку \nlabel_q_rel = r'$q_{rel}$'  \n\n# назва рисунку\nfile_name = f\"q_rel_name={symbol}_window={window}_step={tstep}_rettype={ret_type}_maxlag={max_lag}\"\n\n# колір показника\ncolor = 'red'  \n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          q_rels, \n          ylabel, \n          label_q_rel,\n          xlabel,\n          file_name,\n          color)\n\n\n\n\n\n\n\nРис. 10.5: Порівняльна динаміка коливань ціни індексу Доу Джонса та показника \\(q_{rel}\\)\n\n\n\n\n\nДля досліджуваного показника на Рис. 10.5 видно, що ступінь релаксації зростає саме в передкризовий стан системи, що є індикатором зростання самоорганізації трейдерів через певні зовнішні показники. Дана динаміка узгоджується зі зростанням ступеня автокореляції під час кризових подій, що ми мали змогу спостерігати в першій лабораторній.\n\n\n10.2.3 Розрахунок показника \\(q_{sens}\\)\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nrev = True         # Чи повторювати розрахунок ф-ції флуктуацій з кінця\naccumulate = False # Повторна акумуляція детрендованого ряду для роботи із сильно антиколрельованими рядами\n\nq_min = -5         # мінімальне значення q\nq_max = 5          # максимальне значення q\nq_inc = 1          # крок збільшення q\n\nwin_beg = 10       # Початкова ширина сегменту\nwin_end = window-1 # Кінцева ширина сегменту\n\nlength = len(time_ser)\n\nq = np.arange(q_min, q_max+q_inc, q_inc)\nq = np.round_(q, decimals = 1)\n\norder = 1          # порядок поліному для детрендування (MF-DFA)\n\nq_sens_values = []\n\n\nfor i in tqdm(range(0,length-window,tstep)):\n\n    fragm = time_ser.iloc[i:i+window].copy()  # відбираємо фрагмент\n\n    fragm = transformation(fragm, ret_type)   # виконуємо процедуру \n                                              # трансформації ряду\n\n    if accumulate == True:\n        fragm = np.cumsum(fragm-np.mean(fragm))\n\n    a = fu.toAggregated(fragm)\n\n    pymfdfa = fathon.MFDFA(a)\n\n    wins = fu.linRangeByStep(win_beg, win_end)\n\n    n, F = pymfdfa.computeFlucVec(wins, q, revSeg=rev, polOrd=order)\n    list_H, list_H_intercept = pymfdfa.fitFlucVec()\n\n    if accumulate == True:\n        list_H = list_H - 1\n    \n    # розрахунок значень tau(q)\n    tau = q * list_H - 1\n\n    # розрахунок значень сингулярності\n    alpha = np.gradient(tau, q, edge_order=2)\n\n    # максимальне значення сингулярності\n    maximal_alpha = alpha.max()\n\n    # мінімальне значення сингулярності\n    minimal_alpha = alpha.min()\n\n    # розрахунок q_sens\n    q_sens = (maximal_alpha-minimal_alpha-maximal_alpha*minimal_alpha)/(maximal_alpha-minimal_alpha)\n\n    q_sens_values.append(q_sens)\n\n100%|██████████| 1762/1762 [01:29&lt;00:00, 19.72it/s]\n\n\nЗберігаємо отримані результати в текстовому файлі:\n\nname = f\"q_sens_name={symbol}_ret={ret_type}_qmin={q_min}_qmax={q_max}_qinc={q_inc}_wind={window}_step={tstep}.txt\"\n\nnp.savetxt(name, q_sens_values)\n\nВизначаємо параметри для збереження рисунків:\n\n# позначення показника q_rel в легенді рисунку \nlabel_q_sens = r'$q_{sens}$'  \n\n# назва рисунку\nfile_name = f\"q_sens_name={symbol}_ret={ret_type}_qmin={q_min}_qmax={q_max}_qinc={q_inc}_wind={window}_step={tstep}\"\n\n# колір показника\ncolor = 'green'  \n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          q_sens_values, \n          ylabel, \n          label_q_sens,\n          xlabel,\n          file_name,\n          color)\n\n\n\n\n\n\n\nРис. 10.6: Порівняльна динаміка коливань ціни індексу Доу Джонса та показника \\(q_{sens}\\)\n\n\n\n\n\nДля показника \\(q_{sens}\\) спостерігається спад у передкризові періоди, що вказує на особливу чутливість ринку саме в ці моменти часу. Для повністю ідентичних та незалежно розподілених значень \\(q_{sens}\\) залишався б на рівні 1. У передкризові стани \\(q_{sens}\\) прямує до від’ємних значень, що говорить про конвергенцію атрактора системи до сингулярності, тобто збіжність траєкторій один до одного.\n\n\n10.2.4 Розрахунок ентропії Тсалліса\n\nwindow = 250    # розмір вікна\ntstep = 1       # крок вікна\nret_type = 1    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n\nlength = len(time_ser)\n\ntsallis_en = []\n\n\nfor i in tqdm(range(0,length-window,tstep)):\n    \n    fragm = time_ser.iloc[i:i+window].copy()  # відбираємо фрагмент\n\n    fragm = transformation(fragm, ret_type)   # виконуємо процедуру \n                                              # трансформації ряду\n\n    p, be = np.histogram(fragm,               # розраховуємо щільність ймовірностей\n                        bins='auto', \n                        density=True)  \n    r = be[1:] - be[:-1]                      # знаходимо dx\n    P = p * r                                 # представляємо ймовірність як f(x)*dx\n    P = P[P!=0]                               # фільтруємо по всім ненульовим ймовірностям\n    \n    tsen, _ = nk.entropy_tsallis(freq=P, \n                                 q=1, \n                                 base=np.exp(1))\n    tsen /= np.log(len(P))\n    \n    tsallis_en.append(tsen)\n\n100%|██████████| 1762/1762 [00:00&lt;00:00, 1876.04it/s]\n\n\nЗберігаємо отримані результати в текстовому файлі:\n\nname = f\"tsen_name={symbol}_ret={ret_type}_wind={window}_step={tstep}.txt\"\n\nnp.savetxt(name, tsallis_en)\n\nВизначаємо параметри для збереження рисунків:\n\n# позначення ентропії Тсалліса в легенді рисунку \nlabel_ts_en = r'$TsEn$'  \n\n# назва рисунку\nfile_name = f\"tsen_name={symbol}_ret={ret_type}_wind={window}_step={tstep}\"\n\n# колір показника\ncolor = 'purple'  \n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          tsallis_en, \n          ylabel, \n          label_ts_en,\n          xlabel,\n          file_name,\n          color)\n\n\n\n\n\n\n\nРис. 10.7: Порівняльна динаміка коливань ціни індексу Доу Джонса та ентропії Тсалліса$\n\n\n\n\n\nЗ Рис. 10.7 видно, що неекстенсивна ентропія Тсалліса спадає в передкризові періоди, що вказує на зростання ступеня неадитивності (самоорганізованої динаміки) ринку.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Лабораторна робота № 10</span>"
    ]
  },
  {
    "objectID": "lab_10.html#висновок",
    "href": "lab_10.html#висновок",
    "title": "10  Лабораторна робота № 10",
    "section": "10.3 Висновок",
    "text": "10.3 Висновок\nУ даній лабораторній роботі було представлено неекстенсивний підхід статистичної механіки до динаміки щоденних історичних значень ціни Доу Джонса та його прибутковостей. Встановлено, що індекс Доу Джонса підпорядковується статистиці Тсалліса. Було промодельовано часову динаміку \\(q\\)-триплету, що дало можливість при співставленні з вихідним часовим рядом отримати реакцію компонентів триплету на формування та протікання кризових явищ. Величина \\(q_{stat}\\) у періоди криз зростає, оскільки зростають власне цінові флуктуації. Значення \\(q_{rel}\\) зростає у передкризові періоди, що, очевидно, зумовлено переходом системи в нерівноважний стан і подальшою релаксацією. Нарешті, \\(q_{sens}\\) має мінімальне значення в передкризовий період, вказуючи на особливу чутливість системи поблизу точки біфуркації, якою і є сама криза.\nПерспективним представляється дослідження особливостей \\(q\\)-триплету для складних мережних структур, що отримуються при перетворенні часового ряду в мережу одним з відомих методів. Цікавим також є пошук альтернативних компонентів неекстенсивності як, наприклад, міри незворотності часового ряду, чи міри рекурентності тощо. Очевидно, що означені підходи можуть забезпечити необхідний прогрес як на фундаментальному, так і на прикладному рівнях щодо досягнення більш глибокого розуміння природи складних систем.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Лабораторна робота № 10</span>"
    ]
  },
  {
    "objectID": "lab_10.html#завдання-для-самостійного-виконання",
    "href": "lab_10.html#завдання-для-самостійного-виконання",
    "title": "10  Лабораторна робота № 10",
    "section": "10.4 Завдання для самостійного виконання",
    "text": "10.4 Завдання для самостійного виконання\n\nОберіть часовий ряд згідно вашого варіанту\nПобудуйте емпіричний розподіл прибутковостей вашого ряду та теоретичні адитивний і неекстенсивний розподіли Гауса\nПобудуйте та проаналізуйте динаміку триплету Тсалліса та неекстенсивної ентропії для кризових подій\nСформуйте звіт і зробіть висновки\n\n\n\n\n\n[1] A. Bielinskyi, S. Semerikov, O. Serdyuk, V. Solovieva, V. N. Soloviev, and L. Pichl, Econophysics of Sustainability Indices, in Proceedings of the Selected Papers of the Special Edition of International Conference on Monitoring, Modeling & Management of Emergent Economy (M3E2-MLPEED 2020), Odessa, Ukraine, July 13-18, 2020, edited by A. Kiv, Vol. 2713 (CEUR-WS.org, 2020), pp. 372–392.\n\n\n[2] G. Nicolis, I. Prigogine, W. H. Freeman, and Company, Exploring Complexity: An Introduction (W.H. Freeman, 1989).\n\n\n[3] C. Tsallis, Possible Generalization of Boltzmann-Gibbs Statistics, Journal of Statistical Physics 52, 479 (1988).\n\n\n[4] C. Tsallis, Dynamical Scenario for Nonextensive Statistical Mechanics, Physica A: Statistical Mechanics and Its Applications 340, 1 (2004).\n\n\n[5] C. Tsallis, M. Gell-Mann, and Y. Sato, Asymptotically Scale-Invariant Occupancy of Phase Space Makes the Entropy &lt;i&gt;s&lt;sub&gt;q&lt;/Sub&gt;&lt;/i&gt; Extensive, Proceedings of the National Academy of Sciences 102, 15377 (2005).\n\n\n[6] C. Tsallis, Economics and Finance: Q-Statistical Stylized Features Galore, Entropy 19, (2017).\n\n\n[7] C. Tsallis, Beyond Boltzmann–Gibbs–Shannon in Physics and Elsewhere, Entropy 21, (2019).\n\n\n[8] E. G. Pavlos, O. E. Malandraki, O. V. Khabarova, L. P. Karakatsanis, G. P. Pavlos, and G. Livadiotis, Non-Extensive Statistical Analysis of Energetic Particle Flux Enhancements Caused by the Interplanetary Coronal Mass Ejection-Heliospheric Current Sheet Interaction, Entropy 21, (2019).\n\n\n[9] R. de Oliveira, S. Brito, L. da Silva, and C. Tsallis, Connecting Complex Networks to Nonadditive Entropies, Scientific Reports 11, 1130 (2021).\n\n\n[10] G. Pavlos, A. Iliopoulos, L. Karakatsanis, M. Xenakis, and E. Pavlos, Complexity of Economical Systems., Journal of Engineering Science & Technology Review 8, (2015).\n\n\n[11] G. L. Ferri, M. F. Reynoso Savio, and A. Plastino, Tsallis’ q-Triplet and the Ozone Layer, Physica A: Statistical Mechanics and Its Applications 389, 1829 (2010).\n\n\n[12] S. Umarov, C. Tsallis, and S. Steinberg, On Aq-Central Limit Theorem Consistent with Nonextensive Statistical Mechanics, Milan Journal of Mathematics 76, 307 (2008).\n\n\n[13] C. Anteneodo and C. Tsallis, Breakdown of Exponential Sensitivity to Initial Conditions: Role of the Range of Interactions, Phys. Rev. Lett. 80, 5313 (1998).\n\n\n[14] C. TSALLIS, Some Open Problems in Nonextensive Statistical Mechanics, International Journal of Bifurcation and Chaos 22, 1230030 (2012).\n\n\n[15] D. Stosic, D. Stosic, T. B. Ludermir, and T. Stosic, Nonextensive Triplets in Cryptocurrency Exchanges, Physica A: Statistical Mechanics and Its Applications 505, 1069 (2018).\n\n\n[16] A. O. Bielinskyi, A. V. Matviychuk, O. A. Serdyuk, S. O. Semerikov, V. V. Solovieva, and V. N. Soloviev, Correlational and Non-Extensive Nature of Carbon Dioxide Pricing Market, in ICTERI 2021 Workshops, edited by O. Ignatenko, V. Kharchenko, V. Kobets, H. Kravtsov, Y. Tarasich, V. Ermolayev, D. Esteban, V. Yakovyna, and A. Spivakovsky, Vol. 1635 (Springer International Publishing, Cham, 2022), pp. 183–199.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Лабораторна робота № 10</span>"
    ]
  },
  {
    "objectID": "lab_11.html",
    "href": "lab_11.html",
    "title": "11  Лабораторна робота № 11",
    "section": "",
    "text": "11.1 Теоретичні відомості\nСкладні системи — це відкриті системи, які обмінюються енергією, речовиною та інформацією з навколишнім середовищем. Досліджуючи складні системи в природничих науках, Пригожин  [1] зробив фундаментальне узагальнення, вказавши на необхідність розгляду явищ незворотності та нерівноважності, які є фундаментальними властивостями складних систем різної природи: соціальні, економічні, біомедичні тощо  [2]. Пригожин вважав, що найважливіші зміни в сучасній науковій революції пов’язані зі зняттям попередніх обмежень у науковому розумінні часу. Нелінійний світ характеризується рисами темпоральності, тобто незворотністю і швидкоплинністю процесів і явищ. Самоорганізація розглядається як спонтанний процес формування складних систем, що інтегруються. Саме через неоднозначність вибору в точках біфуркації час у теоріях самоорганізації стає справді незворотнім. На відміну від лінійних динамічних теорій — класичної, релятивістської, квантової (де час обернений), в термодинаміці дисипативних структур, створеної Пригожиним, час перестає бути простим параметром і стає поняттям, що виражає темп і напрямок розвитку подій.\nНезворотність часу є фундаментальною властивістю нерівноважних дисипативних систем, і її втрата може свідчити про розвиток деструктивних процесів  [2,3].\nЗ огляду на статистичні властивості досліджуваного сигналу, його еволюцію можна було б назвати незворотною, якби була відсутня інваріантність, тобто був би отриманий той же сигнал, якби ми виміряли його в протилежному напрямку. Функція \\(f\\) може бути застосована для знаходження характеристик, які відрізняються прямою і зворотною версіями, тобто часові ряди незворотні, якщо \\(f(X^d) \\neq f(X^r)\\). Основна ідея цього визначення полягає в тому, що немає ніяких обмежень на \\(f(\\cdot )\\).\nПередбачається, що стаціонарний процес \\(X\\) називається статистично зворотним у часі, якщо розподіл імовірностей прямої та зворотної систем приблизно однаковий  [4–6]. Незворотність часових рядів вказує на наявність нелінійних залежностей (пам’яті)  [7] у динаміці далекої від рівноваги системи, включаючи негаусові випадкові процеси та дисипативний хаос.\nУ першій групі методів виконується символізація часових рядів, а потім проводиться аналіз шляхом статистичного порівняння рядка символів у прямому та зворотному напрямках  [8]. Іноді використовуються додаткові алгоритми стиснення  [9]. Важливим кроком для цієї групи є символізація — перетворення часового ряду у символьний ряд вимагає додаткової спеціальної інформації (наприклад, поділ діапазону або розмір алфавіту) і, отже, містить проблему залежності алгоритму від цих додаткових параметрів. Друга проблема виникає при розгляді масштабної інваріантності складних сигналів. Оскільки процедури типових символізацій є локальними, врахування різних масштабів може викликати певні труднощі  [3].\nІнша група методів формалізації показника незворотності не використовує процедуру символізації, а базується на використанні вихідних значень часових рядів або прибутковостей.\nОдин з таких підходів базується на асиметрії розподілу точок на діаграмі Пуанкаре, побудованої на основі значень досліджуваного часового ряду  [10,11].\nНещодавно було запропоновано принципово новий підхід до вимірювання незворотності часових рядів, який використовує методи теорії складних мереж  [4,12] та поєднує два інструменти: алгоритм графа видимості, що перетворює часові ряди у складну мережу та алгоритм дивергенції Куллбака-Лейблера  [12]. Перший формує спрямовану мережу за геометричним критерієм. Потім ступінь незворотності ряду оцінюється за розбіжністю Кульбака-Лейблера. Цей метод є обчислювально ефективним, не вимагає спеціального процесу символізації і, на думку авторів, природно враховує мультимасштабність.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Лабораторна робота № 11</span>"
    ]
  },
  {
    "objectID": "lab_11.html#теоретичні-відомості",
    "href": "lab_11.html#теоретичні-відомості",
    "title": "11  Лабораторна робота № 11",
    "section": "",
    "text": "11.1.1 Незворотність на основі діаграм Пуанкаре\nДіаграма Пуанкаре для часового ряду являє собою графік, на осі \\(x\\) якого розташовані значення для поточного часу \\(t\\), а на осі \\(y\\) — його наступні значення в часі \\(t+\\tau\\). Усі наступні значення, які рівні один одному \\((x(t) = x(t+\\tau))\\), розташовані на лінії ідентичності (line of identity, LI). Інтервали, що представляють зростаючу тендецію, відмічені вище LI \\(( x(t)&lt;x(t+\\tau) )\\), тоді як спадна тенденція характеризуватиметься скупченням точок нижче LI \\((x(t)&gt;x(t+\\tau))\\). Оцінюючи асиметрію точок на діаграмі, ми можемо вивести різні кількісні показники незворотності (асиметрії) досліджуваних систем  [13,14].\n\n11.1.1.1 Індекс Гузіка (GIx)\nGIx  [10] можна визначити як відношення відстаней точок вище LI до відстаней усіх точок на діаграмі:\n\\[\nGIx = \\sum_{i=1}^{a} \\left( D_{i}^{+} \\right)^{2} \\bigg/ \\sum_{i=1}^{m} \\left( D_{i} \\right)^{2},\n\\]\nде \\(a = C(P_{i}^{+})\\) позначає кількість точок над LI; \\(m = C(P_{i}^{+}) + C(P_{i}^{-})\\) позначає кількість точок на графіку Пуанкаре; \\(D_{i}^{+}\\) це відстань від точки над LI до самої LI. Відстань точки до LI можна визначити як\n\\[\nD_{i} = \\left( |x(i+\\tau) - x(i)| \\right) / \\sqrt{2}.\n\\]\n\n\n11.1.1.2 Індекс Порти (PIx)\nІндекс Порти (Porta index, PIx)  [11] визначається як кількість точок нижче LI, поділена на загальну кількість точок на графіку Пуанкаре, за винятком тих, що знаходяться на LI:\n\\[\nPIx = b / m,\n\\]\nде \\(b = C(P_{i}^{-})\\) кількість точок нижче LI.\n\n\n11.1.1.3 Індекс Кошти (CIx)\nІндекс Кошти (Costa index, CIx) [@ [15];costa2008multiscale] враховує кількість приростів \\((x(i+1)-x(i) &gt; 0)\\) та спадів \\((x(i+1)-x(i) &lt; 0)\\). Вони представляються симетричними, якщо рівні один одному. Даний індекс розраховується для двовимірної мультимасштабної площини \\((x(i), x(i+L))\\), де новий крос-гранульований ряд \\(y_{\\tau}(i) = x(i+L)-x(i)\\) для \\(1 \\leq i \\leq N-\\tau\\) відображає асиметрію приростів та спадів ряду, й індекс незворотності для діапазону масштабів \\(\\tau\\) визначається виразом:\n\\[\nCIx_{\\tau} = \\left( \\sum_{y_{\\tau}&lt;0} H[y_{\\tau}] - \\sum_{y_{\\tau}&gt;0} H[y_{\\tau}] \\right) \\Bigg/ \\left( N-\\tau \\right).\n\\]\nУзагальнений CIx для діапазону мастабів \\(\\tau\\) може бути визначений як\n\\[\nCIx = \\frac{1}{L} \\sum_{\\tau=1}^{L} |CIx_{\\tau}|,\n\\]\nде \\(L\\) — це максимальний масштаб.\n\n\n11.1.1.4 Індекс Ейлера (EIx)\nОпираючись на асиметрію розподілу точок нижче та вище LI, Ейлер запропонував наступний індекс асиметрії  [16]:\n\\[\nEIx = \\sum_{i=1}^{N-1} \\left[ x(i)-x(i+\\tau) \\right]^{3} \\Bigg/ \\left( \\sum_{i=1}^{N-1} \\left[ x(i)-x(i+\\tau) \\right]^{2} \\right)^{3/2}.\n\\]\nЗначне відхилення \\(EIx\\) від 0 вказує на асиметрію системи. Якщо \\(EIx&gt;0\\), розподіл точок на діаграмі Пуанкаре значно зміщений у сторону вище LI. Зворотня ситуація спостерігається для \\(EIx&lt;0\\). Для \\(EIx \\approx 0\\) досліджувані сегменти представляються зворотніми в часі.\n\n\n11.1.1.5 Індекс площі (AIx)\nІндекс площі  [17] визначається як сукупна площа секторів, що сформовані точками над LI поділена на сукупну площу секторів, що відповідають усім точкам на діаграмі Пуанкаре (крім тих, що розташовані точно на LI). Площа сектора, що відповідає певній точці \\(P_{i}\\), обчислюється як\n\\[\nS_{i} = 1/2 \\times R\\theta_{i} \\times r^{2},\n\\]\nде \\(r\\) — це радіус сектора; \\(R\\theta_{i} = \\theta_{LI} - \\theta_{i}\\); \\(\\theta_{LI}\\) — це фазовий кут, і \\(\\theta_{i} = \\arctan{\\left[ x(i+\\tau)/x(i) \\right]}\\), що визначає фазовий кут \\(i\\)-ої точки. \\(AIx\\) визначається за формулою:\n\\[\nAIx = \\sum_{i=1}^{a}|S_{i}| \\Bigg/ \\sum_{i=1}^{m}|S_{i}|.\n\\]\n\n\n11.1.1.6 Індекс кута нахилу (SIx)\nНа додачу до представлених вище мір, було запропоновано розраховувати незворотність сигналу з відношення кутів нахилу точок над LI до нахилу всіх точок на діаграмі  [18]:\n\\[\nSIx = \\sum_{i=1}^{a}|R\\theta_{i}| \\Bigg/ \\sum_{i=1}^{m}|R\\theta_{i}|.\n\\]\n\n\n\n11.1.2 Методи складних мереж\nГрафи видимості (VG) базуються на простому відображенні часових рядів у мережну область, де кожне спостереження є вершиною в складній мережі. Дві вершини \\(i\\) та \\(j\\) пов’язані ребром, якщо для них застосовується наступна умова  [12]:\n\\[\nx_{k} &lt; x_{j} + \\left( x_{i} - x_{j} \\right)\\left( t_{j}-t_{k} \\right)/\\left( t_{j}-t_{i} \\right).\n\\]\nде \\(x_{k}\\) представляє певну перешкоду, якої не має бути, щоб дві вершини можна було зв’язати шляхом.\nМатрицю суміжності (\\(A_{ij}\\)) представленого ненаправленого та незваженого VG можна представити як:\n\\[\nA_{ij}^{VG} = A_{ji}^{VG} = \\prod_{k=i+1}^{j-1} H \\left[ x_{k} &lt; x_{j} + \\left( x_{i} - x_{j} \\right)\\left( t_{j}-t_{k} \\right)/\\left( t_{j}-t_{i} \\right) \\right],\n\\]\nде \\(H( \\cdot )\\) — функція Хевісайда.\nГраф горизонтальної видимості (HVG) є спрощеною версією цього алгоритму  [19]. Для досліджуваного часового ряду набори вершин VG і HVG однакові, тоді як набір ребер HVG відображає взаємну горизонтальну видимість двох спостережень \\(x_{i}\\) та \\(x_{j}\\). Тобто, можна побудувати ребро \\((i,j)\\), якщо \\(x_{k} &lt; \\min(x_{i}, x_{j})\\) для всіх \\(k\\) при \\(t_{i} &lt; t_{k} &lt; t_{j}\\) так що\n\\[\nA_{ij}^{VG} = A_{ji}^{VG} = \\prod_{k=i+1}^{j-1} H \\left( x_{i} - x_{k} \\right) H \\left( x_{j} - x_{k} \\right).\n\\]\nVG і HVG фіксують по суті одні й ті ж властивості досліджуваної системи, оскільки HVG є підграфом VG з тим же набором вершин, але володіє тільки підмножиною ребер VG. Зверніть увагу, що VG інваріантний щодо суперпозиції лінійних трендів, тоді як HVG — ні.\nОскільки визначення VGs та HVGs чітко враховує часовий порядок спостережень, напрямок часу нерозривно пов’язаний з отриманою структурою мережі. Щоб врахувати цей факт, ми визначаємо набір нових статистичних мережних показників на основі двох простих характеристик вершин:\n\nОскільки кількість ребер інцидентних вершині \\(i\\) можна визначити як \\(k_{i}^{r} = \\sum_{j} A_{ij}\\), для (H)VG ми можемо переписати дану кількісну характеристику для вершини в час \\(t_{i}\\) відносно її минулих та майбутніх вершин  [6,20]:\n\\[\nk_{i}^{r} = \\sum_{j&lt;i} A_{ij} \\quad \\mathrm{і} \\quad k_{i}^{a} \\sum_{j&gt;i} A_{ij},\n\\]\nде \\(k_{i} = k_{i}^{r} + k_{i}^{a}\\), і \\(k_{i}^{r}\\) та \\(k_{i}^{a}\\) сприймаються як вхідні (минулі) та вихідні (майбутні) вершини.\nЛокальний коефіцієнт кластеризації \\(C_{i} = \\left[2/k_i(k_i - 1)\\right]\\sum_{j,k} A_{ij}A_{jk}A_{ki}\\) інша властивість старшного порядку структурного сусідства вершини \\(i\\). Для дослідження незворотності ми можемо переписати дані характеристики наступним чином:\n\n\\[\nC_{i}^{r} =  \\left[2/k_{i}^{r}(k_{i}^{r}-1)\\right]\\sum_{j&lt;i,k&lt;i} A_{ij}A_{jk}A_{ki} \\quad \\textrm{і} \\quad C_{i}^{a} =  \\left[2/k_{i}^{a}(k_{i}^{a}-1)\\right]\\sum_{j&gt;i,k&gt;i} A_{ij}A_{jk}A_{ki}.\n\\]\nЯкщо уявити нашу систему зворотною в часі, ми припускаємо, що розподіли ймовірностей прямих і зворотних за часом характеристик повинні бути однаковими. Для незворотних процесів ми очікуємо статистичну нееквівалентність. Ця нееквівалентність буде визначатися через дивергенцію Кульбака-Лейблера  [5]:\n\\[\nD_{KL}(p||q) = \\sum_{i=1}^{N} p(x_{i}) \\cdot \\log{\\left[ p(x_{i})/q(x_{i}) \\right]},\n\\]\nде \\(p(\\cdot)\\) відповідатиме розподілу вхідних характеристикам, а \\(q(\\cdot)\\) — зворотніх. Крім того, подібність обох величин можна оцінити за допомогою відстані Дженсена-Шеннона  [21]:\n\\[\nJS(p||q) = \\sqrt{\\left[ D_{KL}(p||m) + D_{KL}(q||m)/2 \\right]},\n\\]\nде \\(m=0.5 \\cdot (p + q)\\), а \\(D_{KL}\\) — дивергенція Кульбака-Лейблера.\n\n\n11.1.3 Незворотність на основі пермутаційних шаблонів\nІдея аналізу пермутаційних шаблонів (permutation patterns, PP) спочатку була запропонована Бандтом і Помпе  [22] як простий та ефективний інструмент для характеристики складності динаміки реальних систем. Він уникає порогу амплітуди і замість цього має справу з порядковими шаблонами перестановок. Їх частоти дозволяють відрізнити детерміновані процеси від абсолютно випадкових. Розрахунки PP припускають, що часовий ряд розбивається на пересічні підвектори довжини \\(d_{E}\\):\n\\[\n\\vec{X}(i) = \\left\\{ x(i), x(i+\\tau), ... , x(i+[d_{E}-1]\\tau) \\right\\},\n\\]\nде часова затримка \\(\\tau\\) відповідає часу розділення між елементами.\nПісля цього кожен вектор представляється у вигляді порядкового шаблону \\(\\pi = \\{ r_0, r_1, ... , r_{d_{E}-1} \\}\\), що має задовільняти наступній умові:\n\\[\nx(i+r_0) \\leq x(i+r_1) \\leq ... \\leq x(i+r_{d_{E}-1}).\n\\]\nЦікава для нас міра незворотності часу на основі PP може бути отримана шляхом врахування їх відносної частоти як для початкового, так і для оберненого часового ряду. Відповідно, якщо обидва типи мають приблизно однакові розподіли ймовірностей своїх патернів, часові ряди представляються зворотними, а для іншого випадку робиться протилежний висновок  [23].\nРізницю між розподілами прямих часових рядів (\\(P^{d}\\)) та зворотних (\\(P^{r}\\)) можна оцінити за допомогою дивергенції Кульбака-Лейблера або Дженсена-Шеннона.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Лабораторна робота № 11</span>"
    ]
  },
  {
    "objectID": "lab_11.html#хід-роботи",
    "href": "lab_11.html#хід-роботи",
    "title": "11  Лабораторна робота № 11",
    "section": "11.2 Хід роботи",
    "text": "11.2 Хід роботи\nПідключаємо необхідні бібліотеки:\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport scienceplots\nimport pandas as pd\nimport yfinance as yf\nimport networkx as nx\nimport neurokit2 as nk\nfrom sklearn import preprocessing\nfrom collections import defaultdict, Counter\nfrom ordpy import ordinal_distribution\nfrom tqdm import tqdm\nfrom scipy.spatial import distance\nfrom ts2vg import NaturalVG, HorizontalVG\n\n%matplotlib inline\n\nІ встановлюємо параметри для побудови графіків:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    'axes.labelsize': 22,             # розмір підписів по осям\n    'legend.fontsize': 22,            # розмір легенди\n    'xtick.labelsize': 22,            # розмір розмітки по осі Ох\n    'ytick.labelsize': 22,            # розмір розмітки по осі Ох\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\nДалі визначаємо функцію для побудови рекурентного графа:\n\ndef recurrence_net(time_ser, rec_thr, dim, tau, dist_type='euclidien'):\n    time_series = nk.complexity_embedding(time_ser, dimension=dim, delay=tau)\n    rp = (distance.cdist(time_series, time_series, dist_type) &lt;= rec_thr).astype(int)\n    adj_matrix_RN = rp\n    np.fill_diagonal(adj_matrix_RN, 0)\n\n    rec_nw = nx.from_numpy_matrix(adj_matrix_RN)\n    \n    return rec_nw\n\ndef node_positions_recurrence_net(ts, xs):\n    return {i: (xs[i], ts[i]) for i in range(len(ts))}\n\nУ подальшому ми представимо індекс Кошти для розрахунку якого визначимо наступну функцію:\n\ndef Costa_1(time_ser, taus):\n    Cst = []\n    for tau in taus:\n        fragm_Costa = np.array([time_ser[tau:], time_ser[:-tau]])\n        DiffCosta = np.diff(fragm_Costa, axis=0)\n        IncCosta = np.sum(DiffCosta&gt;0)\n        DecCosta = np.sum(DiffCosta&lt;0)\n        C = (DecCosta-IncCosta)/(len(time_ser)-tau)\n        Cst.append(C)\n    Costa = np.mean(np.abs(Cst))\n    return Costa\n\n\n11.2.1 Оголошення функцій для підрахунку показників незворотності\nПермутаційна незворотність:\n\ndef PermIrrever(time_ser, d_e, tau, delta=1e-10, distance_irr=\"kullback\"):\n\n    # створення зворотньої версії ряду\n    rev_arr = np.flip(time_ser)\n    \n    # отримання розподілу порядкових шаблонів для вихідного ряду\n    _, dist_dir = ordinal_distribution(time_ser, \n                                        dx=d_e, \n                                        taux=tau, \n                                        return_missing=True)\n    \n    # отримання розподілу порядкових шаблонів для зворотного ряду\n    _, dist_rev = ordinal_distribution(rev_arr,\n                                        dx=d_e,\n                                        taux=tau,\n                                        return_missing=True)\n    \n    if distance_irr == \"kullback\":\n        KLD_perm = dist_dir * np.log((dist_dir + delta) / (dist_rev + delta))\n        return np.sum(KLD_perm)   \n    else:\n        return distance.jensenshannon(dist_dir + delta, dist_rev + delta)\n\nФункція для підрахунку ймовірностей:\n\ndef calc_prob_dist(p, q):\n\n    p_cnt, q_cnt = dict(Counter(p)), dict(Counter(q))\n    p_sum = sum(p_cnt.values())\n    p_dist = {k: v / p_sum for k, v in p_cnt.items()}\n    q_sum = sum(q_cnt.values())\n    q_cnt = {k: v / q_sum for k, v in q_cnt.items()}\n    q_dist = defaultdict(lambda: 0)\n    q_dist.update(q_cnt)\n\n    for k in p_dist.keys():\n        if k not in q_dist.keys():\n            q_dist[k] = 0.0\n\n    for k in q_dist.keys():\n        if k not in p_dist.keys():\n            p_dist[k] = 0.0\n\n    return p_dist, q_dist\n\nДивергеція Кульбака-Лейблера:\n\ndef KLD(p_dist, q_dist, delta=1e-10):\n\n    div_list = [\n        p_proba * np.log((p_proba + delta) / (q_dist[k] + delta)) for k, p_proba in p_dist.items()\n    ]\n    \n    kld = np.sum(np.array(div_list))\n\n    return kld\n\nДивергенція Дженсена-Шеннона:\n\ndef JS(p_dist, q_dist, delta=1e-10):\n\n    m_dist = {k: 0.5*(p_dist[k]+q_dist[k]) for k in p_dist.keys()}\n\n    js = 0.5*KLD(p_dist, m_dist) + 0.5*KLD(q_dist, m_dist)\n\n    return js\n\nГрафо-динамічна незворотність:\n\ndef GraphIrrever(fragm, \n                 graph_type='classic', \n                 delta=1e-10, \n                 d_e_rec=3, \n                 tau_rec=1, \n                 eps_rec=0.1, \n                 dist_rec='chebyshev', \n                 distance_irr='kullback'):\n    \n    # будуємо граф \n    if graph_type == 'classic':\n        g = NaturalVG(directed=None).build(fragm)\n    elif graph_type == 'horizontal':\n        g = HorizontalVG(directed=None).build(fragm)\n    else:\n        g = recurrence_net(fragm, \n                           rec_thr=eps_rec*np.abs(np.std(fragm)), \n                           dim=d_e_rec, \n                           tau=tau_rec, \n                           dist_type=dist_rec)\n    \n    # розраховуємо вхідні та вихідні характеристики \n    adjacency_mat = g.adjacency_matrix()\n    ret_deg, adv_deg = GetDegree(adjacency_mat)\n    ret_clust, adv_clust = GetLocalClusteringCoefficient(adjacency_mat, ret_deg, adv_deg)\n\n    # знаходимо розподіл імовірностей\n    ret_deg_probs, adv_deg_probs = calc_prob_dist(ret_deg, adv_deg)\n    ret_clust_probs, adv_clust_probs = calc_prob_dist(ret_clust, adv_clust)\n\n    # розраховуємо асиметрію (незворотність) за допомогою Кульбака-Лейблера\n    if distance_irr == \"kullback\":\n        distance_deg = KLD(ret_deg_probs, adv_deg_probs, delta)\n        distance_clust = KLD(ret_clust_probs, adv_clust_probs, delta)\n\n    # розраховуємо асиметрію (незворотність) за допомогою Йенсена-Шеннона\n    if distance_irr == \"shannon\":\n        distance_deg = JS(ret_deg_probs, adv_deg_probs, delta)\n        distance_clust = JS(ret_clust_probs, adv_clust_probs, delta)\n      \n    return distance_deg, distance_clust\n\n\n11.2.1.1 Функції для отримання ступеня вершини та локальної кластеризації\nПроцедура знаходження ступеня вершини та локальної кластеризації кожної вершини є доволі громіздкою. Для прискорення розрахунків відповідних процедур скористаємось бібліотекою numba. Numba — це швидкий компілятор для Python, який найкраще працює з кодом, що використовує масиви, функції та цикли NumPy. Найпоширеніший спосіб використання Numba — це колекція декораторів, які можна застосувати до ваших функцій, щоб доручити Numba їх компілювати. Коли здійснюється виклик функції, прикрашеної Numba, вона компілюється у машинний код “just-in-time” для виконання, і весь або частина вашого коду може згодом виконуватися зі швидкістю власного машинного коду!\nВстановити її можна в наступний спосіб:\n\n!pip install numba==0.56.4\n\nNumba надає декілька утиліт для генерації коду, але центральною функцією є декоратор numba.jit(). За допомогою цього декоратора ви можете позначити функцію для оптимізації JIT-компілятором Numba. Різні режими виклику викликають різні варіанти компіляції та поведінки. Імпортуємо відповідний декоратор з бібліотеки numba:\n\nfrom numba import jit\n\n\n@jit(nopython=True, nogil=True) \ndef GetDegree(AM):\n    numNodes = AM.shape[0]\n    retarded_degree = np.zeros((numNodes))\n    advanced_degree = np.zeros((numNodes))\n     \n    for i in range(numNodes):\n        retarded_degree[i] = AM[i, :i].sum()\n\n    for i in range(numNodes):\n        advanced_degree[i] = AM[i, i:].sum()\n        \n    return retarded_degree, advanced_degree\n\n\n@jit(nopython=True, nogil=True) \ndef GetLocalClusteringCoefficient(AM, ret_deg, adv_deg):\n    \n    numNodes = AM.shape[0]\n    retardedCC = np.zeros( (numNodes) )\n    advancedCC = np.zeros( (numNodes) )\n    ret_norm = ret_deg * (ret_deg - 1) / 2\n    adv_norm = adv_deg * (adv_deg - 1) / 2\n    \n    for i in range(numNodes):\n        if ret_norm[i] != 0: \n            counter = 0\n            \n            for j in range(i):\n                for k in range(j): \n                    if AM[i, j] == 1 and AM[j, k] == 1 and AM[k, i] == 1: \n                        counter += 1\n                        \n            retardedCC[i] = counter / ret_norm[i]\n    \n    for i in range(numNodes-2):\n        if adv_norm[i] != 0: \n            counter = 0\n            \n            for j in range(i+1, numNodes):\n                for k in range(i+1, j): \n                    if AM[i, j] == 1 and AM[j, k] == 1 and AM[k, i] == 1: \n                        counter += 1\n                        \n            advancedCC[i] = counter / adv_norm[i]\n                 \n                \n    return retardedCC, advancedCC\n    \n\nРозглянемо можливість використання всіх згаданих показників у якості індикаторів або індикаторів-передвісників кризових явищ. Для прикладу завантажимо часовий ряд фондового індексу Russell 2000:\n\nsymbol = '^RUT'                       # Символ індексу\n\ndata = yf.download(symbol)            # вивантажуємо дані\ntime_ser = data['Adj Close'].copy()   # зберігаємо саме ціни закриття\n\nxlabel = 'time, days'                 # підпис по вісі Ох \nylabel = symbol                       # підпис по вісі Оу\n\ndate_in_num = mdates.date2num(time_ser.index)\n\nnp.savetxt(f'{symbol}_initial_time_series.txt', time_ser.values)\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nУвага\n\n\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance, а із власного файлу. Зрозуміло, що й аналіз результатів, і висновки залежать від того, з яким рядом ми працюємо\n\n\n\n\nsymbol = 'sMpa11'                  # Символ індексу\n\npath = \"databases\\sMpa11.txt\"      # шлях по якому здійснюється зчитування файлу\ndata = pd.read_csv(path,           # зчитування даних \n                   names=[symbol])\ntime_ser = data[symbol].copy()     # копіюємо значення кривої \n                                   # \"напруга-видовження\" до окремої змінної\n\nxlabel = r'$\\varepsilon$'          # підпис по вісі Ох \nylabel = symbol                    # підпис по вісі Оу\n\ndate_in_num = time_ser.index\n\n\nВиведемо досліджуваний ряд:\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\n\n\n\nРис. 11.1: Динаміка щоденних змін індексу Russell 2000\n\n\n\n\n\nКористуючись тими методами, що ми розглянули в попередній лабораторній роботі, побудуємо діаграму Пуанкаре та граф нашого часового ряду. Але, перш за все, для діаграми Пункаре треба знайти стандартизовані прибутковості. Оголосимо функцію transformation(), що прийматиме на вхід часовий сигнал, тип ряду, і повертатиме його перетворення:\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\n\n\n\n11.2.2 Виводимо діаграму Пуанкаре та розраховуємо міри на її основі\nРозглянемо діаграму Пуанкаре для фрагмента прибутковостей часового ряду:\n\nfor_puank = time_ser.copy()\n\ntau_assym = 1   # часова затримка для діаграми Пуанкаре\n\nret_type = 4    # тип ряду: \n                # 1 - вихідний, \n                # 2 - детрендований\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований вихідний часовий ряд\n\nidx_beg = 3000  # кінцевий відлік\nidx_end = 5000  # початковий відлік\n\nfragm = for_puank[idx_beg:idx_end]  # виокремлюємо фрагмент ряду\n\nfor_puank = transformation(fragm, ret_type)\n\n\nfig, ax1 = plt.subplots(1, 1)\n\nax1.scatter(for_puank[:-tau_assym], for_puank[tau_assym:], marker=\"X\", s=180, c=\"g\")\n\nlow_x, high_x = ax1.get_xlim()\nlow_y, high_y = ax1.get_ylim()\nax1.axline([low_x, low_y], [high_x, high_y])\n\nax1.set_aspect('equal', 'box')\nax1.set_xlabel(r'$g(t)$')\nax1.set_ylabel(r'$g(t+\\tau)$') \nax1.set_xlim(left=low_x, right=high_x)\nax1.set_ylim(bottom=low_y, top=high_y)\nplt.locator_params(axis='y', nbins=7)\n\nplt.savefig(f\"Poincare_plot_{symbol}_{tau_assym}_{idx_beg}_{idx_end}.jpg\", bbox_inches=\"tight\")\nplt.show()\n\n\n\n\n\n\n\n\nВиходячи з даної діаграми, можна припустити, що для прибутковостей індексу Russell 2000 спостерігається асиметрія у сторону негативних флуктуацій ряду.\n\n\n11.2.3 Віконна процедура\nВизначаємо функцію для побудови парних графіків:\n\ndef plot_pair(x_values, \n              y1_values,\n              y2_values,  \n              y1_label, \n              y2_label,\n              x_label, \n              file_name, clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(x_values, \n                  y1_values, \n                  \"b-\", label=fr\"{y1_label}\")\n\n    p2, = ax2.plot(x_values,\n                   y2_values, \n                   color=clr, \n                   label=y2_label)\n\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(f\"{y1_label}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', labelrotation=45, **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n    ax2.legend(handles=[p1, p2])\n\n    \n\n    plt.savefig(file_name + \".jpg\")\n        \n    plt.show();\n\nВизначаємо параметри та оголошуємо масиви для збереження результатів:\n\nwindow = 500    # розмір ковзного вікна\ntstep = 1       # часовий крок\n\nret_type = 1    # тип ряду: \n                # 1 - вихідний, \n                # 2 - детрендований\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований вихідний часовий ряд\n\n# параметри для мір асиметрії\ntau_assym = 1                 # часова затримка для діаграми Пуанкаре\ntau_Costa_begin = 1           # початковий часовий масштаб для індексу Кошти\ntau_Costa_end = 20            # кінцевий часовий масштаб для індексу Кошти\ntaus_Costa = np.arange(tau_Costa_begin, tau_Costa_end+1) # формуємо масив масштабів\n                \nlength = len(time_ser)\n\nPIx = []\nGIx = []\nSIx = []\nAIx = []\nEIx = []\nCIx = []\n\nРозраховуємо відповідні міри у віконній процедурі:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    fragm = time_ser.iloc[i:i+window].copy() # відбираємо фрагмент ряду \n    \n    fragm = transformation(fragm, ret_type)\n    \n    Temp_fragm = np.array([fragm[:-tau_assym], fragm[tau_assym:]])\n    \n    T2   = np.transpose(np.arctan(Temp_fragm[1,:]/Temp_fragm[0,:])*180/np.pi)\n    Dup  = abs(np.diff(Temp_fragm[:,T2&gt;45],axis=0))\n    Dtot = abs(np.diff(Temp_fragm[:,T2!=45],axis=0))\n    Sup  = np.sum(abs(T2[T2&gt;45]-45))\n    Stot = np.sum(abs(T2[T2!=45]-45))\n    Aup  = np.sum(abs(np.transpose(((T2[T2&gt;45]-45))*np.sqrt(np.sum(Temp_fragm[:,T2&gt;45]**2,axis=0)))))\n    Atot = np.sum(abs(np.transpose(((T2[T2!=45]-45))*np.sqrt(np.sum(Temp_fragm[:,T2!=45]**2,axis=0)))))\n    Ethird = np.sum(np.transpose(Temp_fragm[0,:]-Temp_fragm[1,:])**3)\n    Etot = (np.sum(np.transpose(Temp_fragm[0,:]-Temp_fragm[1,:])**2))**(3/2)\n    \n    Porta = sum(T2&lt;45)/sum(T2!=45)\n    Gudzik = np.sum(Dup**2)/np.sum(Dtot**2)\n    Slope = Sup/Stot\n    Area = Aup/Atot\n    Eiler = Ethird/Etot\n    Costa = Costa_1(fragm, taus_Costa)\n    \n    PIx.append(Porta)\n    GIx.append(Gudzik)\n    SIx.append(Slope)\n    AIx.append(Area)\n    EIx.append(Eiler)\n    CIx.append(Costa)\n\n100%|██████████| 8692/8692 [00:08&lt;00:00, 973.46it/s] \n\n\nЗберігаємо значення до .txt файлів\n\nnp.savetxt(f\"Porta_idx_{symbol}_{window}_{tstep}_{ret_type}_{tau_assym}.txt\", PIx)\nnp.savetxt(f\"Gudzik_idx_{symbol}_{window}_{tstep}_{ret_type}_{tau_assym}.txt\", GIx)\nnp.savetxt(f\"Slope_idx_{symbol}_{window}_{tstep}_{ret_type}_{tau_assym}.txt\", SIx)\nnp.savetxt(f\"Area_idx_{symbol}_{window}_{tstep}_{ret_type}_{tau_assym}.txt\", AIx)\nnp.savetxt(f\"Eiler_idx_{symbol}_{window}_{tstep}_{ret_type}_{tau_assym}.txt\", EIx)\nnp.savetxt(f\"Costa_idx_{symbol}_{window}_{tstep}_{ret_type}_{tau_assym}.txt\", CIx)\n\n\n\n11.2.4 Візуалізація показників на основі діаграми Пуанкаре\n\n11.2.4.1 Індекс Порти\n\nmeasure_label = r\"$PIx$\"\nfile_name = f\"PIx_{symbol}_{tau_assym}_{window}_{tstep}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          PIx, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"crimson\")\n\n\n\n\n\n\n\nРис. 11.2: Динаміка індексу Russell 2000 та індексу Порти\n\n\n\n\n\nЯк можна бачити з Рис. 11.2, індекс Порти спадає у передкризові періоди, що вказує на переважне зростання частки позитивних прибутковостей у межах вікна в 500 днів. У кризові періоди індекс Порти зростає, що вказує на домінацію негативного тренду (точок нижче головної діагоналі) в цінових флуктуаціях Russell 2000. Видно, що \\(PIx\\) закономірно спадає перед кризами 1997, 2008, 2011, 2015, 2020 років. Тобто, даний показник можна використовувати в якості індикатора-передвісника крахових подій на фондовому ринку.\n\n\n11.2.4.2 Індекс Гузіка\n\nmeasure_label = r\"$GIx$\"\nfile_name = f\"GIx_{symbol}_{tau_assym}_{window}_{tstep}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          GIx, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"crimson\")\n\n\n\n\n\n\n\nРис. 11.3: Динаміка індексу Russell 2000 та індексу Гузіка\n\n\n\n\n\nНа рисунку Рис. 11.3 спостерігається спад індексу Гузіка в кризові та передкризові періоди. Це говорить про те, що сумарна частка точок, яка знаходились вище LI стала меншою. Також варто зазначити, що ці точки в передкризові періоди стають усе ближче до головної діагоналі, тому й сума квадратів відстаней точок вище LI поступово стає меншою. Подібного роду поведінку можна використовувати для передчасної ідентифікації крахових подій на фондовому ринку.\n\n\n11.2.4.3 Індекс кута нахилу\n\nmeasure_label = r\"$SIx$\"\nfile_name = f\"SIx_{symbol}_{tau_assym}_{window}_{tstep}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          SIx, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"crimson\")\n\n\n\n\n\n\n\nРис. 11.4: Динаміка індексу Russell 2000 та індексу кута нахилу\n\n\n\n\n\nНа Рис. 11.4 у закономірний спосіб спостерігається спад індексу кута нахилу в передкрахові періоди Russell 2000. Це говорить про спад сумарної частки точок над LI, які формували певних кут нахилу. У перекризові та кризові періоди вони починають “лягати” на LI, що робить кут нахилу деяких із цих точок близьким до нуля. Подібну закономірність також можна використовувати для передбачення кризових подій.\n\n\n11.2.4.4 Індекс площі секторів\n\nmeasure_label = r\"$AIx$\"\nfile_name = f\"AIx_{symbol}_{tau_assym}_{window}_{tstep}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          AIx, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"crimson\")\n\n\n\n\n\n\n\nРис. 11.5: Динаміка індексу Russell 2000 та індексу площі секторів\n\n\n\n\n\nРис. 11.5 демонструє характерний спад індексу площі в передкрахові періоди Russell 2000. Це говорить про спад сумарної частки точок над LI, які формували площі. У перекризові та кризові періоди вони починають “лягати” на LI, що робить їх результуючу площу близькою до нуля. Як можна бачити, площі секторів точок над LI закономірно спадають у передкрахові періоди найбільш ключових криз на фондовому ринку.\n\n\n11.2.4.5 Індекс Ейлера\n\nmeasure_label = r\"$EIx$\"\nfile_name = f\"EIx_{symbol}_{tau_assym}_{window}_{tstep}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          EIx, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"crimson\")\n\n\n\n\n\n\n\nРис. 11.6: Динаміка індексу Russell 2000 та індексу Ейлера\n\n\n\n\n\nРис. 11.6 показує, що індекс Ейлера зростає під час кризових моментів. Це говорить про зростання різниці між наступними значеннями та попередніми. Даний показник можна пробувати використовувати в якості індикатора, але не передвісника крахових подій.\n\n\n11.2.4.6 Індекс Кошти\n\nmeasure_label = r\"$CIx$\"\nfile_name = f\"CIx_{symbol}_{tau_assym}_{window}_{tstep}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          CIx, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"crimson\")\n\n\n\n\n\n\n\nРис. 11.7: Динаміка індексу Russell 2000 та індексу Кошти\n\n\n\n\n\nНа Рис. 11.7 можна спостерігати зростання \\(CIx\\) у передкризові періоди, що вказує на зростання асиметрії між декрементами та інкриментами в динаміці досліджуваного індексу. Подібну поведінку можна розглядати в якості індикатора незворотності системи до входження у стан краху.\n\n\n\n11.2.5 Побудова графу досліджуваного ряду\n\ngraph_type = 'classic'    # тип графу: classic, horizontal, recurrent\n\n# параметри для рекурентного графу\nd_e_rec = 3 # розмірність вкладень\ntau_rec = 1 # часова затримка\neps_rec = 1.3 # радіус\ndist_rec = 'chebyshev' # відстань між траєкторіями: \n                       # canberra’, ‘chebyshev’, ‘cityblock’, ‘correlation’, \n                       # ‘cosine’, ‘dice’, ‘euclidean’, ‘hamming’, ‘jaccard’, \n                       # ‘jensenshannon’, ‘kulsinski’, ‘kulczynski1’, ‘mahalanobis’, \n                       # ‘matching’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, \n                       # ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’.\n\n\nindex_begin = 2000  # початковий індекс для графу\nindex_end = 4000    # кінцевий індекс для графу\n\nret_type = 1        # вид ряду\n\n\nfor_graph_plot = time_ser.copy()\n\nfor_graph_plot = transformation(for_graph_plot, ret_type)\n\ndate = date_in_num[index_begin:index_end] # вилучаємо необхідні по індексам дати\n\n# будуємо граф у залежності від типу графа\nif graph_type == 'classic':\n    g = NaturalVG(directed=None).build(for_graph_plot[index_begin:index_end], xs=date)\n    pos = g.node_positions()\n    nxg = g.as_networkx()\nelif graph_type == 'horizontal':\n    g = HorizontalVG(directed=None).build(for_graph_plot[index_begin:index_end], xs=date)\n    pos = g.node_positions()\n    nxg = g.as_networkx()\nelse:\n    g = recurrence_net(for_graph_plot[index_begin:index_end], \n                       rec_thr=eps_rec * np.abs(np.std(for_graph_plot[index_begin:index_end])), \n                       dim=d_e_rec, \n                       tau=tau_rec, \n                       dist_type=dist_rec)\n    \n    pos = node_positions_recurrence_net(for_graph_plot[index_begin:index_end], date)\n    nxg = g\n    \n    \n# встановлення параметрів для побудови графів\ngraph_plot_options = {\n    'with_labels': False,\n    'node_size': 2,\n    'node_color': [(0, 0, 0, 1)],\n    'edge_color': [(0, 0, 0, 0.15)],\n}\n\nВиводимо зв’язки видимості:\n\nfig, ax = plt.subplots(1, 1)\n\nnx.draw_networkx(nxg, ax=ax, pos=pos, **graph_plot_options)\nax.tick_params(bottom=True, labelbottom=True, labelrotation=45)\nax.plot(time_ser.index[index_begin:index_end], \n        for_graph_plot[index_begin:index_end], \n        label=fr\"{symbol}\")\n\nax.set_title('Visibility Connections', fontsize=22)\nax.set_xlabel(xlabel)\nax.set_ylabel(fr\"{symbol}\")\nax.set_xlim(time_ser.index[index_begin:index_end][0],\n            time_ser.index[index_begin:index_end][-1])\nax.legend(loc='upper right')\n\nplt.savefig(f\"Time_ser_connections_symbol={symbol}_ \\\n    idx_beg={index_begin}_idx_end={index_end}_sertype={ret_type}_ \\\n    network_type={graph_type}.jpg\", bbox_inches=\"tight\")\n\nplt.show(); \n\n\n\n\n\n\n\n\nВиходячи з графу взятого нами фрагменту видно, що крах поблизу 2000-го року характеризується високою концентрацією вузлів. Це вказує на високий ступінь довготривалої пам’яті для кризових явищ фондового ринку, що в свою чергу впливає і на їх незворотність.\nТепер розглянемо сам граф:\n\npos = nx.spring_layout(nxg, k=0.15, iterations=100)\n# знаходимо вузол близький до центру графа (0.5,0.5)\ndmin = 1\nncenter = 0\nfor n in pos: \n    x, y = pos[n]\n    d = (x - 0.5)**2 + (y - 0.5)**2\n    if d &lt; dmin:\n        ncenter = n\n        dmin = d\n\n# розфарбовуємо в залежності від ступеня вершини\n\np = dict(nx.degree(nxg))\nfig, ax2 = plt.subplots(1, 1)\nax2.set_title('Graph representation')\nnx.draw_networkx_edges(nxg, ax=ax2, pos=pos, nodelist=[ncenter], alpha=0.4,width=0.1)\nnx.draw_networkx_nodes(nxg, ax=ax2, pos=pos, nodelist=list(p.keys()),\n                       node_size=10, edgecolors='r', linewidths=0.01,\n                       node_color=list(p.values()),\n                       cmap=plt.cm.Blues_r)\n        \nvmin = np.asarray(list(p.values())).min()\nvmax = np.asarray(list(p.values())).max()\n\nsm = plt.cm.ScalarMappable(cmap=plt.cm.Blues_r, norm=plt.Normalize(vmin=vmin, vmax=vmax))\ncb = plt.colorbar(sm, ax=ax2)\ncb.set_label('degree')\n\nplt.savefig(f\"Graph_representation_symbol={symbol}_ \\\n            idx_beg={index_begin}_idx_end={index_end} \\\n            _sertype={ret_type}_network_type={graph_type}.jpg\", bbox_inches=\"tight\")\nplt.show(); \n\n\n\n\n\n\n\n\n\n\n11.2.6 Побудова показників незворотності на основі пермутаційних шаблонів та графів\nІніціалізуємо масиви для збереження результатів розрахунків:\n\nwindow = 500      # ширина вікна\ntstep = 1         # часовий крок\n\nret_type = 1      # вид ряду: \n                  # 1 - вихідний\n                  # 2 - детрендований (різниця між теп. значенням та попереднім)\n                  # 3 - прибутковості \n                  # 4 - стандартизовані прибутковості \n                  # 5 - абсолютні значення (волатильності)\n                  # 6 - стандартизований ряд   \n\n# параметри для рекурентного графу\nd_e_rec = 3            # розмірність вкладень\ntau_rec = 1            # часова затримка\neps_rec = 1.3          # радіус\n\ndist_rec = 'chebyshev' # відстань між траєкторіями: \n                       # canberra’, ‘chebyshev’, ‘cityblock’, ‘correlation’, \n                       # ‘cosine’, ‘dice’, ‘euclidean’, ‘hamming’, ‘jaccard’, \n                       # ‘jensenshannon’, ‘kulsinski’, ‘kulczynski1’, ‘mahalanobis’, \n                       # ‘matching’, ‘minkowski’, ‘rogerstanimoto’, ‘russellrao’, ‘seuclidean’, \n                       # ‘sokalmichener’, ‘sokalsneath’, ‘sqeuclidean’, ‘yule’.\n\n# параметри для мір незворотності\nd_e_perm = 3              # розмірність вкладень для пермутаційних патернів \ntau_perm  = 1             # часова затримка для пермутаційних патернів\ndistance_irr = 'kullback' # відстань між розподілами: kullback, shannon\ngraph_type = 'classic'    # тип графу: classic, horizontal, recurrent\n\nlength = len(time_ser.values)  # довжина самого ряду\n\nDegree = []\nClust = []\nPerm = []\n\nРозпочинаємо процедуру рухомого вікна:\n\nfor i in tqdm(range(0,length-window,tstep)):\n    fragm = time_ser.iloc[i:i+window].copy() # відбираємо фрагмент ряду\n\n    fragm = transformation(fragm, ret_type)  # виконуємо перетворення\n        \n    deg, clust = GraphIrrever(fragm, \n                            graph_type=graph_type, \n                            delta=1e-10, \n                            d_e_rec=d_e_rec, \n                            tau_rec=tau_rec, \n                            eps_rec=eps_rec, \n                            dist_rec=dist_rec, \n                            distance_irr=distance_irr)  \n    \n    perm = PermIrrever(fragm, \n                        d_e=d_e_perm, \n                        tau=tau_perm, \n                        delta=1e-10, \n                        distance_irr=distance_irr)\n      \n    Degree.append(deg)\n    Clust.append(clust)\n    Perm.append(perm)\n\n100%|██████████| 8692/8692 [01:58&lt;00:00, 73.50it/s]\n\n\nЗберігаємо результати до .txt файлів\n\nnp.savetxt(f\"{distance_irr}_deg_symbol={symbol}_wind={window} \\\n            _step={tstep}_ret_type={ret_type}_graph_type={graph_type}.txt\", Degree)\nnp.savetxt(f\"{distance_irr}_clust_symbol={symbol}_wind={window} \\\n            _step={tstep}_ret_type={ret_type}_graph_type={graph_type}.txt\", Clust)\nnp.savetxt(f\"{distance_irr}_perm_symbol={symbol}_wind={window} \\\n            _step={tstep}_ret_type={ret_type}_d_e={d_e_perm}_tau={tau_perm}.txt\", Perm)\n\n\n\n11.2.7 Візуалізація показників на основі графів та пермутаційних шаблонів\n\n11.2.7.1 Ступінь незворотності на основі ступеня вершини\n\nmeasure_label = r\"$Dist_{deg}$\"\nfile_name = f\"Degree_symbol={symbol}_wind={window}_ \\\n            step={tstep}_ret_type={ret_type} \\\n            _graph_type={graph_type}_dist={distance_irr}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          Degree, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"darkgreen\")\n\n\n\n\n\n\n\nРис. 11.8: Динаміка індексу Russell 2000 та показника незворотності на основі ступеня вершини\n\n\n\n\n\nПоказник незворотність на основі ступенів вершини починає зростати в передкризові періоди, вказуючи на зростання асиметрії вхідних та вихідних ступенів вершини в предкризові періоди. У момент самої кризи можна спостерігати зростання симетрії цих характеристик.\n\n\n11.2.7.2 Ступінь незворотності на основі показника локальної кластеризації\n\nmeasure_label = r\"$Dist_{clust}$\"\nfile_name = f\"Clust_symbol={symbol}_wind={window}_ \\\n            step={tstep}_ret_type={ret_type} \\\n            _graph_type={graph_type}_dist={distance_irr}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          Clust, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"darkgreen\")\n\n\n\n\n\n\n\nРис. 11.9: Динаміка індексу Russell 2000 та показника незворотності на основі локальної кластеризації\n\n\n\n\n\nНа Рис. 11.9 видно, що в передкризові та кризові періоди незворотність системи зростає, що відображається асиметрією розподілом локальних коефіцієнтів кластеризації вхідних і вихідних вершин.\n\n\n11.2.7.3 Ступінь незворотності на основі пермутаційних шаблонів\n\nmeasure_label = r\"$Dist_{perm}$\"\nfile_name = f\"Perm_symbol={symbol}_wind={window}_ \\\n            step={tstep}_ret_type={ret_type} \\\n            d_e={d_e_perm}_tau={tau_perm} \\\n            _dist={distance_irr}\"\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          Perm, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name, \n          clr=\"darkgreen\")\n\n\n\n\n\n\n\nРис. 11.10: Динаміка індексу Russell 2000 та показника незворотності на основі пермутаційних шаблонів\n\n\n\n\n\nРис. 11.10 демонструє зростання \\(Dist_{perm}\\) у передкризові періоди, що слугує індикатором біфуркації ринку та початку незворотніх трансформацій. Найвищий ступінь незворотності спостерігався саме напередодні кризи 1997 року. Пермутаційна незворотність закономірно зростає напередодні майже всіх фондових криз, але подальша динаміка \\(Dist_{perm}\\) не перевищує зростання напередодні початку нового століття.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Лабораторна робота № 11</span>"
    ]
  },
  {
    "objectID": "lab_11.html#висновок",
    "href": "lab_11.html#висновок",
    "title": "11  Лабораторна робота № 11",
    "section": "11.3 Висновок",
    "text": "11.3 Висновок\nУ даній роботі було розглянуто показники незворотності (асиметрії) системи на основі діаграм Пуанкаре, графу видимості та пермутаційних шаблонів. Було продемонстровано побудову діграми Пуанкаре та зв’язків видимості як для всього ряду, так і для деяких із його фрагментів. Видно, що значення на діаграмі Пуанкаре характеризуються розподілом точок, що виходять за межі нормального Гаусового розподілу. Для графу видимості видно, що кризові стани характеризуються значною концентрацією зв’язків, що є довгостроковими. Таким чином, поставало актуальним проводити розрахунок показників незворотності графового типу для вихідного ряду.\nПоказники на основі діаграми Пуанкаре демонструють зріст або спад у кризові періоди, що вказує на зростання незворотності (асиметрії) системи в дані періоди часу. Дані показники можуть слугувати в якості індикаторів кризових явищ.\nВиходячи з 3 показників незворотності, що представлені вище, видно, що дані показники починають зростати в передкризові періоди або прямо в момент кризи, вказуючи на стартовий період хаосу. Найгірше серед них себе поводить \\(Dist_{deg}\\). Найращим чином \\(Dist_{clust}\\) та \\(Dist_{perm}\\). Їх і варто використовувати в якості індикаторів-передвісників крахів.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Лабораторна робота № 11</span>"
    ]
  },
  {
    "objectID": "lab_11.html#завдання-для-самостійної-роботи",
    "href": "lab_11.html#завдання-для-самостійної-роботи",
    "title": "11  Лабораторна робота № 11",
    "section": "11.4 Завдання для самостійної роботи",
    "text": "11.4 Завдання для самостійної роботи\n\nВиберіть часовий ряд згідно вашого варіанту\nВиділіть на вашому ряді стани стабільності та криз і проаналізуйте їх із використанням індексів асиметрії та незворотності\n\nПроведіть варіацію часового вікна та кроку. Які висновки можна зробити?\n\n\n\n\n\n[1] I. Prigogine and E. N. Hiebert, From Being to Becoming: Time and Complexity in the Physical Sciences, Physics Today 35, 69 (1982).\n\n\n[2] M. Costa, A. L. Goldberger, and C.-K. Peng, Multiscale Entropy Analysis of Biological Signals, Phys. Rev. E 71, 021906 (2005).\n\n\n[3] M. D. Costa, C.-K. Peng, and A. L. Goldberger, Multiscale Analysis of Heart Rate Dynamics: Entropy and Time Irreversibility Measures, Cardiovascular Engineering 8, 88 (2008).\n\n\n[4] J. F.Donges, R. V. Donner, and J. Kurths, Testing Time Series Irreversibility Using Complex Network Methods, Europhysics Letters 102, 10004 (2013).\n\n\n[5] M. Zanin, A. Rodríguez-González, E. Menasalvas Ruiz, and D. Papo, Assessing Time Series Reversibility Through Permutation Patterns, Entropy 20, (2018).\n\n\n[6] R. Flanagan and L. Lacasa, Irreversibility of Financial Time Series: A Graph-Theoretical Approach, Physics Letters A 380, 1689 (2016).\n\n\n[7] A. Puglisi and D. Villamaina, Irreversible Effects of Memory, Europhysics Letters 88, 30004 (2009).\n\n\n[8] C. Diks, J. C. van Houwelingen, F. Takens, and J. DeGoede, Reversibility as a Criterion for Discriminating Time Series, Physics Letters A 201, 221 (1995).\n\n\n[9] C. S. Daw, C. E. A. Finney, and M. B. Kennel, Symbolic Approach for Measuring Temporal “Irreversibility”, Phys. Rev. E 62, 1912 (2000).\n\n\n[10] P. Guzik, J. Piskorski, T. Krauze, A. Wykretowicz, and H. Wysocki, Heart Rate Asymmetry by Poincaré Plots of RR Intervals, Biomedical Engineering / Biomedizinische Technik 51, 272 (2006).\n\n\n[11] A. Porta, S. Guzzetti, N. Montano, T. Gnecchi-Ruscone, R. Furlan, and A. Malliani, Time Reversibility in Short-Term Heart Period Variability, in 2006 Computers in Cardiology (2006), pp. 77–80.\n\n\n[12] L. Lacasa, A. Nuñez, É. Roldán, J. M. R. Parrondo, and B. Luque, Time Series Irreversibility: A Visibility Graph Approach, The European Physical Journal B 85, (2012).\n\n\n[13] A. O. Bielinskyi, S. V. Hushko, A. V. Matviychuk, O. A. Serdyuk, S. O. Semerikov, and V. N. Soloviev, Irreversibility of Financial Time Series: A Case of Crisis, in Proceedings of the Selected and Revised Papers of 9th International Conference on Monitoring, Modeling & Management of Emergent Economy (M3E2-MLPEED 2021), Odessa, Ukraine, May 26-28, 2021, edited by A. E. Kiv, V. N. Soloviev, and S. O. Semerikov, Vol. 3048 (CEUR-WS.org, 2021), pp. 134–150.\n\n\n[14] A. Kiv, A. Bryukhanov, A. Bielinskyi, V. Soloviev, T. Kavetskyy, D. Dyachok, I. Donchev, and V. Lukashin, Irreversibility of Plastic Deformation Processes in Metals, in Information Technology for Education, Science, and Technics, edited by E. Faure, O. Danchenko, M. Bondarenko, Y. Tryus, C. Bazilo, and G. Zaspa (Springer Nature Switzerland, Cham, 2023), pp. 425–445.\n\n\n[15] M. Costa, A. L. Goldberger, and C.-K. Peng, Broken Asymmetry of the Human Heartbeat: Loss of Time Irreversibility in Aging and Disease, Phys. Rev. Lett. 95, 198102 (2005).\n\n\n[16] C. L. Ehlers, J. Havstad, D. Prichard, and J. Theiler, Low Doses of Ethanol Reduce Evidence for Nonlinear Structure in Brain Activity, The Journal of Neuroscience 18, 7474 (1998).\n\n\n[17] C. Yan, P. Li, L. Ji, L. Yao, C. Karmakar, and C. Liu, Area Asymmetry of Heart Rate Variability Signal, BioMedical Engineering OnLine 16, (2017).\n\n\n[18] C. K. Karmakar, A. Khandoker, and M. Palaniswami, Phase Asymmetry of Heart Rate Variability Signal, Physiological Measurement 36, 303 (2015).\n\n\n[19] B. Luque, L. Lacasa, F. Ballesteros, and J. Luque, Horizontal Visibility Graphs: Exact Results for Random Time Series, Phys. Rev. E 80, 046103 (2009).\n\n\n[20] L. Lacasa and R. Flanagan, Time Reversibility from Visibility Graphs of Nonstationary Processes, Phys. Rev. E 92, 022817 (2015).\n\n\n[21] I. Grosse, P. Bernaola-Galván, P. Carpena, R. Román-Roldán, J. Oliver, and H. E. Stanley, Analysis of Symbolic Sequences Using the Jensen-Shannon Divergence, Physical Review E 65, (2002).\n\n\n[22] C. Bandt and B. Pompe, Permutation Entropy: A Natural Complexity Measure for Time Series, Phys. Rev. Lett. 88, 174102 (2002).\n\n\n[23] A. O. Bielinskyi, A. E. Kiv, Y. O. Prikhozha, M. A. Slusarenko, and V. N. Soloviev, Complex Systems and Physics Education, in Proceedings of the 9th Workshop on Cloud Technologies in Education, CTE 2021, Kryvyi Rih, Ukraine, December 17, 2021, edited by A. E. Kiv, S. O. Semerikov, and M. P. Shyshkina, Vol. 3085 (CEUR-WS.org, 2021), pp. 56–80.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Лабораторна робота № 11</span>"
    ]
  },
  {
    "objectID": "lab_12.html",
    "href": "lab_12.html",
    "title": "12  Лабораторна робота № 12",
    "section": "",
    "text": "12.1 Теоретичні відомості",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Лабораторна робота № 12</span>"
    ]
  },
  {
    "objectID": "lab_12.html#теоретичні-відомості",
    "href": "lab_12.html#теоретичні-відомості",
    "title": "12  Лабораторна робота № 12",
    "section": "",
    "text": "12.1.1 Імпортуємо необхідні бібліотеки\n\n\n\n\n\n\nУвага\n\n\n\nПерш за все, для подальшої роботи, нам необхідно буде встановити бібліотеку pylevy. Встановити її можна з відповідного GitHub репозиторію. Ми у свою чергу завантажимо її напряму через команду pip install слідуючим чином:\n\n\n\n!pip install git+https://github.com/josemiotto/pylevy.git\n\nПісля встановлення необхідного модулю можна приступати до подальшої роботи.\n\nimport numpy as np                 # бібліотека для роботи з масивами чисел\nimport matplotlib.pyplot as plt    # бібліотека для побудови графіків\nimport yfinance as yf              # бібліотека для зчитування фінансових даних з Yahoo Finance\nimport levy                        # бібліотека для роботи з альфа-стабільним розподілом Леві\nimport pandas as pd                # бібліотека для фільтрації даних та їх обробки\nimport scienceplots\nfrom scipy.stats import norm, laplace  # бібліотека для побудови теоретичного розподілу Гауса\n                                       # та Лапласа\nfrom tqdm import tqdm                  # бібліотека для виводу шкали завантаження\n\n%matplotlib inline\n\n\n\n12.1.2 Виконуємо налаштування рисунків\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    'axes.labelsize': 22,             # розмір підписів по осям\n    'legend.fontsize': 22,            # розмір легенди\n    'xtick.labelsize': 22,            # розмір розмітки по осі Ох\n    'ytick.labelsize': 22,            # розмір розмітки по осі Ох\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\n\n\n12.1.3 Зчитування даних\n\ndf = pd.read_csv('databases\\^spx_d.csv')\ndf.set_index('Date', inplace=True)\ndf.index = pd.to_datetime(df.index)\ndf = df[df.index &gt;= '1950-01-01']\n\n\n\n12.1.4 Важкі хвости та Чорні понеділки\nСпочатку ми розглянемо розподіл щоденних прибутковостей індексу S&P 500, починаючи з 1950 року. Нижче буде показано Pandas dataframe df, що містить дані OHLC з файлу (ці дані були попередньо завантажені).\n\ndf\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n1950-01-03\n16.66\n16.66\n16.66\n16.66\n7.000000e+05\n\n\n1950-01-04\n16.85\n16.85\n16.85\n16.85\n1.050000e+06\n\n\n1950-01-05\n16.93\n16.93\n16.93\n16.93\n1.416667e+06\n\n\n1950-01-06\n16.98\n16.98\n16.98\n16.98\n1.116667e+06\n\n\n1950-01-07\n17.09\n17.09\n17.09\n17.09\n1.116667e+06\n\n\n...\n...\n...\n...\n...\n...\n\n\n2023-04-10\n4085.20\n4109.50\n4072.55\n4109.11\n1.951642e+09\n\n\n2023-04-11\n4110.29\n4124.26\n4102.61\n4108.94\n2.000949e+09\n\n\n2023-04-12\n4121.72\n4134.37\n4086.94\n4091.95\n2.249009e+09\n\n\n2023-04-13\n4100.04\n4150.26\n4099.40\n4146.22\n2.198579e+09\n\n\n2023-04-14\n4140.11\n4163.19\n4113.20\n4137.64\n2.088609e+09\n\n\n\n\n18527 rows × 5 columns\n\n\n\n\nЦей історичний ряд щоденних цін містить 18527 торгових днів. Замість того, щоб дивитися безпосередньо на ціни, ми розглянемо щоденні log-прибутковості індексу S&P 500. Пам’ятайте, що логарифмічна прибутковість \\(r_t\\) визначається як логарифм відношення між цінами закриття \\(p_t\\) у теперішній момент часу та попередній \\(p_{t-1}\\):\n\\[\nr_t = \\log p_t - \\log p_{t-1}.\n\\]\nЛогарифмічні прибутковості характеризуються симетрією, тобто якщо в один день індекс втрачає \\(r_t = -0.1\\), а на наступний день індекс набирає \\(r_{t+1} = +0.1\\), індекс знову досягає початкової ціни. Зі звичайними прибутковостями, спочатку втративши \\(10\\%\\), а потім знову набравши \\(10\\%\\), ви не повернетесь до початкової ціни. Ми зберігаємо прибутковості в новому фреймі даних lr:\n\nlr = np.log(df['Close']).diff().dropna()\n\n\nlr.plot()\nplt.show();\n\n\n\n\n\n\n\nРис. 12.1: Динаміка логарифмічних прибутковостей індексу S&P 500\n\n\n\n\n\nВикористовуючи вбудовані функції Pandas та Numpy, тепер ми можемо подивитись на найгірше щоденнє значення прибутковостей. Ми виявляємо, що найгірше значення становило \\(-0.229\\), що відповідає \\(-22.9\\%\\), і що це сталося в понеділок, 19 жовтня 1987 року — так званий Чорний понеділок.\n\n# найгірший щоденні прибутковості від закриття до закриття\nlr.min()\n\n-0.2289972265656708\n\n\n\n# відповідні арифметичні прибутковості\nnp.exp(lr.min()) - 1\n\n-0.20466926070038938\n\n\n\n# індекс найгіршого значення\nnp.argmin(lr)\n\n9583\n\n\n\ndf.iloc[9583:9585]\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\n\n\nDate\n\n\n\n\n\n\n\n\n\n1987-10-16\n298.08\n298.92\n281.52\n282.70\n188055556.0\n\n\n1987-10-19\n282.70\n282.70\n224.83\n224.84\n335722222.0\n\n\n\n\n\n\n\n\nДалі ми будуємо графік вибіркового розподілу щоденних логарифмічних прибутковостей, що демонструє нам, як часто ми спостерігаємо щоденні прибутковості певного знака та величини. Зверніть увагу, що ми встановили логарифмічне масштабування для вісі Оy, щоб мати краще представлення про хвости розподілу, тобто дуже великі, але рідкісні негативні та позитивні прибутковості — крахи та підйоми:\n\nplt.figure()\nplt.hist(lr, 100, density=True)\nplt.yscale('log')\nplt.ylabel(r'$P(r_t)$')\nplt.xlabel(r'$r_t$')\nplt.xlim([-0.25, 0.125])\nplt.ylim([0.011, 99])\nplt.annotate('Чорний понеділок (1987)', [-0.226, 0.02], xytext=[-0.215, 0.15],\n             arrowprops=dict(facecolor='0.4', width=1, headwidth=6, headlength=6),\n             ha='left', va='bottom', fontsize=15)\nplt.show();\n\n\n\n\n\n\n\nРис. 12.2: Розподіл імовірностей логарифмічних прибутковостей S&P 500 в напів- логарифмічному масштабі\n\n\n\n\n\nЯк ми можемо описати погіршення розподілу прибутковості? За допомогою логарифмічного масштабування осі Oy, щоб експоненціальний спад виглядав лінійно, тому ми могли би запідозрити, що при малих абсолютних значеннях прибутковості розподіл зменшується експоненціально, але в міру збільшення абсолютних значень прибутковостей ми все більше і більше відхиляємося від початкового нахилу. Насправді, ми можемо показати, що ні розповсюджений розподіл Гауса (так званий нормальний розподіл), ні розподіл Лапласа з його експоненціальним спадом з обох сторін точно не можуть змоделювати розподіл прибутковостей S&P 500. Як розподіл Гауса, так і розподіл Лапласа недооцінюють ймовірність екстремальних подій — підйомів та крахів:\n\n# підганяємо розподіл Гауса та Лапласа під дані\nnorm_loc, norm_scale = norm.fit(lr)\nlaplace_loc, laplace_scale = laplace.fit(lr)\n\n# генеруємо значення (x,y) з використанням отриманих параметрів розподілів\nx_fit = np.linspace(-0.25, 0.125, 1000)\ny_fit_norm = norm(loc=norm_loc, scale=norm_scale).pdf(x_fit)\ny_fit_laplace = laplace(loc=laplace_loc, scale=laplace_scale).pdf(x_fit)\n\n\nplt.figure()\nplt.hist(lr, 100, density=True)\nplt.plot(x_fit, y_fit_laplace, lw=3, c='C3', alpha=0.8, label='Лаплас')\nplt.plot(x_fit, y_fit_norm, lw=3, c='0.2', alpha=0.8, label='Гаусіан')\nplt.yscale('log')\nplt.ylabel(r'$P(r_t)$')\nplt.xlabel(r'$r_t$')\nplt.xlim([-0.25, 0.125])\nplt.ylim([0.011, 99])\nplt.legend()\n\nplt.annotate('Чорний понеділок (1987)', [-0.226, 0.02], xytext=[-0.215, 0.15],\n             arrowprops=dict(facecolor='0.4', width=1, headwidth=6, headlength=6),\n             ha='left', va='bottom', fontsize=15)\n\nplt.show();\n\n\n\n\n\n\n\nРис. 12.3: Розподіл імовірностей логарифмічних прибутковостей S&P 500 у порівнянні з розподілами Гауса й Лапласа в напів- логарифмічному масштабі\n\n\n\n\n\n\n\n12.1.5 Відхилення в 23 сигма\nЯк ми можемо бачити вище, як розподіл Гауса, так і розподіл Лапласа різко недооцінюють ймовірність дуже волатильної динаміки! Часто зазначають, що трапилась “3-сигма подія на ринку” або щось подібне, тому що багатьом людям подобається оцінювати індивідуальні прибутковості по стандартному відхиленню (зване “сигмою”) підібраного гаусового розподілу. Однак, якщо Розподіл Гауса взагалі не відповідає розподілу вибірки, як у нашому випадку, оцінка краху в одиницях “сигма” може ввести в оману! Щоб показати це, ми спочатку перевіримо, на скільки сигм ринок змістився в Чорний понеділок 1987 року:\n\nnp.abs(np.min(lr))/np.std(lr, ddof=1)\n\n22.969393251617504\n\n\nВиходячи з розподілу Гауса, Чорний понеділок був би подією з “23 сигмами”! Тепер розподіл Гауса чітко говорить нам, як часто X-сигма подія має відбуватися:\n\n1-сигма: прибл. 1 з 3 днів\n2-сигма: прибл. 1 з 22 днів\n3-сигма: прибл. 1 з 370 днів\n\n…\nМи можемо розширити цю таблицю, попрацювавши з функцією кумулятивного розподілу, тісно пов’язана з функцією виживання, і перемістивши обчислення в логарифмічний простір для чисельної стабільності, як демонструють наступні комірки коду:\n\n# у середньому, як багато днів між 1-сигма подіями?\n1/(2*(1. - norm.cdf(1.)))\n\n3.1514871875343764\n\n\n\n# у середньому, як багато днів між 2-сигма подіями?\n1/(2*(1. - norm.cdf(2.)))\n\n21.97789450799283\n\n\n\n# у середньому, як багато днів між 3-сигма подіями?\n1/(2*(1. - norm.cdf(3.)))\n\n370.3983473449564\n\n\n\n# те саме, що й вище, але із використанням функції виживання\n1/(2*norm.sf(3.))\n\n370.39834734495923\n\n\n\n# те саме, що й вище, але із використанням логарифмічної функції функції виживання для чисельної стабільності\nnp.exp(-norm.logsf(3.) - np.log(2.))\n\n370.398347344959\n\n\n\n# те саме, що й вище, але із використанням логарифму з основою 10 для замість натурального логарифму для кращої інтерпретації\n10**(-norm.logsf(3.)/np.log(10) - np.log10(2.))\n\n370.3983473449588\n\n\n\n# як багато днів (log10 значення) між 23-сигма подіями, в середньому?\n(-norm.logsf(23.)/np.log(10) - np.log10(2.))\n\n116.33149536636726\n\n\nЩоб отримати додаткову інформацію про частоту екстремальних прибутковостей, ми можемо навіть створити невеликий інтерактивний віджет за допомогою повзунка, який вказує кількість “сигм”, а текст поруч із ним показує середню кількість днів між двома такими екстремальними подіями:\n\nfrom ipywidgets import IntSlider, interact\n\ndef years_between(X):\n    log10_days = (-norm.logsf(X)/np.log(10) - np.log10(2.))\n    days = int(10**log10_days)\n    return f'У середньому, між двома X-сигма подіями, варто очікувати {days} дні(в), (день).'\n\ninteract(years_between, X=IntSlider(min=1, max=23, step=1, value=1));\n\n\n\n\n\nnp.log10(float(214533622638557983431869220329015643677794364342592664885632499326649056583365324378194575664549945880382591666749440))\n\n116.33149536636726\n\n\nВи бачите, що при переміщенні повзунка вправо кількість днів або років між двома наступними “X-сигма” подіями швидко наростає. Наша таблиця тепер виглядає наступним чином:\n\n1-сигма: прибл. 1 з 3 днів\n2-сигма: прибл. 1 з 22 днів\n3-сигма: прибл. 1 з 370 днів\n\n…\n\n23-сигма: приблизно 1 з \\(10^{116}\\) днів (!!!)\n\nВнесемо ясність: виходячи з розподілу Гауса, щоденні втрати Чорної п’ятниці 1987 року повинні бути подією, яку ми очікуємо раз на \\(10^{116}\\) днів. Це число в значній мірі незбагненно велике. Якби S&P 500 почав торгуватися відразу після народження Всесвіту, це становило б лише \\(10^{13}\\) торгових днів. Через \\(10^{116}\\) днів усі зірки у Всесвіті давно згорять, навіть усі чорні діри випаруються і Всесвіт стане темним і порожнім місцем. Тож або ми повинні бути дуже щасливі, що єдина очікувана Чорна п’ятниця в історії та майбутньому Всесвіту позаду, або ви дійсно ніколи не повинні використовувати розподіл Гауса для моделювання прибутковості акцій! Незалежно від ринку та деталей, якщо хтось говорить про події з 10 сигмами або 23 сигмами, він, безумовно, використовує неправильну модель, оскільки шанс спостерігати таку подію в нашому житті незначний.\nОтже, виникає питання: як ми справляємось із такими екстремальними значеннями? Чи слід позначати їх як викиди або артефакти, щоб наші фінансові моделі краще описували більшість значень? Звичайно, ні, оскільки результат наших інвестицій може критично залежати не від більшості менших прибутковостей, а саме від таких екстремальних подій! Далі ми познайомимося з підходом, який може пояснити та екстраполювати за межі екстремальних подій, таких як Чорний понеділок. Але ми також побачимо, що не всі способи врахування важких хвостів розподілів працюють добре, оскільки деякі моделі, такі як GARCH може заколисати нас помилковим почуттям безпеки, коли ми завжди беремо до уваги минулі екстремальні події, але завжди дивуємося новим.\n\n\n12.1.6 Статистика степеневого розподілу\nТепер, коли ми знаємо, що розподіл Гауса не є хорошим вибором, який тип розподілу насправді може описати частоту екстремальних подій, що струшують S&P 500? Частота екстремально позитивних і негативних прибутковостей асиметрична для більшості фінансових активів, оскільки крахи, як правило, більш сильні, ніж підйоми. Ось чому нижче ми зосередимося на лівому (негативному) хвості розподілу прибутковостей.\nОскільки нас цікавлять лише екстремальні події, ми розглянемо лише ті прибутковості, які менше \\(-0.03\\) (приблизно три стандартних відхилення від середнього значення). Нижче ми візуалізуємо ці екстремально негативні показники прибутковостей у логарифмічному масштабі (тут ми використовуємо абсолютні значення негативних прибутковостей). Як вісь Oy (яка показує частоту логарифмічних прибутковостей), так і вісь Ox (яка показує величину логарифмічних прибутковостей) масштабуються логарифмічно:\n\n# хвіст починається приблизно при трьох стандартних відхиленнях від середнього\nnp.mean(lr) - 3*np.std(lr)\n\n-0.029610514264246313\n\n\n\nneg_lr = -lr[lr &lt; 0]\n\nplt.figure()\nhist = plt.hist(neg_lr, bins=np.logspace(np.log10(0.03),np.log10(0.3), 20), density=True)\nplt.yscale(\"log\")\nplt.xscale(\"log\")\nplt.ylabel(r'$\\log[P(|r_t|)]$')\nplt.xlabel(r'$\\log[|r_t|]$')\n\nplt.ylim([0.1, 110])\nplt.xlim([0.03, 0.3])\nplt.xticks([0.03, 0.04, 0.05, 0.06, 0.1, 0.2, 0.3], [0.03, 0.04, 0.05, 0.06, 0.1, 0.2, 0.3], rotation=35)\nplt.show();\n\n\n\n\n\n\n\nРис. 12.4: Розподіл імовірностей щоденних негативних абсолютних прибутковостей у подвійному логарифмічному масштабі\n\n\n\n\n\nЯк ми бачимо, ця гістограма екстремальних абсолютних прибутковостей зменшується приблизно лінійно при логарифмічному масштабуванні. Щоразу, коли ви виявляєте пряму лінію на логарифмічному графіку, це вказує на так званий степеневий розподіл. Степеневе співвідношення ймовірності спостереження великих абсолютних логарифмічних прибутковостей задається у вигляді:\n\\[\np(|r_t|) = c \\cdot |r_t|^{-\\alpha}.\n\\]\nКонстанта \\(c\\) на даний момент нас не надто турбує, вона просто гарантує, що ліва частина рівняння належним чином нормалізована, але показник \\(\\alpha\\) представляє найбільший інтерес, оскільки він говорить, як працює степеневий закон. Якщо ви знаєте частоту даних абсолютних логарифмічних прибутковостей, то степеневий закон підкаже, у скільки разів менш імовірними були б логарифмічні прибутковості подвоєного розміру:\n\\[\np(2 \\cdot |r_t|) = c \\cdot (2 \\cdot |r_t|)^{-\\alpha} = 2^{-\\alpha} \\cdot c \\cdot |r_t|^{-\\alpha} = 2^{-\\alpha} \\cdot p(|r_t|),\n\\]\nабо простіше:\n\\[\np(2 \\cdot |r_t|) \\big/ p(|r_t|) = 2^{-\\alpha}.\n\\]\nДля \\(\\alpha = 0\\) всі прибутковості однаково ймовірні, для \\(\\alpha = 2\\) подвоєння прибутковостей робить їх у 4 рази менш імовірними. Це правило подвоєння вгору або вниз не залежить від значення самого \\(\\left| r_t \\right|\\), але працює для всіх значень \\(\\left| r_t \\right|\\). Степеневе співвідношення також називають “безмасштабним” або масштабно-інваріантним.\nЯк щодо показника \\(\\alpha\\) для наших надзвичайно негативних логарифмічних прибутковостей S&P 500? Ми можемо легко оцінити його за даними, але є деякі тонкощі у правильному розбитті вищезазначеної гістограми, які можуть вплинути на нашу оцінку (див., наприклад,  [1]). Щоб обійти ці проблеми з розбиттям, можемо замість цього оцінити степеневий показник кумулятивної гістограми логарифмічних прибутковостей, а потім відняти одиницю від отриманого нахилу:\n\n# підганяємо лінію регресії до кумулятивної гістограми (log10 для ймовірності) vs. лог-прибутковості (log10)\nsorted_neg_lr = np.sort(neg_lr.values)\ncumulative_probability = np.linspace(1, 0, len(sorted_neg_lr)+1)[:-1]\n\nx_min = 0.03\nmask = sorted_neg_lr &gt;= x_min\nm, b = np.polyfit(np.log10(sorted_neg_lr[mask]), np.log10(cumulative_probability[mask]), 1)\nalpha = -(m - 1)\n\nplt.figure()\nplt.plot(np.log10(sorted_neg_lr[mask]), np.log10(cumulative_probability[mask]))\nplt.scatter(np.log10(sorted_neg_lr[mask]), np.log10(cumulative_probability[mask]), label='sample data')\n\nx_fit = np.linspace(np.log10(x_min), np.log10(1.1*np.max(neg_lr)), 100)\ny_fit = m*x_fit + b\n\nplt.plot(x_fit, y_fit, label='степенева підгонка')\nplt.title(f'розрахований степеневий показник $\\\\alpha={alpha:.2f}$')\nplt.ylabel(r'$\\log[P(|r_t| \\geq r_{min})]$')\nplt.xlabel(r'$\\log[|r_t|]$')\nplt.show();\n\n\n\n\n\n\n\nРис. 12.5: Кумулятивний розподіл імовірностей негативних абсолютних прибутковостей у подвійному логарифмічному масштабі й лінія регресії з якої отримується кут нахилу даного розподілу\n\n\n\n\n\nВиходячи з цієї оцінки, ми отримуємо степеневої показник, рівний \\(\\alpha \\approx 3.7\\). Якщо ми побудуємо гістограму наших надзвичайно негативних логарифмічних прибутковостей, то побачимо, що вона добре відображає зниження ймовірності, оскільки масштаб збоїв збільшується. Однак цей степеневий показник все ще недооцінює ймовірність настання чергового “Чорного понеділка”, оскільки лінія відповідності не ідеально підходить до цієї крайньої точки. Якби ми захотіли створити ще більш консервативну модель екстремальних подій, нам потрібно було б вручну ще більше зменшити значення \\(\\alpha\\), щоб врахувати більшу ймовірність настання чорних понеділків за рахунок втрати точності підгонки для менших збоїв. Це перший раз, коли ми можемо побачити, як одна точка даних впливає на наші рішення щодо моделювання. На даний момент ми будемо довіряти оцінці параметра і погодимося з оцінкою \\(\\alpha=3.7\\).\n\nneg_lr = -lr[lr &lt; 0]\n\nplt.figure()\nhist = plt.hist(neg_lr, bins=np.logspace(np.log10(0.03), np.log10(0.3), 20), density=True)\nplt.yscale(\"log\")\nplt.xscale(\"log\")\nplt.ylabel(r'$\\log[P(|r_t|)]$')\nplt.xlabel(r'$\\log[|r_t|]$')\nplt.ylim([0.1, 110])\nplt.xlim([0.03, 0.3])\nplt.xticks([0.03, 0.04, 0.05, 0.06, 0.1, 0.2, 0.3], [0.03, 0.04, 0.05, 0.06, 0.1, 0.2, 0.3], rotation=35);\n\nx_fit = np.logspace(np.log10(0.03),np.log10(0.3), 100)\ny_fit = 0.00022*(x_fit**-3.7)\n\nplt.plot(x_fit, y_fit, lw=3)\nplt.show();\n\n\n\n\n\n\n\nРис. 12.6: Розподіл імовірностей щоденних негативних абсолютних прибутковостей у подвійному логарифмічному масштабі й лінія регресії з якої отримується кут нахилу даного розподілу\n\n\n\n\n\nМи можемо використовувати цей степеневий закон, щоб екстраполювати, як часто ми очікуємо настання чорного понеділка (або “події з 23 сигмами”) у довгостроковій перспективі. Виходячи з наявних у нас даних, ми можемо оцінити, що ймовірність спостереження зниження на \\(-0.03\\) або гірше становить близько \\(1.6\\%\\). Слідуючи нашій степеневій кривій, така крахова подія як чорний понеділок, що мала спад на \\(-0.229\\), відбувалася б із частотою \\(\\left(0.229/0.03\\right)^{-3.7} \\approx 1/1845\\). Так само часто, як зниження на \\(-0.03\\), яке ми спостерігаємо приблизно раз на три місяці. Це означає, що, виходячи з нашої степеневої кривої, очікується, що чорні понеділки, подібні до 1987 року, траплятимуться приблизно раз на 450 років (див. код нижче).\n\n# як часто ринок знижується більш ніж на -0.03\n(neg_lr &gt; 0.03).mean()\n\n0.016091417910447763\n\n\n\n# у середньому, як багато днів між цима трьома подіями\n1./(neg_lr &gt; 0.03).mean()\n\n62.144927536231876\n\n\n\n# у скільки разів менше відбувається подій \"Чорного понеділка\" в порівнянні зі зниженням на -0.03?\n(np.max(neg_lr)/0.03)**3.7\n\n1845.1139014672306\n\n\n\nnp.max(neg_lr)\n\n0.2289972265656708\n\n\n\n# скільки років пройшло між двома подіями \"Чорного понеділка\",\n# виходячи з припущення, що подія -0.03 відбувається один раз у 63 дня?\n(1./((1/62)*1/1845))/252\n\n453.92857142857144\n\n\nЯкби ми включили більше даних, що містять інші приклади екстремальних подій, наприклад, з Великої депресії, ця оцінка може стати ще меншою. Аналогічно, якби ми скоригували степеневий показник так, щоб він був більш консервативним, ніж передбачає наш простий метод оцінки, ми також отримали б коротший період. Хоча 460 років все ще є дуже великим періодом, це набагато більш реалістична оцінка порівняно з \\(10^{114}\\) роками, які ми отримали, використовуючи розподіл Гауса. Однією з переваг степеневої моделі є її здатність до екстраполяції: використовуючи степеневу оцінку \\(\\alpha\\) та частоту менших прибутковостей, щодо яких ми маємо достовірні дані, ми можемо оцінити частоту майбутніх серйозних збоїв, щодо яких ми ще не отримали жодних даних. Ще одну перевагу оцінки \\(\\alpha\\) ми розглянемо далі.\n\n\n\n\n\n\nПРИМІТКА\n\n\n\nНе існує такого поняття, як “подія з 10 сигмами”! Справжня подія з 10 сигмами настільки неймовірно рідкісна, що пережити її протягом нашого життя — мізерно мало. Екстремальні події, безумовно, трапляються, але вони не можуть бути описані за допомогою “сигм”, їх потрібно враховувати і екстраполювати з використанням степеневих законів. Завжди будьте обережні, коли аналітики виправдовують себе за те, що вони не були готові до “події з 10 сигмами”, оскільки це вказує на те, що їх моделі ризиків вкрай недосконалі\n\n\n\n\n12.1.7 Статистичні моменти під впливом важких хвостів\nЧому все це так важливо? Степенневий показник \\(\\alpha\\) насправді багато говорить нам про стабільність і збіжність моментів розподілу ймовірностей, і це, в свою чергу, має значення для деяких найбільш часто використовуваних моделей у фінансах. Перші чотири центральні моменти — це:\n\nСереднє: очікуване значення розподілу.\nДисперсія: Квадрат стандартного відхилення, який часто використовується для оцінки волатильності на основі розподілу прибутковостей.\nАсиметрія: вимірює зміщення розподілу. Розподіл прибутковостей, як правило, має негативну асиметрію, оскільки збої є більш стрімкими, ніж підйоми.\nЕксцес: вимірює тяжкість хвостів розподілу. Розподіли прибутковостей зазвичай мають ексцес більший ніж у розподілу Гауса, тобто екстремальні події відбуваються частіше, ніж очікувалося в гаусовій моделі.\n\nЕксцес Гаусового розподілу дорівнює 3 (використовуючи визначення Пірсона, з визначенням Фішера надлишкового ексцесу це 0), але наша вибірка щоденних логарифмічних прибутковостей S&P 500 має ексцес \\(28.6\\), що знову вказує на те, що екстремальні події набагато більш вірогідні.\n\n# обчислюємо ексцес лог-прибутковостей\nfrom scipy.stats import kurtosis\n\nkurtosis(lr, fisher=False, bias=False)\n\n28.610931957080663\n\n\nТепер ми дійшли до того моменту, коли ми могли б запитати: наскільки сильно одна точка впливає на нашу оцінку ексцесу? Якщо ми перерахуємо ексцес всіх щоденних прибутків S&P 500 з 1950 року і виключимо тільки чорний понеділок 1987 року, ми отримаємо значення \\(14.3\\), майже половину від значення, яке ми отримуємо, коли використовуємо всі значення! Видалення подальших найгірших днів, звичайно, ще більше зменшує ексцес, але ефект значно менший.\n\n# обчислюємо ексцес лог-прибутковостей після видалення Чорного понеділка 1987\nkurtosis(lr[lr &gt; np.min(lr)], fisher=False, bias=False)\n\n14.3083633314701\n\n\n\nlr_sorted = lr.sort_values()\nkurt = [kurtosis(lr_sorted.iloc[i:], fisher=False, bias=False) for i in range(10)]\n\nplt.figure()\nplt.plot(np.arange(10), kurt, c='C3', lw=2, zorder=2)\nplt.scatter(np.arange(10), kurt, s=120, lw=0.75, edgecolor='k', facecolor='C3', zorder=3)\nplt.grid()\nplt.xlabel('Кількість виключених найгірших днів')\n\nplt.ylabel(r'Kurt$[r_t]$')\nplt.ylim([0, 32])\nplt.show();\n\n\n\n\n\n\n\nРис. 12.7: Залежність ексцесу логарифмічних прибутковостей від кількості видалених найгірших днів на ринку S&P 500\n\n\n\n\n\nБільш радикальний погляд на цю проблему досягається, якщо ми будуємо графік ексцесу в покроковому режимі, тобто для кожного дня ми будуємо розрахунковий ексцес, використовуючи всі минулі логарифмічні дані до цього моменту часу. Як ви можете бачити нижче, до “Чорного понеділка” ексцес ніколи не перевищував \\(15\\), проте навіть через 35 років після “Чорного понеділка” ексцес ще не “оговтався” від цієї події і залишається на позначці 30. Чи означає це, що ми повинні просто ігнорувати Чорний понеділок як викид і продовжувати використовувати нашу “чисту” оцінку в \\(14.6\\)? Ні, точно ні! Як ми побачимо незабаром, ми скоріше повинні запитати себе, чи є сенс у тому, щоб оцінювати ексцес!\n\nkurt = [kurtosis(lr.iloc[:i], fisher=False, bias=False) for i in range(1000, len(lr))]\n\n\nplt.figure()\nplt.plot(lr.index[1000:], kurt, c='C3', lw=3, zorder=2)\nplt.xlabel('Date')\nplt.ylabel(r'Kurt$[r_t]$')\nplt.xticks(rotation=35)\nplt.show();\n\n\n\n\n\n\n\nРис. 12.8: Залежність ексцесу логарифмічних прибутковостей від кількості включених значень індексу S&P 500\n\n\n\n\n\nТой факт, що одна точка даних може настільки сильно змінити нашу оцінку ексцесу, і, схоже, вона більше не сходиться, коли додаються нові точки даних, є чітким свідченням того, що ексцес базового розподілу ймовірностей насправді нескінченний! Проблема полягає в тому, що для кінцевої вибірки даних (а всі набори даних скінченні) ми завжди зможемо оцінити скінчене значення ексцесу. scipy.stats.kurtosis не може повернути нескінченність (\\(\\infty\\)), він завжди повідомлятиме про скінченний ексцес вибірки. Але ця кінцева оцінка не допоможе нам нічого сказати про майбутню поведінку ринку, оскільки ексцес продовжуватиме переходити до ще більших значень до наступного чорного понеділка, іншого чорного вівторка … у більш-менш віддаленому майбутньому.\nОднак опис частоти екстремальних подій згідно степеневого закону, який ми представили вище, може допомогти нам вирішити, чи є ексцес кінцевим, так що, переглянувши достатню кількість точок даних, ми зможемо вирішити, чи слід прийняти той факт, що ексцес не піддається оцінці і справді нескінченний. Щоб побачити це, нам потрібно ввести ще один розподіл ймовірностей, t-розподіл Стьюдента. Це колоколообразний симетричний розподіл зі степеневими хвостами. Його додатковий параметр \\(\\nu\\) визначає показник степеневого закону \\(\\alpha=(\\nu+1)\\). Нижче ми підганяємо t-розподіл до наших логарифмічних прибутковостей S&P 500, зберігаючи при цьому степеневий показник \\(\\alpha=3.7\\) (відповідний \\(\\nu = 2.7\\)), який ми підганяли раніше:\n\nfrom scipy.stats import t\n\nfit_params = t.fit(lr, fix_df=2.7)\n\nx_fit = np.linspace(-0.25, 0.125, 1000)\ny_fit_t = t(*fit_params).pdf(x_fit)\n\n\nplt.figure()\nplt.hist(lr, 100, density=True)\nplt.plot(x_fit, y_fit_t, lw=3, c='C3', alpha=0.8, label=\"Підігнаний t-розподіл\")\nplt.yscale('log')\nplt.ylabel(r'$P(r_t)$')\nplt.xlabel(r'$r_t$')\nplt.xlim([-0.25, 0.125])\nplt.ylim([0.0011, 99])\nplt.legend(fontsize=18)\n\nplt.title(f'Підігнаний степеневий показник: $\\\\alpha = {(fit_params[0]+1):.2f}$')\nplt.show();\n\n\n\n\n\n\n\nРис. 12.9: Розподіл імовірностей логарифмічних прибутковостей S&P 500 у порівнянні з розподілами Гауса й Лапласа в напів- логарифмічному масштабі\n\n\n\n\n\nЗвичайно, можна поставити під сумнів, чи дійсно t-розподіл Стьюдента добре відповідає нашим логарифмічним прибутковостям, оскільки він переоцінює частоту абсолютних прибутковостей і все ще недооцінює ймовірність настання Чорного понеділка. На даний момент ми ігноруємо ці деталі та зосереджуємось на ексцесі t-розподілу Стьюдента та зв’язку зі степеневим показником \\(\\alpha\\):\n\n\\(\\text{Kurt} = 6/(\\nu-4) + 3~\\) для \\(~\\nu &gt; 4~\\) або \\(~\\alpha &gt; 5\\);\n\\(\\text{Kurt} = \\infty~~~~~~~~~~\\,\\) для \\(~2 &lt; \\nu \\leq 4~\\) або \\(~3 &lt; \\alpha \\leq 5\\);\nінакше невизначено.\n\nЦе підтверджує наше початкове припущення про те, що при степеневому показнику, рівному \\(\\alpha \\approx 3.7\\), ексцес дійсно нескінченний, і немає сенсу оцінювати його по скінченій вибірці, оскільки оціночне значення буде зберігатися тільки до наступної екстремальної події, яка підштовхне його ще вище. Ми можемо легко змоделювати цей ефект, витягуючи випадкові вибірки з t-розподілів Стьюдента з різними степеневими показниками, які відповідають трьом режимам, зазначеним вище. Далі ми генеруємо 100 000 випадкових значень з t-розподілів з \\(\\alpha = 2\\), \\(\\alpha = 3.7\\), \\(\\alpha = 6\\), і поетапно обчислюємо ексцес, як і раніше, щоб побачити еволюцію передбачуваного ексцесу у трьох різних випадках:\n\nnp.random.seed(1)\nsamples_alpha_20 = t(1.0, 0., 1.).rvs(100000)\nsamples_alpha_37 = t(2.7, 0., 1.).rvs(100000)\nsamples_alpha_60 = t(5.0, 0., 1.).rvs(100000)\n\n\nm = np.arange(1000, 100000, 50)\nkurt_alpha_20 = [kurtosis(samples_alpha_20[:i], fisher=False, bias=False) for i in m]\nkurt_alpha_37 = [kurtosis(samples_alpha_37[:i], fisher=False, bias=False) for i in m]\nkurt_alpha_60 = [kurtosis(samples_alpha_60[:i], fisher=False, bias=False) for i in m]\n\n\nplt.figure()\n\nplt.subplot(311)\nplt.plot(m, kurt_alpha_60, c='C2', lw=3, zorder=2)\nplt.axhline(6/(5-4) + 3, lw=1, ls='--', c='k')\nplt.ylabel('Kurt')\nplt.xlabel('Кількість значень')\nplt.title('Степеневий показник $\\alpha=6$')\n\nplt.subplot(312)\nplt.plot(m, kurt_alpha_37, c='C1', lw=3, zorder=2)\nplt.ylabel('Kurt')\nplt.xlabel('Кількість значень')\nplt.title('Степеневий показник $\\alpha=3.7$')\n\nplt.subplot(313)\nplt.plot(m, kurt_alpha_20, c='C3', lw=3, zorder=2)\nplt.ylabel('Kurt')\nplt.xlabel('Кількість значень')\nplt.title('Степеневий показник $\\alpha=2$')\n\nplt.tight_layout()\nplt.show();\n\n\n\n\n\n\n\nРис. 12.10: Випадкові значення згенеровані з \\(t\\)-розподілів при \\(\\alpha=2\\), \\(\\alpha=3.7\\) та \\(\\alpha=6\\)\n\n\n\n\n\nУ випадку \\(\\alpha=6\\) (\\(\\nu=5\\)) ексцес скінченний і повинен приймати значення \\(6/(\\nu - 4)+3=9\\). Хоча ми бачимо деякі коливання, оціночне значення ексцесу повільно наближається до справжнього значення. Для прикладу, який відповідає степеневому показнику індексу S&P 500 з \\(\\alpha = 3.7\\), моделювання показує таку ж поведінку, що і для реальних даних: збіжність не може бути виявлена, скоріше кожна екстремальна подія збільшує ексцес; при нескінченних вибірках ми би досягли нескінченного ексцесу. У третьому випадку з \\(\\alpha = 2\\) ексцес t-розподілу Стьюдента навіть не визначений належним чином, і ми можемо побачити інший тип поведінки моделювання: зараз більшість точок даних збільшують оцінку ексцесу, а не лише кілька окремих екстремальних зразків. При такому повільно спадаючому степеневому законі екстремальні значення зустрічаються повсюдно, що призводить до збільшення ексцесу при моделюванні.\n\n\n12.1.8 Ексцес при розрахунках волатильності\nОскільки ексцес при появі крахових подій збільшується — або стає нескінченним — це також впливає на нижчі моменти, найголовніше, на дисперсію. Квадратний корінь дисперсії — це стандартне відхилення, або волатильність, як це називається у фінансах. Волатильність є розповсюдженним параметром при оптимізації портфеля і практично в кожній фінансовій моделі, оскільки в кінцевому підсумку вона стала найбільш часто використовуваним показником “ризику”. Щоб побачити, як ексцес впливає на оцінку волатильності, ми проводимо простий експеримент Монте-Карло: ми генеруємо щоденну прибутковість за 50 років з розподілу ймовірностей за нашим вибором (ми будемо використовувати Гаусовий і t-розподіл Стьюдента) і обчислюємо оцінку волатильності, використовуючи вибіркове стандартне відхилення. Ми робимо це не тільки один раз, але і 10000 разів, кожен раз, коли ми розраховуємо нову гіпотетичну щоденну прибутковість за 50 років і обчислюємо відповідну оцінку волатильності. Звичайно, не всі ці оцінки будуть однаковими, але коливатимуться навколо справжньої волатильності, оскільки ми маємо лише кінцеву кількість даних. Оскільки ми знаємо справжній розподіл ймовірностей, який ми використовуємо для моделювання, ми також знаємо справжню волатильність. Щоб побачити, наскільки точно ми можемо оцінити волатильність на основі даних за 50 років, ми обчислюємо відносну похибку між оцінками волатильності та справжньою волатильністю.\n\nn_sim = 10000\n\ntrue_var = 1.\ntrue_vol = np.sqrt(true_var)\n\nrand_vol = np.array([np.std(norm(0., 1.).rvs(50*252), ddof=1) for i in tqdm(range(n_sim))])\n\n100*np.mean(np.abs(rand_vol - true_vol))/true_vol\n\n100%|██████████| 10000/10000 [00:08&lt;00:00, 1192.91it/s]\n\n\n0.5039568642168273\n\n\n\nn_sim = 10000\n\nalpha = 6.0\nnu = alpha - 1\n\ntrue_var = nu/(nu-2)\ntrue_vol = np.sqrt(true_var)\n\nrand_vol = np.array([np.std(t(nu, 0., 1.).rvs(50*252), ddof=1) for i in tqdm(range(n_sim))])\n\n100*np.mean(np.abs(rand_vol - true_vol))/true_vol\n\n100%|██████████| 10000/10000 [00:14&lt;00:00, 706.21it/s]\n\n\n0.9616583563963519\n\n\n\nn_sim = 10000\n\nalpha = 4.0\nnu = alpha - 1\n\ntrue_var = nu/(nu-2)\ntrue_vol = np.sqrt(true_var)\n\nrand_vol = np.array([np.std(t(nu, 0., 1.).rvs(50*252), ddof=1) for i in tqdm(range(n_sim))])\n\n100*np.mean(np.abs(rand_vol - true_vol))/true_vol\n\n100%|██████████| 10000/10000 [00:14&lt;00:00, 693.83it/s]\n\n\n3.8082666425995932\n\n\n\nn_sim = 10000\n\nalpha = 3.7\nnu = alpha - 1\n\ntrue_var = nu/(nu-2)\ntrue_vol = np.sqrt(true_var)\n\nrand_vol = np.array([np.std(t(nu, 0., 1.).rvs(50*252), ddof=1) for i in tqdm(range(n_sim))])\n\n100*np.mean(np.abs(rand_vol - true_vol))/true_vol\n\n100%|██████████| 10000/10000 [00:15&lt;00:00, 664.22it/s]\n\n\n6.115969986022528\n\n\n\nn_sim = 10000\n\nalpha = 3.1\nnu = alpha - 1\n\ntrue_var = nu/(nu-2)\ntrue_vol = np.sqrt(true_var)\n\nrand_vol = np.array([np.std(t(nu, 0., 1.).rvs(50*252), ddof=1) for i in tqdm(range(n_sim))])\n\n100*np.mean(np.abs(rand_vol - true_vol))/true_vol\n\n100%|██████████| 10000/10000 [00:14&lt;00:00, 700.96it/s]\n\n\n38.47707859009605\n\n\nМи отримуємо наступні відносні похибки для різних розподілів та їх параметрів: Гаусовий розподіл, відн. похиб. \\(\\approx 0.5\\%\\); t-розподіл Стьюдента з \\(\\alpha=6.0\\) (\\(\\nu=5.0\\)), відн. похиб. \\(\\approx 1.0\\%\\); t-розподіл Стьюдента з \\(\\alpha=4.0\\) (\\(\\nu=3.0\\)), відн. похиб. \\(\\approx 3.8\\%\\); t-розподіл Стьюдента з \\(\\alpha=3.7\\) (\\(\\nu=2.7\\)), відн. похиб. \\(\\approx 6.1\\%\\).\nПри степеневому показнику \\(\\alpha= 3.7\\) (що відповідає \\(\\nu = 2.7\\); як ми підрахували для S&P 500, щоб пояснити Чорний понеділок), щоденна прибутковість за 50 років все ще призводить до помилки оцінки волатильності більш ніж на \\(6%\\)! Зверніть увагу, що ця велика помилка оцінки зростатиме ще швидше, коли вона наближається до \\(\\alpha = 3.0\\), оскільки дисперсія стає нескінченною для t-розподілу Стьюдента при \\(\\alpha = 3.0\\) (\\(\\nu = 2.0\\)). Виходячи з t-розподілу прибутковостей S&P 500, похибка оцінки волатильності зросла в 12 разів (!) порівняно з тим, що ми очікували б, припускаючи, що прибутковості розподілені за Гаусом.\n\n\n\n\n\n\nПримітка\n\n\n\nЧим важчі хвости розподілу прибутковостей, тим більше точок даних необхідно для досягнення необхідної достовірності оцінок певних параметрів. Пам’ятайте про це щоразу, коли ви намагаєтеся оцінити ризик інвестування у відносно новий фінансовий актив, який має дані лише за кілька років\n\n\n\n\n12.1.9 Наслідки для моделювання та прогнозування\nТой факт, що більш високі моменти розподілу прибутковості можуть бути нескінченними і що оцінка нижчих моментів, таких як дисперсія, дестабілізується за наявності “товстих хвостів”, має суттєві наслідки для деяких найбільш використовуваних моделей у фінансах. Ми коротко розглянемо дві з них, щоб проілюструвати проблему.\n\n\n12.1.10 Марковіц і стабільність волатильності\nУ 1954 році Гаррі Марковіц отримав ступінь доктора економіки за роботу з теорії фінансових портфелів. Його робота стала наріжним каменем сучасної портфельної теорії і тоді це було настільки новим, що під час захисту докторської дисертації Мілтон Фрідман стверджував, що ця робота насправді не стосується галузі економіки (як пояснив Марковіц у своїй Нобелівській лекції у 1990 році). Марковіц стверджував, що при заданому рівні волатильності існує набір вагових коефіцієнтів розподілу активів у портфелі, які дають максимальну надлишкову прибутковість портфеля (порівняно з безризиковим активом). Якщо ви переглянете діапазон значень волатильності та обчислите максимальну віддачу портфеля, отримавши оптимальні ваги розподілу, тоді отримаєте набір ефективних портфелів, який також називають межею ефективності. Одна точка на цій межі в багатьох випадках представляє особливий інтерес, — це точка максимуму коефіцієнта Шарпа.\nКоефіцієнт Шарпа — це часто критикований, але також часто використовуваний показник для оцінки ефективності портфеля або фінансових активів загалом. Він визначається як \\(S = (\\mu - \\mu_{\\text{rf}})\\big/\\sigma\\).\nТут \\(\\mu\\) означає річну прибутковість портфеля, \\(\\mu_{\\text{rf}}\\) позначає річну безризикову прибутковість, а \\(\\sigma\\) є річною волатильністю портфеля. Припускаючи нульову безризикову ставку (\\(\\mu_{\\text{rf}} = 0\\)), коефіцієнт Шарпа можна інтерпретувати як співвідношення сигнал/шум, відомий у фізиці та інженерії: максимізуючи коефіцієнт Шарпа, ми знаходимо найкращий компромісне значення, що максимізує наші прибутковості (сигнал) та мінімізує волатильності (шум).\nЯкщо наш портфель складається з \\(n\\) фінансових активів з очікуваною прибутковістю \\(\\vec{\\mu} = (\\mu_{1},...,\\mu_{n})\\), з коваріаційною матрицею \\(\\Sigma\\), яка містить інформацію про волатильність і кореляції між активами, і з вектором ваги розподілу \\(\\vec{w} = (w_1,...,w_n)\\) (нормалізовано до одиниці: \\(\\sum_{i=1}^n w_i = 1\\)), тоді коефіцієнт Шарпа у контексті сучасної теорії портфеля можна обчислити наступним чином:\n\\[\nS(\\vec{w}) = \\vec{w}\\cdot\\vec{\\mu} \\Big/ \\sqrt{\\vec{w}\\Sigma\\vec{w}^T}.\n\\]\nЗверніть увагу, що з цього моменту ми завжди припускаємо, що \\(\\mu_{\\text{rf}} = 0\\).\nЩоб максимізувати прибутковість відносно волатильності та отримати портфель з максимальним коефіцієнтом Шарпа, ми змінюємо ваги розподілу, поки не знайдемо набір ваг \\(\\vec{w}^*\\) з максимальним коефіцієнтом Шарпа:\n\\[\n\\vec{w}^* = \\text{argmax}_{\\vec{w}} ~ S(\\vec{w}).\n\\]\nОсновне припущення, що лежить в основі сучасної теорії портфеля, полягає в тому, що об’єднаний розподіл логарифмічних прибутковостей всіх активів у портфелі може бути повністю охарактеризований мультиваріативним розподілом Гауса. Але тепер ми добре розуміємо цю помилку, і сучасна теорія портфеля по праву зазнала жорсткої критики за відсутність врахування “важких хвостів”, наприклад, з боку Нассіма Талеба:\n\n\n\n\n\n\nНассім Талеб у Чорний лебідь: Про (не)ймовірне в реальному житті (ст. 277)\n\n\n\nПісля краху фондового ринку (у 1987 році) вони нагородили двох теоретиків, Гаррі Марковіца та Вільяма Шарпа, які побудували прекрасні платонівські моделі на гаусовій основі, сприяючи тому, що називається сучасною теорією портфеля. Простіше кажучи, якщо ви видалите їх гаусові припущення і розглядаєте ціни як масштабно-інваріантні, у вас залишиться гаряче повітря. Нобелівський комітет міг би протестувати моделі Шарпа і Марковіца — вони працюють як шарлатанські ліки, що продаються в Інтернеті, — але ніхто в Стокгольмі, схоже, про це не подумав.\n\n\nТут ми просто хочемо проілюструвати наслідки припущення, що фінансові системи підпорядковуються Гаусіану, змоделювавши прибутковість двох гіпотетичних акцій і спробувавши знайти ваги розподілу по максимуму Шарпа. Як ми побачимо, великі хвости в модельованих розподілах прибутковостей вносять істотну невизначеність у розрахункові ваги розподілу — прямий наслідок нестабільності дисперсії.\nСпочатку ми генеруємо випадкові числа з Гаусового розподілу, достатнього для моделювання щоденної прибутковості двох гіпотетичних акцій за 10 років. Ми присвоюємо їм різну позитивну очікувану прибутковість і волатильність. Для простоти ми припускаємо, що кореляція між двома активами є нульовою:\n\nnp.random.seed(1234)\nn_years = 10\n\nlog_ret_1 = 0.0025*norm.rvs(size=n_years*252) + 0.0005\nlog_ret_2 = 0.0050*norm.rvs(size=n_years*252) + 0.0010\n\nplt.figure()\nplt.plot(np.exp(np.cumsum(log_ret_1)), label='Акція 1')\nplt.plot(np.exp(np.cumsum(log_ret_2)), label='Акція 2')\n\nplt.ylabel('Ціна акції')\nplt.xlabel('Date')\nplt.legend()\nplt.yscale('log')\nplt.show();\n\n\n\n\n\n\n\nРис. 12.11: Динаміка двох акцій, що згенерована за Гаусовим розподілом у напів- логарифмічному масштабі$\n\n\n\n\n\nЗвичайно, насправді ми не знаємо справжньої очікуваної прибутковості або значень волатильності кореляцій між активами. Таким чином, ми повинні оцінити очікувану прибутковість за середнім значенням вибірки та матрицю коваріації за матрицею коваріації вибірки. Зверніть увагу, що існують більш складні способи оцінки цих параметрів на основі історичних даних, таких як оцінки усадки для коваріаційної матриці або моделі розподілу Блека-Літтермана. Ці методи, безумовно, покращують вибіркові оцінки, але загальний недолік сучасної теорії портфеля через наявність “важких хвостів” залишається. Щоб вивчити та протестувати різні методи оптимізації портфеля, рекомендуємо розглянути бібліотеку PyPortfolioOpt, що представляє собою добре задокументований пакет Python, який реалізує як основні, так і складні методи оптимізації портфеля та дозволяє легко тестувати ці методи на реальних даних. Поки ми дотримуємося вибіркових оцінок (і використовуємо коефіцієнт 252 — середню кількість торгових днів на рік для отримання очікуваної прибутковості в річному обчисленні і коваріації за щоденними оцінками):\n\nexp_returns = np.array([np.mean(log_ret_1), np.mean(log_ret_2)])*252\ncov_matrix = np.cov([log_ret_1, log_ret_2])*252\n\nДалі ми визначаємо функцію, яка приймає ваги розподілу кандидатів, розрахункову очікувану прибутковість і коваріаційну матрицю і виводить коефіцієнт Шарпа портфеля:\n\ndef sharpe(weights, exp_returns, cov_matrix):\n    return weights.dot(exp_returns) / np.sqrt(weights.dot(cov_matrix).dot(weights.T))\n\nsharpe(np.array([0.5, 0.5]), exp_returns, cov_matrix)\n\n4.810679451759804\n\n\nЗамість того, щоб використовувати реальний оптимізатор для пошуку ваг розподілу, які максимізують коефіцієнт Шарпа, ми просто переглядаємо всі можливі комбінації ваг з інтервалом \\(1\\%\\). Звичайно, це розумний підхід лише для двох активів, і він швидко стає нерозв’язним, використовуючи більше двох активів.\n\ncandidate_weights = np.array([np.linspace(0, 1, 101), 1. - np.linspace(0, 1, 101)]).T\ncandidate_weights[:5]\n\narray([[0.  , 1.  ],\n       [0.01, 0.99],\n       [0.02, 0.98],\n       [0.03, 0.97],\n       [0.04, 0.96]])\n\n\nТепер ми можемо перебрати всі наші ваги-кандидати, обчислити коефіцієнт Шарпа портфеля для всіх з них, а потім вибрати оптимальний набір ваг.\n\ncandidate_sharpe = [sharpe(weights, exp_returns, cov_matrix) for weights in candidate_weights]\nopt_weights = candidate_weights[np.argmax(candidate_sharpe)]\nopt_weights\n\narray([0.7, 0.3])\n\n\nЩоб отримати уявлення про те, наскільки точно ми можемо оцінити ваги розподілу на основі гіпотетичної щоденної прибутковості за 10 років, ми зараз повторюємо цей експеримент, повторно генеруємо нові випадкові прибутковості, повторно оцінюємо очікувані прибутковості й матрицю коваріації та знаходимо оптимальні ваги. Ми робимо це 10000 разів. Ми реєструємо лише першу вагу розподілу \\(w_1\\) для кожного запуску без втрати інформації, оскільки друга вага безпосередньо випливає з нормалізації \\(w_2 = 1 - w_1\\).\n\nnp.random.seed(1234)\nopt_weight_gaussian = []\n\nfor i in tqdm(range(10000)):\n    log_ret_1 = 0.0025*norm.rvs(size=n_years*252) + 0.0005\n    log_ret_2 = 0.0050*norm.rvs(size=n_years*252) + 0.0010\n\n    exp_returns = [np.mean(log_ret_1), np.mean(log_ret_2)]\n    cov_matrix = np.cov([log_ret_1, log_ret_2])\n\n    candidate_sharpe = [sharpe(weights, exp_returns, cov_matrix) for weights in candidate_weights]\n    opt_weights = candidate_weights[np.argmax(candidate_sharpe)]\n\n    opt_weight_gaussian.append(opt_weights[0])\n\n100%|██████████| 10000/10000 [00:07&lt;00:00, 1366.93it/s]\n\n\nНарешті, ми можемо побудувати гістограму всіх 10000 значень, які ми отримуємо для \\(w_1\\).\n\nplt.figure()\nplt.hist(opt_weight_gaussian, 10, density=True);\nplt.xlim([0, 1])\nplt.xlabel(r'$w_1 = argmax_{w_1} ~ S(w_1)$')\nplt.ylabel(r'$P(w_1)$')\nplt.show();\n\n\n\n\n\n\n\nРис. 12.12: Розподіл імовірностей \\(w_1\\) для портфеля з максимальним коефіцієнтом Шарпа. Портфель акцій розподілений за нормальним Гаусовим розподілом\n\n\n\n\n\nЯк ми бачимо, розподіл для першої акції коливається приблизно від \\(60%\\) до \\(70%\\) на основі історичних даних за 10 років. Ми можемо додатково оцінити це кількісно, обчисливши 5-й і 95-й процентилі всіх значень \\(w_1\\). Таким чином, з упевненістю в \\(90%\\) ми отримаємо розподіл між \\(61%\\) і \\(72%\\):\n\nnp.percentile(opt_weight_gaussian, 5), np.percentile(opt_weight_gaussian, 95)\n\n(0.61, 0.72)\n\n\nТепер давайте повторимо цей експеримент, але цього разу ми замінимо розподіл Гауса на розподіл Стьюдента зі степеневим показником \\(\\alpha=3.7\\) (що відповідає кількості ступенів свободи \\(\\nu=2.7\\)). Ми залишаємо очікувану прибутковість і волатильність (виміряні стандартним відхиленням розподілу) точно такими ж. Зверніть увагу, що стандартне відхилення стандартного t-розподілу визначається як \\(\\sqrt{\\nu/(\\nu-2)}\\) в нашому випадку, тому ми повинні внести поправку на цей коефіцієнт маштабування при вилученні випадкових значень зі стандартного t-розподілу. Нижче ви можете побачити пряме порівняння розподілів Гауса, використаних у наведеному вище моделюванні, та t-розподілів із відповідним середнім значенням та стандартним відхиленням.\n\nscale_correction = np.sqrt(2.7/(2.7-2.))\n\n\nx = np.linspace(-0.025, 0.025, 10000)\n\ny1_norm = norm(loc=0.0005, scale=0.0025).pdf(x)\ny1_t = t(loc=0.0005, scale=0.0025/scale_correction, df=2.7).pdf(x)\n\ny2_norm = norm(loc=0.001, scale=0.005).pdf(x)\ny2_t = t(loc=0.001, scale=0.005/scale_correction, df=2.7).pdf(x)\n\n\nplt.figure()\n\nplt.subplot(121)\n\nplt.fill_between(x, 0., y1_norm, facecolor='C0', alpha=0.7)\nplt.fill_between(x, 0., y1_t, facecolor='C3', alpha=0.5)\nplt.ylim([0, 300])\nplt.ylabel(r'$P(X_1)$')\nplt.xlabel(r'$X_1$')\nplt.xticks(rotation=15)\nplt.title('Акція 1')\n\nplt.subplot(122)\nplt.fill_between(x, 0., y2_norm, facecolor='C0', alpha=0.7, label='Гаусіан')\nplt.fill_between(x, 0., y2_t, facecolor='C3', alpha=0.5, label=\"Стьюдент\")\nplt.ylim([0, 300])\nplt.ylabel(r'$P(X_2)$')\nplt.xlabel(r'$X_2$')\nplt.xticks(rotation=15)\nplt.title('Акція 2')\nplt.legend(fontsize=18)\n\nplt.tight_layout()\nplt.show(); \n\n\n\n\n\n\n\nРис. 12.13: Розподіл Стьюдента та Гауса для значень модельованих акцій\n\n\n\n\n\nЯк ми можемо легко бачити, узгоджені t-розподіли Стьюдента насправді дають менші значення прибутковостей. Однак вони також дають набагато більш екстремальні значення, але ми не можемо легко побачити це, оскільки значення щільності ймовірності в хвостах дуже малі! Якщо ми перейдемо до логарифмічного масштабування вісі Oy, картина стане більш чіткою:\n\nplt.figure()\n\nplt.subplot(121)\n\nplt.plot(x, y1_norm, c='C0', lw=3)\nplt.plot(x, y1_t, c='C3', lw=3)\nplt.ylabel(r'$P(X_1)$')\nplt.xlabel(r'$X_1$')\nplt.xticks(rotation=15)\nplt.title('Акція 1')\nplt.yscale('log')\n\nplt.subplot(122)\nplt.plot(x, y2_norm, c='C0', lw=3, label='Гаусіан')\nplt.plot(x, y2_t, c='C3', lw=3, label=\"Стьюдент\")\nplt.ylabel(r'$P(X_2)$')\nplt.xlabel(r'$X_2$')\nplt.xticks(rotation=15)\nplt.title('Акція 2')\nplt.yscale('log')\nplt.legend(fontsize=18)\n\nplt.tight_layout()\nplt.show();\n\n\n\n\n\n\n\nРис. 12.14: Розподіл Стьюдента та Гауса для значень модельованих акцій у напів- логарифмічному масштабі\n\n\n\n\n\nТепер ми чітко бачимо, що, наприклад, ймовірність щоденних логарифмічних прибутковостей \\(-0.02\\) для акції 1 в \\(10^{10}\\) разів вища за умови t-розподілу порівняно з відповідним розподілом Гауса! Завжди дивіться на розподіли з логарифмічною шкалою щільності ймовірності, щоб отримати чітке уявлення про хвости. Використовуючи наші узгоджені t-розподіли, ми діємо, як і раніше, і моделюємо гіпотетичні 10-річні історичні показники для наших двох некорельованих акцій. Нижче ви можете побачити приклад. Тільки при уважному розгляді ви можете помітити, що ці криві сукупної прибутковості насправді трохи більш нерівні в порівнянні з тими, які були побудовані вище з гаусовими прибутковостями. Але різниця невловима, якщо дивитися на таку “наближену” діаграму десятирічного періоду:\n\nnp.random.seed(1234)\nn_years = 10\n\nlog_ret_1 = (0.0025/scale_correction)*t.rvs(df=2.7, size=n_years*252) + 0.0005\nlog_ret_2 = (0.0050/scale_correction)*t.rvs(df=2.7, size=n_years*252) + 0.0010\n\nplt.figure()\nplt.plot(np.exp(np.cumsum(log_ret_1)), label='Акція 1')\nplt.plot(np.exp(np.cumsum(log_ret_2)), label='Акція 2')\n\nplt.ylabel('Ціна акції')\nplt.xlabel('Date')\nplt.legend()\nplt.yscale('log')\nplt.show();\n\n\n\n\n\n\n\nРис. 12.15: Динаміка двох акцій, що згенерована за розподілом Стьюдента у напів- логарифмічному масштабі\n\n\n\n\n\nНарешті, щоб візуалізувати вплив t-розподілу на оцінку ваги розподілу, ми повторюємо наш експеримент Монте-Карло з моделювання 10000 різних 10-річних історичних значень та оцінюємо ваги розподілу в кожному з цих 10000 випадків. Потім ми будуємо графік розподілу \\(w_1\\) і порівнюємо цей розподіл з розподілом, отриманим в результаті моделювання на основі Гауса.\n\nnp.random.seed(1234)\nopt_weight_t = []\n\nfor i in tqdm(range(10000)):\n    log_ret_1 = (0.0025/scale_correction)*t.rvs(df=2.7, size=n_years*252) + 0.0005\n    log_ret_2 = (0.0050/scale_correction)*t.rvs(df=2.7, size=n_years*252) + 0.0010\n\n    exp_returns = [np.mean(log_ret_1), np.mean(log_ret_2)]\n    cov_matrix = np.cov([log_ret_1, log_ret_2])\n\n    candidate_sharpe = [sharpe(weights, exp_returns, cov_matrix) for weights in candidate_weights]\n    opt_weights = candidate_weights[np.argmax(candidate_sharpe)]\n\n    opt_weight_t.append(opt_weights[0])\n\n100%|██████████| 10000/10000 [00:10&lt;00:00, 998.32it/s]\n\n\n\nplt.figure()\nplt.hist(opt_weight_gaussian, 10, alpha=0.8, density=True, label='Гаусові прибутковості')\nplt.hist(opt_weight_t, 20, alpha=0.8, density=True, label=\"Прибутковості Стьюдента\")\nplt.xlim([0, 1])\nplt.legend(fontsize=18)\nplt.xlabel(r'$w_1 = argmax_{w_1} ~ S(w_1)$')\nplt.ylabel(r'$P(w_1)$')\nplt.show();\n\n\n\n\n\n\n\nРис. 12.16: Порівняння розподілів імовірностей \\(w_1\\) для портфелів акцій з максимальним коефіцієнтом Шарпа. Портфелі розподілені за розподілами Гауса та Стьюдента\n\n\n\n\n\n\nnp.percentile(opt_weight_t, 5), np.percentile(opt_weight_t, 95)\n\n(0.54, 0.77)\n\n\nЯк ми можемо бачити, оптимальна вага розподілу \\(w_1\\) тепер коливається між \\(54\\%\\) і \\(77\\%\\), що приблизно в 2 рази перевищує ширину надійного інтервалу для оптимізації портфеля, яка дотримувалася припущення про гаусовість! Навіть при узгодженому стандартному відхиленні гаусового і t-розподілу, нескінченний ексцес t-розподілу дестабілізує вибіркову дисперсію наших змодельованих прибутковостей і, таким чином, знижує точність наших оціночних ваг розподілу. Хоча цей приклад, безумовно, не розкриває всіх слабкостей сучасної теорії портфеля, він допомагає нам побачити, як досить абстрактні ефекти, такі як нестабільність дисперсії через надмірний ексцес, в кінцевому результаті псують широко використовувані фінансові моделі. Використання модельованої прибутковості, заснованої на гаусовому розподілі порівняно з розподілом з великим хвостом, може допомогти виявити обмеження фінансових моделей або знайти мінімальний обсяг даних, необхідний для досягнення певної достовірності, перш ніж нова модель буде фактично запущена у виробництво!\n\n\n\n\n\n\nПримітка\n\n\n\nВажкі хвости розподілу прибутковостей можуть сильно вплинути на результати усталених математичних моделей у фінансах, або збільшуючи невизначеність параметрів (як у сучасній теорії портфеля), або приводячи до збоїв моделей поза вибірки через нескінченно великі моменти. У фінансах краще екстраполювати частоту екстремальних подій і готуватися до них за допомогою конкретних інструментів геджування, ніж намагатися передбачити майбутню прибутковість за допомогою моделей часових рядів!\n\n\n\n\n12.1.11 Розподіл Леві\nАльфа-стабільний розподіл є узагальненням розподілу Гауса, що враховує “важкі хвости”. З цієї причини він широко використовується при обробці сигналів, наприклад, у медицині чи фінансах.\nЗагальний клас стабільних розподілів був введений і отримав цю назву французьким математиком Полем Леві на початку 1920-х років  [2].\nРаніше ця тема привертала лише помірну увагу провідних експертів. Натхненням для Леві стало бажання узагальнити відому центральну граничну теорему, згідно з якою будь-який розподіл імовірностей з кінцевою дисперсією збігається до розподілу Гауса.\nСтабільні розподіли мають три виняткові властивості, які можна коротко підсумувати, заявляючи, що вони:\n\nінваріантні при додаванні;\nмають власну область збіжності;\nдозволяють канонічну форму характеристичної функції.\n\n\n12.1.11.1 Інваріантність при додавані\nВипадкова величина \\(X\\) підпорядковується стабільному розподілу \\(P(x) = \\text{Prob}\\{X \\leq x\\}\\), якщо для будь-якого \\(n \\geq 2\\) існують додатнє значення \\(c_{n}\\) та дійсне значення \\(d_{n}\\) такі, що\n\\[\nX_1 + X_2 + ... + X_n \\stackrel{d}{=} c_{n}X + d_{n},\n\\]\nде \\(X_1, X_2, ..., X_n\\) характеризуються як незалежні, ідентично розподілені випадкові величини. Також \\(\\stackrel{d}{=}\\) позначає рівність розподілів, тобто, випадкові величини з обох сторін мають однаковий розподіл імовірностей.\nЗагалом, сума незалежних, ідентично розподілених випадкових величин результує у випадкову величину з іншим розподілом. Однак, для випадкових величин, що характеризуються стабільним розподілом, сума ідентично розподілених випадкових величин прямує до величини такого самого розподілу  [3–5]. У цьому випадку результуюча випадкова величина (розподіл) може відрізнятися від попередніх величин характерним масштабом (\\(c_{n}\\)) та зміщенням (\\(d_{n}\\)). Якщо \\(d_{n} = 0\\), розподіл називається строго стабільним.\nВідомо, що нормована константа \\(c_{n}\\) має вид\n\\[\nc_{n} = n^{1/\\alpha} \\, \\text{при} \\,\\, 0 &lt; \\alpha \\leq 2.\n\\]\nПараметр \\(\\alpha\\) є характеристичною експонентою або індексом стабільності розподілу.\nПопередня теорема має альтернативну версію, що включає в суму лише дві випадкові величини. Випадкова величина \\(X\\) підпорядковується стабільному розподілу, якщо для будь-яких позитивних значень \\(A\\) та \\(B\\) існує позитивне число \\(C\\) та дійсне число \\(D\\) такі, що\n\\[\nA X_1 + B X_2 \\stackrel{d}{=} C X + D,\n\\]\nде \\(X_1\\) та \\(X_2\\) незалежні копії \\(X\\). Тоді існує значення \\(\\alpha \\in (0, 2]\\) при яких значення \\(C\\) задовільняє рівність \\(C^{\\alpha} = A^{\\alpha} + B^{\\alpha}\\).\nДля строго стабільних розподілів \\(D = 0\\). Це означає, що всі лінійні комбінації випадкових незалежних, ідентично розподілених величин, що підкоряються строго стабільному розподілу, результують у випадкову величину з одним і тим же типом розподілу.\nСтабільний розподіл вважається симетричним, якщо величина \\(-X\\) має такий самий тип розподілу. Симетричний стабільний розподіл обов’язково строго стабільний.\nОскільки аналітичний вираз функції щільності ймовірностей для стабільного розподілу невідомий, за винятком кількох членів стабільного сімейства, більшість традиційних методів математичної статистики не можуть бути використані. Відповідними вийнятками є\n\nГаусовий розподіл \\(S_2(0,\\mu, \\sigma) = \\mathcal{N}(\\mu, 2\\sigma^2)\\). Гаусовий розподіл є спеціальним випадком стабільного розподілу при \\(\\alpha = 2\\) так що \\(\\mathcal{N}(\\mu, \\sigma) = S(2,0,\\mu, \\frac{\\sigma}{\\sqrt{2}})\\), де \\(\\mu\\) позначає середнє значення нормального розподілу, а \\(\\sigma\\) — це стандартне відхилення. Функція щільності ймовірностей має вид\n\n\\[\n(\\sigma\\sqrt{2\\pi})^{-1}\\text{exp}\\left[-(x-\\mu)^{2}/2\\sigma^{2}\\right].\n\\]\n\nРозподіл Коші. Розподіл Коші — це ще одне представлення стабільного розподілу при \\(\\alpha = 1\\) та \\(\\beta = 0\\) такими, що \\(Cauchy(\\delta, \\gamma) = S_1(1,0,\\gamma,\\delta)\\), де \\(\\gamma\\) — це параметр масштабування, а \\(\\delta\\) — це параметр зсуву розподілу Коші. Функція щільності ймовірностей представлена як:\n\n\\[\n\\gamma \\big/ \\pi[(x-\\delta)^2 + \\gamma^2], \\, -\\infty &lt; x &lt; \\infty.\n\\]\n\nРозподіл Леві також є вийнятком із класу стабільних розподілів, де \\(\\alpha = 0.5\\) і \\(\\beta = 1\\). Іншими словами, \\(Levy(\\delta, \\gamma) = S_{1/2}(0.5, 1, \\gamma, \\delta)\\). Функція щільності ймовірностей має вид\n\n\\[\n\\sqrt{\\gamma/2\\pi}(x-\\delta)^{-3/2}\\exp{\\left[-\\gamma/2(x-\\delta) \\right]}, \\, \\delta &lt; x &lt; \\infty.\n\\]\nЯкщо \\(X \\sim S_{1/2}(0.5, 1, \\gamma, \\delta)\\), тоді для \\(x &gt; 0\\)\n\\[\nP(X \\leq x) = 2 \\left[ 1 - \\phi \\left( \\sqrt{\\gamma/x} \\right) \\right],\n\\]\nде \\(\\phi\\) позначає кумулятивну функцію нормального розподілу.\n\n\n12.1.11.2 Область збіжності\nІнше (еквівалентне) визначення стверджує, що стабільні розподіли — це єдині розподіли, які можна отримати при границі нормалізованих сум незалежних, ідентично розподілених випадкових величин. Кажуть, що випадкова величина \\(X\\) має область збіжності, тобто якщо існує послідовність незалежних, ідентично розподілених випадкових величин \\(Y_1, Y_2, ...\\) і послідовності позитивних чисел \\({\\gamma_n}\\) і дійсних чисел \\({\\delta_n}\\) таких, що\n\\[  \n\\left( Y_1 + Y_2 + ... + Y_n \\right)/\\gamma_n + \\delta_n \\stackrel{d}{\\Rightarrow} X.  \n\\]\nКоли \\(X\\) це гаусова випадкова величина, а \\(Y_i\\) є незалежними, ідентично розподіленими випадковими величинами з визначенною дисперсією, тоді рівняння вище є твердженням звичайної центральної граничної теореми. Область збіжності \\(X\\) вважається нормальною коли \\(\\gamma_n = n^{1/\\alpha}\\).\n\n\n12.1.11.3 Канонічні представлення характеристичної функції\nЧотири параметри використовуються для опису випадкової величини, що слідує за стабільним розподілом: \\(X \\sim S(\\alpha, \\beta, \\mu, \\gamma)\\). Параметр \\(\\alpha \\in (0, 2]\\) — це той, який нас найбільше зацікавить. Цей параметр визначає товщину хвостів. Параметр \\(\\beta \\in [-1, 1]\\) є параметром асиметрії. Останні два параметри позначають розташування \\((\\mu \\in \\Re)\\) і масштаб \\((\\gamma &gt; 0)\\) розподілу. Альфа-стабільний розподіл немає жодного аналітичного виразу для щільності ймовірності \\(X\\), але ми можемо охарактеризувати його характеристичною функцією  [6–8]:\n\\[\n    \\begin{split}\n    \\phi(t) &= E\\left[\\exp(itX)\\right] \\\\\n            &=\n        \\begin{cases}\n            \\exp\\left(i \\mu t - \\gamma^{\\alpha}|t|^{\\alpha} \\left[1-i\\beta\\text{sign}(t)\\tan(\\pi\\alpha/2\\right]\\right) & \\text{при} \\,\\, \\alpha \\neq 1, \\\\ \\exp\\left(i \\mu t - \\gamma|t| \\left[1+i\\beta\\text{sign}(t)(2/\\pi)\\log{|t|}\\right]\\right) & \\text{при} \\,\\, \\alpha = 1.\n        \\end{cases}\n    \\end{split}\n\\]\nМи могли б використовувати перетворення Фур’є, щоб отримати функцію щільності розподілу ймовірностей з характеристичної функції  [9]:\n\\[\nf(x) = \\frac{1}{2\\pi}\\int_{-\\infty}^{+\\infty} \\phi(t) \\cdot \\exp(-itX) dt.\n\\]\nАле наведена вище параметризація не є повністю задовільною, оскільки функція щільності розподілу ймовірностей не є неперервною, зокрема, при \\(\\alpha = 1\\). Дійсно, коли \\(\\beta &gt; 0\\), щільність розподілу зміщується вправо, коли \\(\\alpha &lt; 1\\) і вліво, коли \\(\\alpha &gt; 1\\), зі зсувом в сторону \\(+\\infty\\) (відповідно \\(-\\infty\\)), коли \\(\\alpha\\) прагне до 1. Таким чином, для прикладного аналізу даних та інтерпретації коефіцієнтів слід уникати такої параметризації.\nІснує багато параметризацій для стабільних законів, і ці різні параметризації викликали велику плутанину. Різноманітність параметризацій обумовлена поєднанням історичної еволюції плюс численними проблемами, які були проаналізовані за допомогою спеціалізованих форм стабільного розподілу. Є вагомі причини використовувати різні параметризації в різних ситуаціях. Якщо в пріорітеті є чисельні розрахунки, робота із даними, то краще використовувати одну параметризацію. Якщо бажані прості алгебраїчні властивості розподілу або аналітичні властивості строго стійких законів, то краще розглянути дещо інші. Нолан запропонував використовувати параметризацію Золотарьова  [10], яку також часто позначають як \\(S^{0}\\). Характеристична функція, що відповідає \\(X \\sim S^{0}(\\alpha, \\beta, \\mu_{0}, \\gamma)\\), дорівнює:\n\\[\n    \\begin{split}\n    \\phi(t) &= E\\left[\\exp(itX)\\right] \\\\\n            &=\n        \\begin{cases}\n            \\exp\\left(i \\mu_{0} t - \\gamma^{\\alpha}|t|^{\\alpha} \\left[1+i\\beta\\text{sign}(t)\\tan(\\pi\\alpha/2)\\left( \\gamma^{1-\\alpha}|t|^{1-\\alpha}-1 \\right)\\right]\\right) & \\text{при} \\,\\, \\alpha \\neq 1, \\\\ \\exp\\left(i \\mu_{0} t - \\gamma|t| \\left[1+i\\beta\\text{sign}(t)(2/\\pi)\\left(\\log{|t|} + \\log{\\gamma}\\right) \\right]\\right) & \\text{при} \\,\\, \\alpha = 1.\n        \\end{cases}\n    \\end{split}\n\\]\nЦя альтернативна параметризація недалека від зазначеної напочатку. Єдина відмінність стосується параметра \\(\\mu\\), який у даній параметризації коригує зсув для значень \\(\\alpha\\) близьких до 1:\n\\[\n\\mu_{0} = \\begin{cases} \\mu + \\beta\\gamma\\tan(\\pi\\alpha/2) & \\text{при} \\,\\, \\alpha \\neq 1, \\\\ \\mu + \\beta(2/\\pi)\\gamma\\log{\\gamma} & \\text{при} \\,\\, \\alpha = 1. \\end{cases}\n\\]\n\n\n12.1.11.4 Метод розрахунку параметрів \\(\\alpha\\)-стабільного розподілу\nЧислові методи, такі як метод заснований на квантилях  [11–13], і метод оцінки максимальної правдоподібності  [14] були розроблені на виклик відсутності аналітичних рішень. Припустимо, що \\(\\text{X} = (X_1, ... , X_T)\\) вектор, що складається з \\(T\\) незалежних ідентично розподілених випадкових величин із розподілу Парето, і також \\(x \\sim S_{\\alpha}(\\alpha, \\beta, \\delta, \\gamma)\\). Визначивши \\(\\theta = (\\alpha, \\beta, \\delta, \\gamma)\\), Митник, Доганоглу та Ченайо використали алогритм максимальної правдоподібності і показали, що \\(\\theta\\) можна розрахувати, максимізуючи функцію логарифмічної правдоподібності  [15]:\n\\[\nl(\\theta, x) = \\sum_{i=1}^{T}\\log{f(x_{i}, \\theta)}.\n\\]\nДюмушель визначив функцію правдоподібності наступним чином  [16]:\n\\[\nL(\\theta) = \\prod_{k=1}^{n}S_{\\alpha,\\beta} \\left[ (X_{k} - \\delta)/\\gamma \\right] \\big/ \\gamma,\n\\]\nде \\(\\theta = (\\alpha, \\beta, \\delta, \\gamma)\\) опираючись на \\(x = (x_1, ... , x_n)\\) для розміру вибірки \\(n\\).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Лабораторна робота № 12</span>"
    ]
  },
  {
    "objectID": "lab_12.html#хід-роботи",
    "href": "lab_12.html#хід-роботи",
    "title": "12  Лабораторна робота № 12",
    "section": "12.2 Хід роботи",
    "text": "12.2 Хід роботи\nЗчитуємо, наприклад, дані одного із фондових індексв Індії з Yahoo Finance:\n\n# встановлення назви індексу\nsymbol = \"^BSESN\" \n\n# встановлення діапазону з яким будемо працювати\nstart = \"1980-01-01\"\nend = \"2022-11-07\"\n\n# завантаження даних з Yahoo\ndata = yf.download(symbol, start, end)\ntime_ser = data['Adj Close'].copy()\n\n# підпис по вісі Ох \nxlabel = 'time, days'\n\n# підпис по вісі Оу\nylabel = symbol                       \n\n# збереження результату в текстовий документ \nnp.savetxt(f'{symbol}_initial_time_series.txt', time_ser.values)\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nУвага\n\n\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance, а із власного файлу. Зрозуміло, що й аналіз результатів, і висновки залежать від того, з яким рядом ми працюємо\n\n\n\n\nsymbol = 'sMpa11'                  # Символ індексу\n\npath = \"databases\\sMpa11.txt\"      # шлях по якому здійснюється зчитування файлу\ndata = pd.read_csv(path,           # зчитування даних \n                   names=[symbol])\ntime_ser = data[symbol].copy()     # копіюємо значення кривої \n                                   # \"напруга-видовження\" до окремої змінної\n\nxlabel = r'$\\varepsilon$'          # підпис по вісі Ох \nylabel = symbol                    # підпис по вісі Оу\n\n\nВиводимо графік досліджуваного ряду\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіку\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\n\n\n\nРис. 12.17: Динаміка щоденних значень фондового індексу BSESN\n\n\n\n\n\n\n12.2.1 Побудова розподілу Леві та розрахунок параметрів для всього ряду\nДля приведення ряду до стандартизованого вигляду або прибутковостей визначимо функцію transformations():\n\ndef transformation(signal, ret_type):\n\n    for_rec = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_rec = for_rec.diff()\n    elif ret_type == 3:\n        for_rec = for_rec.pct_change()\n    elif ret_type == 4:\n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n    elif ret_type == 5: \n        for_rec = for_rec.pct_change()\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n        for_rec = for_rec.abs()\n    elif ret_type == 6:\n        for_rec -= for_rec.mean()\n        for_rec /= for_rec.std()\n\n    for_rec = for_rec.dropna().values\n\n    return for_rec\n\nДля побудови в парі певного індикатора та досліджуваного ряду визначимо функцію plot_pair:\n\ndef plot_pair(x_values, \n              y1_values,\n              y2_values,  \n              y1_label, \n              y2_label,\n              x_label, \n              file_name, clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(x_values, \n                  y1_values, \n                  \"b-\", label=fr\"{y1_label}\")\n    p2, = ax2.plot(x_values,\n                   y2_values, \n                   color=clr, \n                   label=y2_label)\n\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(f\"{y1_label}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n\n\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(file_name + \".jpg\")\n        \n    plt.show();\n\nДалі виконаємо приведення ряду до прибутковостей:\n\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд \n\nfor_levy = transformation(time_ser, ret_type)\n\nПідганяємо розподіл Леві та Гауса для порівняння:\n\nparams = levy.fit_levy(for_levy)\nmean, std = norm.fit(for_levy)\n\nОтримуємо параметри розподілу Леві у відповідності до однієї із параметризацій, що пропонує пакет levy:\n\nalpha, beta, mu, sigma = params[0].get('1')\n\nБудуємо теоретичні та емпіричні розподіли:\n\nxmin = for_levy.min()\nxmax = for_levy.max()\n\nx = np.linspace(xmin, xmax, len(for_levy))\npdf = levy.levy(x, alpha, beta, mu, sigma)\npdf_norm = norm.pdf(x, mean, std)\n\nfig, ax = plt.subplots(1, 2, figsize=(12, 8))\n\nfig.suptitle(fr'Теоретичні та емпіричні $\\alpha$-стабільні розподіли для {symbol}', fontsize=20)\n\nax[0].hist(for_levy, bins=50, density=True, alpha=0.6, color='b')\nax[0].plot(x, pdf, 'k')\nax[0].plot(x, pdf_norm, 'r')\nax[0].set_yscale('log')\nax[0].set_xlabel(r'$x$')\nax[0].set_ylabel(r'$f_{\\alpha}(x), \\, \\mathrm{ePDF}$')\n\nax[1].hist(for_levy, bins=50, density=True, alpha=0.6, color='g')\nax[1].plot(x, pdf, 'k')\nax[1].plot(x, pdf_norm, 'r')\nax[1].set_xlabel(r'$x$')\nax[1].set_ylabel(r'$f_{\\alpha}(x), \\, \\mathrm{ePDF}$')\n\n\nplt.savefig(f\"Теоретичні та емпіричні альфа стабільні розподіли для {symbol}.jpg\")\n\nfig.tight_layout()\nplt.show();\n\n\n\n\n\n\n\nРис. 12.18: Теоретичні та емпіричні альфа-стабільні функції щільності ймовірностей\n\n\n\n\n\nВиводимо параметри розподілу Леві для заданого індексу\n\nprint(fr\"Параматери alpha = {alpha:.2f}, beta = {beta:.2f}, mu = {mu:.2f}, sigma = {sigma:.2f}\")\n\nПараматери alpha = 1.63, beta = -0.11, mu = -0.01, sigma = 0.53\n\n\nДля досліджуваного індексу бачимо, що параметр \\(\\alpha &lt; 2.0\\) та \\(\\beta &lt; 0\\), що вказує на відхилення розподілу даного індексу від нормального. Тобто, для даного ряду суттєвими є кризові явища, на що вказують важкі хвости розподілу. Із порівняльного аналізу гаусового та Леві розподілів бачимо, що хвости нормального розподілу значно недооцінюють імовірність появи кризових явищ чого, наприклад, не скажеш про альфа-стабільний розподіл. Взявши логарифм значень імовірності по осі \\(Oy\\), ми можемо спостерігати, що, наприклад, недооцінка негативних прибутковостей гаусовим розподілом, у порівнянні з альфа-стабільним, складає \\(\\approx 10^{15}\\) порядків. Для позитивних прибутковостей, що перевищують значення \\(+10\\sigma\\) недооцінка гаусовим розподілом складає \\(\\approx 10^{27}\\) порядків. Теоретичне значення альфа-стабільного розподілу достатньо точно враховує важкі хвости емпіричного розподілу, що також виражається високим ексцесом розподілу. Також варто зазначити, що коефіцієнт асиметрії \\(\\beta\\) вказує на невелике зміщення розподілу вліво, що також демонструє переважання кризових явищ.\n\n\n12.2.2 Дослідження поведінки альфа-стабільного розподілу Леві\n\nx = np.arange(-5, 5, .01)\nbeta_1 = 0\nmu = 0 \nsigm = 1 \nbeta_2 = 1.0\n\nfig, ax = plt.subplots(2, 2, figsize=(20, 10))\n\nax[0][0].plot(x, levy.levy(x, 0.5, beta_1, mu, sigm), label = r\"$\\alpha=0.5$\")\nax[0][0].plot(x, levy.levy(x, 0.75, beta_1, mu, sigm), label = r\"$\\alpha=0.75$\")\nax[0][0].plot(x, levy.levy(x, 1.0, beta_1, mu, sigm), label = r\"$\\alpha=1.0 $\")\nax[0][0].plot(x, levy.levy(x, 1.25, beta_1, mu, sigm), label = r\"$\\alpha=1.25$\")\nax[0][0].plot(x, levy.levy(x, 1.5, beta_1, mu, sigm), label = r\"$\\alpha=1.5$\")\nax[0][0].set_title(r\"Симетричні $\\alpha$-стабільні розподіли, $\\beta = 0$, $\\mu = 0$, $\\sigma = 1$\", y=1.03, fontsize=20)\nax[0][0].legend(fontsize=20)\nax[0][0].set_xlabel(r\"$ x $\")\nax[0][0].set_ylabel(r\"$ f_{\\alpha}(x) $\")\n\nax[0][1].plot(x, levy.levy(x, 0.5, beta_2, mu, sigm), label = r\"$\\alpha=0.5$\")\nax[0][1].plot(x, levy.levy(x, 0.75, beta_2, mu, sigm), label = r\"$\\alpha=0.75$\")\nax[0][1].plot(x, levy.levy(x, 1.0, beta_2, mu, sigm), label = r\"$\\alpha=1.0$\")\nax[0][1].plot(x, levy.levy(x, 1.25, beta_2, mu, sigm), label = r\"$\\alpha=1.25$\")\nax[0][1].plot(x, levy.levy(x, 1.5, beta_2, mu, sigm), label = r\"$\\alpha=1.5$\")\nax[0][1].set_title(r\"Зміщенні $\\alpha$-стабільні розподіли, $ \\beta = 1 $, $ \\mu = 0 $, $ \\sigma = 1 $\", y=1.03, fontsize=20)\nax[0][1].legend(fontsize=20)\nax[0][1].set_xlabel(r\"$ x $\")\nax[0][1].set_ylabel(r\"$ f_{\\alpha}(x) $\")\n\nax[1][0].plot(x, levy.levy(x, 0.5, beta_1, mu, sigm, cdf=True), label = r\"$ \\alpha=0.5 $\")\nax[1][0].plot(x, levy.levy(x, 0.75, beta_1, mu, sigm, cdf=True), label = r\"$ \\alpha=0.75 $\")\nax[1][0].plot(x, levy.levy(x, 1.0, beta_1, mu, sigm, cdf=True), label = r\"$ \\alpha=1.0 $\")\nax[1][0].plot(x, levy.levy(x, 1.25, beta_1, mu, sigm, cdf=True), label = r\"$ \\alpha=1.25 $\")\nax[1][0].plot(x, levy.levy(x, 1.5, beta_1, mu, sigm, cdf=True), label = r\"$ \\alpha=1.5 $\")\nax[1][0].set_title(r\"Симетричні $\\alpha$-стабільні розподіли, $ \\beta = 0 $, $ \\mu = 0 $, $ \\sigma = 1 $\", y=1.03, fontsize=20)\nax[1][0].legend(fontsize=20, loc=\"lower right\")\nax[1][0].set_xlabel(r\"$x$\")\nax[1][0].set_ylabel(r\"$F_{\\alpha}(x)$\")\n\nax[1][1].plot(x, levy.levy(x, 0.5, beta_2, mu, sigm, cdf=True), label = r\"$ \\alpha=0.5 $\")\nax[1][1].plot(x, levy.levy(x, 0.75, beta_2, mu, sigm, cdf=True), label = r\"$ \\alpha=0.75 $\")\nax[1][1].plot(x, levy.levy(x, 1.0, beta_2, mu, sigm, cdf=True), label = r\"$ \\alpha=1.0 $\")\nax[1][1].plot(x, levy.levy(x, 1.25, beta_2, mu, sigm, cdf=True), label = r\"$ \\alpha=1.25 $\")\nax[1][1].plot(x, levy.levy(x, 1.5, beta_2, mu, sigm, cdf=True), label = r\"$ \\alpha=1.5 $\")\nax[1][1].set_title(r\"Зміщенні $\\alpha$-стабільні розподіли, $\\beta = 1$, $\\mu = 0$, $\\sigma = 1$\", y=1.03, fontsize=20)\nax[1][1].legend(fontsize=20, loc=\"lower right\")\nax[1][1].set_xlabel(r\"$x$\")\nax[1][1].set_ylabel(r\"$F_{\\alpha}(x)$\")\n\nfig.tight_layout()\nplt.show();\n\n\n\n\n\n\n\nРис. 12.19: Залежність функції щільності ймовірностей альфа-стабільного розподілу Леві та кумулятивної функції щільності від різних значень параметрів розподілу\n\n\n\n\n\n\n\n12.2.3 Віконна процедура\nВиконуватимемо подальші обчислення для стандартизованих прибутковостей, Покажемо, що та застосуємо альфа-стабільний розподіл Леві дозволяє змоделювати типові флуктуації складних систем та передчасної ідентифікації в них критичних і кризових явищ.\n\nwindow = 250    # ширина ковзного вікна\ntstep = 5       # крок\n\nret_type = 4    # вид ряду: \n                # 1 - вихідний, \n                # 2 - детрендований (різниця між теп. значенням та попереднім)\n                # 3 - прибутковості звичайні, \n                # 4 - стандартизовані прибутковості, \n                # 5 - абсолютні значення (волатильності)\n                # 6 - стандартизований ряд\n        \nlength = len(time_ser)\n\nalpha = []\nbeta = []\nmu = []\nsigma = []\n\n\nfor i in tqdm(range(0,length-window,tstep)):\n    # відбираємо фрагмент\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n    \n    # здійснюємо підгонку розподілу під значення ряду\n    params = levy.fit_levy(fragm)\n    \n    # отримуємо параметри розподілу\n    a, b, m, s = params[0].get('0')\n    \n    alpha.append(a)\n    beta.append(b)\n    mu.append(m)\n    sigma.append(s)\n\n100%|██████████| 1199/1199 [04:42&lt;00:00,  4.24it/s]\n\n\nЗберігаємо абсолютні значення у текстовому документі:\n\nnp.savetxt(f\"alpha_idx_{symbol}_{window}_{tstep}_{ret_type}.txt\", alpha)\nnp.savetxt(f\"beta_idx_{symbol}_{window}_{tstep}_{ret_type}.txt\", beta)\nnp.savetxt(f\"mu_idx_{symbol}_{window}_{tstep}_{ret_type}.txt\", mu)\nnp.savetxt(f\"sigma_idx_{symbol}_{window}_{tstep}_{ret_type}.txt\", sigma)\n\n\n\n12.2.4 Динаміка показника стабільності \\(\\alpha\\)\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nmeasure_label = r'$\\alpha$'\nfile_name = f\"alpha_idx_{symbol}_{window}_{tstep}_{ret_type}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          alpha, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name)\n\n\n\n\n\n\n\nРис. 12.20: Динаміка фондового індексу BSESN та показника стабільності \\(\\alpha\\)\n\n\n\n\n\nПараметр \\(\\alpha\\) (індекс стабільності хвостів розподілу) починає спадати у (перед)кризовий період, що робить його індикатором(-передвісником) кризових явищ. Під час криз у розподілі прибутковостей зростає ексцес, а самі хвости стають важчими, на що даний показник реагує передчасно.\n\n\n12.2.5 Динаміка показника асиметрії \\(\\beta\\)\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nmeasure_label = r'$\\beta$'\nfile_name = f\"beta_idx_{symbol}_{window}_{tstep}_{ret_type}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          beta, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name)\n\n\n\n\n\n\n\nРис. 12.21: Динаміка фондового індексу BSESN та показника асиметрії \\(\\beta\\)\n\n\n\n\n\nДинаміка даної міри виглядає набагато хаотичніше у порівнянні з індексом стабільності. Для представлених результатів можна зробити наступний висновок: у передкризовий період даний показник має зростати, вказуючи на значну правосторонню асиметрію розподілу прибутковостей (переважання позитивних прибутковостей). Для кризового періоду цей показник має спадати, вказуючи на домінацію негативних прибутковостей (лівостороння асиметрія розподілу). Даний показник важко розглядати у якості надійного індикатора, оскільки його коливання представляються значними навіть при незначних падіннях представленого фондового індексу.\n\n\n12.2.6 Динаміка параметра зміщення \\(\\mu\\)\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nmeasure_label = r'$\\mu$'\nfile_name = f\"mu_idx_{symbol}_{window}_{tstep}_{ret_type}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          mu, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name)\n\n\n\n\n\n\n\nРис. 12.22: Динаміка фондового індексу BSESN та показника зміщення \\(\\mu\\)\n\n\n\n\n\nПоказник розташування \\(\\mu\\) потроху спадає у кризовий період, демонструючи зміщення розподілу в сторону негативних прибутковостей. Тим не менш, цей розподіл представляєть настільки ж хаотичним як і показник асиметрії \\(\\beta\\).\n\n\n12.2.7 Динаміка параметра масштабу \\(\\sigma\\)\nОголошуємо мітки для рисунків та назви збережених рисунків:\n\nmeasure_label = r'$\\sigma$'\nfile_name = f\"sigma_idx_{symbol}_{window}_{tstep}_{ret_type}\"\n\nВиводимо результат:\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          sigma, \n          ylabel, \n          measure_label,\n          xlabel,\n          file_name)\n\n\n\n\n\n\n\nРис. 12.23: Динаміка фондового індексу BSESN та показника масштабу \\(\\sigma\\)\n\n\n\n\n\nІз представлених результатів видно, що даний показник спадає у (перед)кризовий період, вказуючи на зменшення масштабу (форми) альфа-стабільного розподілу Леві.\nОтже, з усіх 4-ох показників, показник стабільності \\(\\alpha\\) є найкращим для ідентифікації кризових явищ та побудови надійних стратегій ризик-менеджменту.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Лабораторна робота № 12</span>"
    ]
  },
  {
    "objectID": "lab_12.html#висновок",
    "href": "lab_12.html#висновок",
    "title": "12  Лабораторна робота № 12",
    "section": "12.3 Висновок",
    "text": "12.3 Висновок\nСтабільні розподіли — захоплюючий і плідний об’єкт досліджень в теорії ймовірностей; більше того, в даний час вони мають велику цінність при моделюванні складних процесів у фізиці, астрономії, економіці, теорії комунікацій тощо.\nУ даній роботі було представлено теоретичні та чисельні обгрунтування в сторону альфа-стабільного розподілу Леві в якості практичної моделі для кращого розуміння та передбачення кризових явищ у складних системах.\nТут ми представили розрахунки як для усього ряду, так і для його підфрагментів, використовуючи алгоритм ковзного вікна. Виходячи з усього ряду прибутковостей, видно, що хвости їх розподілу далеко виходять за межі Гаусового. Найкраще емпіричний розподіл прибутковостей збігається саме з теоретичним альфа-стабільним розподілом Леві.\nВикористовуючи алгоритм сковзного вікна, ми побачили, що параметри альфа-стабільного розподілу змінюються з часом.\n\n\n\n\n\n\nЛітература для подальшого вивчення степеневих розподілів та важких хвостів\n\n\n\nNassim Taleb’s Statistical Consequences of Fat Tails: Real World Preasymptotics, Epistemology, and Applications (вільно доступна за посиланням)",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Лабораторна робота № 12</span>"
    ]
  },
  {
    "objectID": "lab_12.html#завдання-для-самостійної-роботи",
    "href": "lab_12.html#завдання-для-самостійної-роботи",
    "title": "12  Лабораторна робота № 12",
    "section": "12.4 Завдання для самостійної роботи",
    "text": "12.4 Завдання для самостійної роботи\n\nОберіть варіант ряду у викладача\nДля вашого ряду побудуйте еміричний розподіл його прибутковостей і порівняйте його з розподілом Гауса та альфа-стабільним розподілом Леві. Проаналізуйте отримані результати\nВикористовуючи алгоритм ковзного вікна, дослідіть динаміку 4 ключових показників альфа-стабільного розподілу для вашого ряду і зробіть висновки\n\n\n\n\n\n[1] E. P. White, B. J. Enquist, and J. L. Green, On Estimating the Exponent of Power-Law Frequency Distributions, Ecology 89, 905 (2008).\n\n\n[2] P. Lévy, Calcul Des Probabilités, Par Paul lévy, ... (Gauthier-Villars, 1925).\n\n\n[3] P. Lévy, Theorie de l’addition Des Variables Aleatoires (Gauthier-Villars, 1954).\n\n\n[4] B. V. Gnedenko and A. N. Kolmogorov, Limit Distributions for Sums of Independent Random Variables (Addison-Wesley, 1968).\n\n\n[5] T. J. Kozubowski, M. M. Meerschaert, A. K. Panorska, and H.-P. Scheffler, Operator Geometric Stable Laws, Journal of Multivariate Analysis 92, 298 (2005).\n\n\n[6] A. Alvarez and P. Olivares, Méthodes d’estimation Pour Des Lois Stables Avec Des Applications En Finance, Journal de La Société Française de Statistique 146, 23 (2005).\n\n\n[7] J. P. Nolan, An Algorithm for Evaluating Stable Densities in Zolotarev’s (m) Parameterization, Mathematical and Computer Modelling 29, 229 (1999).\n\n\n[8] A. Bielinskyi, V. N. Soloviev, S. Semerikov, and V. Solovieva, Detecting Stock Crashes Using Levy Distribution, in Proceedings of the Selected Papers of the 8th International Conference on Monitoring, Modeling & Management of Emergent Economy, M3E2-EEMLPEED 2019, Odessa, Ukraine, May 22-24, 2019, edited by A. Kiv, S. Semerikov, V. N. Soloviev, L. Kibalnyk, H. Danylchuk, and A. Matviychuk, Vol. 2422 (CEUR-WS.org, 2019), pp. 420–433.\n\n\n[9] D. Salas-Gonzalez, J. M. Górriz, J. Ramírez, M. Schloegl, E. W. Lang, and A. Ortiz, Parameterization of the Distribution of White and Grey Matter in MRI Using the α-Stable Distribution, Computers in Biology and Medicine 43, 559 (2013).\n\n\n[10] V. M. Zolotarev, One-Dimensional Stable Distributions (American Mathematical Society, 1986).\n\n\n[11] E. F. Fama and R. Roll, Parameter Estimates for Symmetric Stable Distributions, Journal of the American Statistical Association 66, 331 (1971).\n\n\n[12] J. H. McCulloch, Simple Consistent Estimators of Stable Distribution Parameters, Communications in Statistics - Simulation and Computation 15, 1109 (1986).\n\n\n[13] J. H. McCulloch, 13 Financial Applications of Stable Distributions, in Statistical Methods in Finance, Vol. 14 (Elsevier, 1996), pp. 393–425.\n\n\n[14] J. P. Nolan, Maximum Likelihood Estimation and Diagnostics for Stable Distributions, in Lévy Processes: Theory and Applications, edited by O. E. Barndorff-Nielsen, S. I. Resnick, and T. Mikosch (Birkhäuser Boston, Boston, MA, 2001), pp. 379–400.\n\n\n[15] S. Mittnik, S. T. rachev, T. Doganoglu, and D. Chenyao, Maximum Likelihood Estimation of Stable Paretian Models, Mathematical and Computer Modelling 29, 275 (1999).\n\n\n[16] W. H. Dumouchel, Stable Distributions in Statistical Inference: 1. Symmetric Stable Distributions Compared to Other Symmetric Long-Tailed Distributions, Journal of the American Statistical Association 68, 469 (1973).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Лабораторна робота № 12</span>"
    ]
  },
  {
    "objectID": "lab_13.html",
    "href": "lab_13.html",
    "title": "13  Лабораторна робота № 13",
    "section": "",
    "text": "13.1 Теоретичні відомості\nДля сучасних складних систем характерна нерегулярність зв’язків і висока чисельність елементів, яка може досягати десятків і сотень тисяч. Таким системам та їх мережним моделям, які володіють нетривіальними топологічними властивостями, найбільше відповідає термін “комплексні”. Комплексною мережею вважається система, яка\nДана мережа є графом з досить великою кількістю вузлів різної природи, що характеризуються багатовимірним кортежем ознак і динамічно мінливими зв’язками; розподіл ознак вузлів і характеристик зв’язків може бути описаний ймовірнісною моделлю (багатомірним розподілом).\nОсновною причиною підвищення актуальності розробок у області теорії і практики комплексних мереж є результати сучасних досліджень реальних комп’ютерних, біологічних і соціальних мереж. Властивості багатьох реальних мереж істотно відрізняються від властивостей класичних випадкових графів з рівноймовірними зв’язками між вузлами, які донедавна розглядалися в якості їх базисного математичного модельного прототипу, і тому побудову їх моделей було запропоновано здійснювати з використанням зв’язних структур і степеневих розподілів.\nУ теорії комплексних мереж виділяють три основні напрямки:\nКомплексні мережі використовуються для моделювання об’єктів і систем, дослідження яких іншими способами (за допомогою спостереження або активного експерименту) недоцільні або неможливі. Комп’ютерні мережі відносяться до мереж, які постійно ростуть і розвиваються. Серед факторів, що впливають на зростання мережі в першу чергу необхідно відзначити розмір або протяжність локальної мережі, яка визначається відстанню між найвіддаленішими станціями, при якій в нормальному режимі роботи вузлів чітко розпізнаються колізії, і кількість об’єднаних у мережу комп’ютерів. Для Інтернет-мереж цей розмір називається діаметром мережі і складає приблизно 1 км відстані, що дозволяє отримати високу швидкість зв’язку та максимально можливий рівень сервісу. При зростанні мережі збільшується кількість колізій, різко падає її корисна пропускна здатність і швидкодія передавання сигналу. Обмеження мережі за довжиною є передумовою вибору структури мережі, розбиття її на окремі частини (сегменти), появи додаткових серверів з новою мережею зв’язків, проблеми генеруються в контексті технологій так званої “останньої милі”. Спостерігається динаміка зростання мережі, своєрідна кластеризація, сервери виступають центрами утворених кластерів, відбувається просторове позиціонування компонент мережі у вигляді чітких ієрархічних структур.\nМережа розглядається як множина сегментів, кожен з яких закінчується точкою розгалуження або кінцевої вершиною мережі. Вершинами мережі є сервери, комутатори й кінцеві користувачі, загальну кількість яких позначимо \\(N\\). Локальні комп’ютерні мережі є об’єктними прототипами графових структур і тому для їх дослідження застосовують методи теорії графів.\nМоделювання мереж із використанням апарата теорії графів є важливим напрямком досліджень дискретної математики. В останні роки зросла зацікавленість дослідників до складних мереж з великою кількістю вузлів, зокрема до комп’ютерних мереж, структура яких нерегулярна, складна і динамічно розвивається в часі. Для таких мереж доводиться генерувати стохастичні графи з величезною кількістю вершин.\nУ загальному вигляді модель комп’ютерної мережі являє собою випадковий граф, закон взаєморозміщення ребер і вершин для якого задається розподілом ймовірностей.\nУ даний час найпоширенішими є два основних підходи до моделювання складних мереж:\nПерший передбачає генерацію випадкового графа із заздалегідь відомою кількістю вершин і заданими ймовірнісними властивостями. Його ще називають графом Ердоша-Рені зі сталою кількістю вершин \\(N\\). Розподіл ступенів вузлів \\(k\\) для цього графа визначається формулою Пуассона \\(P(k) = \\exp^{-\\left\\langle k \\right\\rangle} \\left\\langle k \\right\\rangle^k / k!\\). Побудова графа здійснюється генеруванням, коли до \\(N\\) відокремлених вершин послідовно додаються ребра, що з’єднують випадковим чином довільні пари вершин. Початково граф складається із сукупності малих вершин, які в процесі генерування з часом розростаються до гігантського кластера зв’язаних між собою вершин, число яких є скінченною частиною загальної кількості \\(N\\). При генерації постійно зростає ймовірність зв’язування вершин, яка досягає з часом деякого критичного значення. В результаті процесу, який має характер фазового переходу, граф спонтанно розростається до гігантського кластера вершин, пов’язаних між собою, що нагадує конденсацію краплі води в перенасиченій парі.\nМодель Ваттса-Строґаца є комп’ютерною моделлю тісного світу. Її побудова зводиться до наступного: розглядається одновимірний, замкнений у кільце, періодичний ланцюг, який складається із \\(N\\) вершин. Спочатку кожну вершину з’єднують з іншими сусідніми, які знаходяться від неї на відстані, не більшій за \\(k\\), а потім кожне ребро з певною ймовірністю \\(m\\) перез’єднується з довільною вершиною, що призводить до трансформації регулярного ланцюга у граф тісного світу (Рис. 13.1). Оскільки в цій моделі кількість ребер є сталою, а ймовірності реалізації графів — різні, то вона зводиться до канонічного ансамблю графів і описує реально існуючі мережі, топологія яких не є ані цілком регулярною, ані цілком випадковою.\nБільшість реальних графів підпорядковуються степеневому закону розподілу \\(P(k)\\). Ці графи побудови мереж описуються моделлю переважного приєднання Барабаші-Альберт. Через далекосяжні взаємодії у системи не існує масштабу зміни характерних величин. Ріст і переважне приєднання є основними механізмами побудови безмасштабних (масштабно-інваріантних) мереж.\nНехай вузол \\(i\\) має \\(k_i\\) зв’язків і він може бути приєднаним (зв’язаним) до інших вузлів \\(k_i\\). Ймовірність приєднання нового вузла до вузла \\(i\\) залежить від ступеня \\(k_i\\) вузла \\(i\\). Величину \\(W(k_i) = k_i/\\sum_{j}k_j\\) називають переважним приєднанням (preferential attachment). Не всі вузли мають однакову кількість зв’язків, тому вони характеризуються функцією розподілу \\(P(k)\\), що визначає ймовірність того, що випадково вибраний вузол має \\(k\\) зв’язків. Для складних мереж функція \\(P(k)\\) відрізняється від розподілу Пуассона для випадкових графів. Для переважної більшості складних мереж спостерігається степенева залежність \\(P(k)\\propto k^{-\\gamma}\\).\nПокажемо, яким чином у межах єдиного алгоритму розрахувати і проаналізувати основні спектральні і топологічні властивості найпростіших графів. Для аналізу мережі досліджують характеристики окремих вузлів (локальні), характеристики мережі в цілому (глобальні) та характеристики мережних підструктур. Числові показники деяких глобальних характеристик мережі можуть бути представлені у вигляді аналітичних узагальнень її локальних характеристик (наприклад — найменше, найбільше, середнє значення локального показника, взяте по всім вузлам). Окрім того, що глобальна характеристика може бути представлена у формі одного числа, це також може бути представлення у вигляді розподілу значень локальної характеристики вузлів по всій мережі.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Лабораторна робота № 13</span>"
    ]
  },
  {
    "objectID": "lab_13.html#теоретичні-відомості",
    "href": "lab_13.html#теоретичні-відомості",
    "title": "13  Лабораторна робота № 13",
    "section": "",
    "text": "складається з великої кількості компонентів;\nдопускає «далекосяжні» зв’язки між компонентами;\nволодіє великомасштабною (у тому числі просторово-часовою) мінливістю.\n\n\n\n\n\nдослідження статистичних властивостей, які характеризують поведінку мереж;\nстворення моделей мереж;\nпрогнозування поведінки при зміні структурних властивостей мереж.\n\n\n\n\n\n\n\nвипадкові пуассонівські графи та узагальнені випадкові графи;\nмодель “тісного світу” Ватса і Строґаца та її узагальнення, еволюційна модель зростання мережі Барабаші й Альберт.\n\n\n\n\n\n\n\n\n\nРис. 13.1: Зростання ступеня випадковості при побудові (Increasing randomness) приводить до трансформації регулярного (Regular) ланцюга у граф тісного світу (Small-world) і далі у випадковий (Random) граф\n\n\n\n\n\n\n\n\n\n\n\n\nПримітка\n\n\n\nЧастина представлених у даній лабораторній ілюстрацій була зроблена із використанням наступної книги  [1]. Також у відкритому доступі наявні вихідні коди програм даної книги: https://github.com/PacktPublishing/Network-Science-with-Python-and-NetworkX-Quick-Start-Guide\n\n\n\n13.1.1 NetworkX\nДля аналізу складних мереж і їх спектральних і топологічних характеристик можна скористатися бібліотекою NetworkX.\nNetworkX дозволяє моделювати, аналізувати та візуалізувати мережі різної природи та складності. Пакет надає класи для представлення декількох типів мереж та реалізацію багатьох алгоритмів, що використовуються в мережній науці. NetworkX відносно простий у встановленні та використанні і має багато вбудованих функцій, тому він ідеально підходить для аналізу мереж різної природи та складності.\nNetworkX є безкоштовним програмним забезпеченням з відкритим вихідним кодом. Це означає, що вихідний код доступний для читання, модифікації та розповсюдження (за певних умов). Сам код доступний за адресою https://github.com/networkx/networkx.\n\n13.1.1.1 Встановлюємо NetworkX\nДля встановлення даної бібліотеки можна скористатися командою:\n\n!pip install networkx\n\nДалі можемо імпортувати відповідні бібліотеки:\n\nimport networkx as nx\nimport matplotlib.pyplot as plt # для візуалізації графіків\nimport numpy as np              # для роботи з матрицями\n\n%matplotlib inline\n\nПам’ятайте, що оператори import знаходяться у верхній частині вашого коду, вказуючи Python завантажити зовнішній модуль. У цьому випадку ми хочемо завантажити NetworkX, але дамо йому короткий псевдонім nx, оскільки нам доведеться вводити його неодноразово, звідси й інструкція as.\nДавайте перевіримо встановлену версію NetworkX. Ми хочемо переконатися, що не використовуємо застарілий пакет.\n\nnx.__version__\n\n'3.1'\n\n\nДалі виконаємо налаштування формату виведення рисунків:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 14,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\n\n\n\n13.1.2 Типи мереж\nПочнемо з простих мереж, представлених у NetworkxX класом Graph.\n\n13.1.2.1 Простий граф (ненаправлений та незважений)\n\n# \"звичайний\" граф є неорієнтованим\nG = nx.Graph()\n\n# дайте кожній вершині \"ім'я\", яке у цьому випадку є літерою.\nG.add_node('a')\n\n# метод add_nodes_from дозволяє додавати вузли з послідовності, у цьому випадку зі списку\nnodes_to_add = ['b', 'c', 'd']\nG.add_nodes_from(nodes_to_add)\n\n# додаємо ребро з 'a' в 'b'\n# оскільки граф неорієнтовний, то порядок не має значення\nG.add_edge('a', 'b')\n\n# так само як і add_nodes_from, ми можемо додавати ребра з послідовності\n# ребра повинні бути задані як 2-кортежі\nedges_to_add = [('a', 'c'), ('b', 'c'), ('c', 'd')]\nG.add_edges_from(edges_to_add)\n\n# будуємо граф\n\nplt.figure(figsize=(6, 4))\n\nnx.draw_networkx(G, with_labels=True)\n\n\n\n\n\n\n\nРис. 13.2: Найпростійший граф\n\n\n\n\n\nІснує багато необов’язкових аргументів для функції draw_network(), щоб налаштувати зовнішній вигляд.\n\nplt.figure(figsize=(6, 4))\n\nnx.draw_networkx(G,\n        with_labels=True,\n        node_color='blue',\n        node_size=1600,\n        font_color='white',\n        font_size=16,\n        )\n\n\n\n\n\n\n\nРис. 13.3: Найпростійший граф із додатковими налаштуваннями фонтів рисунку\n\n\n\n\n\n\n\n13.1.2.2 Зважена мережа\nПовертаючись до випадку неорієнтованих мереж, іноді не всі ребра є рівними. Наприклад, у мережі, що представляє міську систему водопостачання, ребра можуть представляти серію труб, якими вода транспортується з одного місця в інше. Деякі з них можуть мати більшу пропускну здатність, ніж інші. Коли вершини графа можуть мати різну силу зв’язності, мережа називається зваженою, а зв’язність кількісно вимірюється числом, яке називається вагою. Зваженими можуть бути як орієнтовані, так і неорієнтовані мережі. При візуалізації мережі вагу ребер часто вказують, змінюючи товщину або непрозорість ребра. Ваги ребер можна використовувати для представлення різних типів атрибутів.\n\nplt.figure(figsize=(6, 4))\n\n# зважена мережа\nG_weighted = nx.Graph()\n\nG_weighted.add_edge(\"A\",\"B\",weight=6)\nG_weighted.add_edge(\"A\",\"D\",weight=3)\nG_weighted.add_edge(\"A\",\"C\",weight=0.5)\nG_weighted.add_edge(\"B\",\"D\",weight=1)\n\nnx.draw_networkx(G_weighted, with_labels=True)\n\n\n\n\n\n\n\nРис. 13.4: Найпростійший зважений граф\n\n\n\n\n\n\n\n13.1.2.3 Направлений граф\nІноді буває корисно додавати трохи більше деталей до мережі. Ребра, які ми бачили попередньо, не враховують звідки одна вершина прямує або куди; Вони просто з’єднують два вузли, тому їх називають симетричними або неорієнтованими.\nУявіть собі мережу, яка являє собою систему доріг (ребер) і перехресть (вузлів). A мережа з ненаправленими ребрами була б гарним представленням, доки ви не натрапили на вулицю з одностороннім рухом. Ненаправлене ребро припускає, що ви можете рухатися в будь-якому напрямку однаково. Хоча в реальності напрям руху по дорожній смузі матиме значення навіть для вашого життя.\nКоли напрямок має значення, мережа називається орієнтованою (направленою). В направленій мережі кожне ребро має вузол-джерело і вузол-приймач. Як правило, ребро представляє якийсь потік, наприклад, трафік, від джерела до цілі. Але що, якщо не всі з’єднання є односторонніми? Двосторонні з’єднання створюються шляхом поєднання двох спрямованих ребер, що йдуть в протилежних напрямках. У спрямованих мережах ребра зображуються стрілками, що вказують на ціль.\n\nplt.figure(figsize=(6, 4))\n\n# направлений граф\nG_di = nx.DiGraph()\n\nG_di.add_edge(\"A\",\"B\",weight=1)\nG_di.add_edge(\"A\",\"D\",weight=3)\nG_di.add_edge(\"A\",\"C\",weight=1)\nG_di.add_edge(\"B\",\"D\",weight=2)\n\n# створити словник позицій для вузлів\npos = nx.spring_layout(G_di)\n\n# будуємо граф\nnx.draw_networkx_nodes(G_di, pos)\nnx.draw_networkx_edges(G_di, pos)\nnx.draw_networkx_labels(G_di, pos)\n\n# створити словник міток ребер\nedge_labels = {(u, v): d['weight'] for u, v, d in G_di.edges(data=True)}\n\n# створення міток для ребер\nnx.draw_networkx_edge_labels(G_di, pos, edge_labels=edge_labels);\n\n\n\n\n\n\n\nРис. 13.5: Направлений та зважений граф\n\n\n\n\n\nОб’єкт граф має деякі властивості та методи, які надають дані про весь граф.\n\n# Cписок усіх вузлів\nG_di.nodes()\n\nNodeView(('A', 'B', 'D', 'C'))\n\n\n\n# Список усіх ребер  \nG_di.edges()\n\nOutEdgeView([('A', 'B'), ('A', 'D'), ('A', 'C'), ('B', 'D')])\n\n\n\nG_di.edges(data=True) # триплет зі словником (третім йде вага ребра)\n\nOutEdgeDataView([('A', 'B', {'weight': 1}), ('A', 'D', {'weight': 3}), ('A', 'C', {'weight': 1}), ('B', 'D', {'weight': 2})])\n\n\n\nG_di.edges[\"A\",\"B\"] # виводимо вагу, вказуючи цікаві для нас вузли напряму. У результаті отримуємо словник\n\n{'weight': 1}\n\n\n\nG_di.edges[\"A\",\"C\"][\"weight\"] # виводимо вагу, вказуючи цікаві для нас вузли напряму. У результаті отримуємо скаляр\n\n1\n\n\n\npos # виводимо словник координат розташувань вузлів на графіку\n\n{'A': array([-0.02519378,  0.01909902]),\n 'B': array([-0.0183856,  0.41574  ]),\n 'D': array([-0.18244663,  0.56516098]),\n 'C': array([ 0.22602601, -1.        ])}\n\n\nОб’єкти NodeView та EdgeView мають ітератори, тому ми можемо використовувати їх у циклах for:\n\nfor node in G_di.nodes:\n    print(node)\n\nA\nB\nD\nC\n\n\n\nfor edge in G_di.edges:\n    print(edge)\n\n('A', 'B')\n('A', 'D')\n('A', 'C')\n('B', 'D')\n\n\nЗверніть увагу, що ребра подано у вигляді 2-кортежів, так само, як ми їх ввели.\nМи можемо отримати кількість вершин та ребер у графі за допомогою методів number_of_.\n\nG_di.number_of_nodes()\n\n4\n\n\n\nG_di.number_of_edges()\n\n4\n\n\nДеякі методи роботи з графами приймають ребро або вершину як аргумент. Вони надають властивості графа для даного ребра або вершини. Наприклад, метод .neighbors() повертає вершини, пов’язані з даною вершиною:\n\n# список сусідів вершини 'A'\nG_di.neighbors('A')\n\n&lt;dict_keyiterator at 0x272ff7b7810&gt;\n\n\nЗ міркувань продуктивності багато методів для роботи з графами повертають ітератори замість списків. Їх зручно використовувати у циклах:\n\nfor neighbor in G_di.neighbors('A'):\n    print(neighbor)\n\nB\nD\nC\n\n\nІ ви завжди можете використати конструктор list для створення списку з ітератора:\n\nlist(G_di.neighbors('A'))\n\n['B', 'D', 'C']\n\n\nЗверніть увагу на асиметрію в методах роботи з ребрами, таких як has_edge():\n\nG_di.has_edge('A', 'B')\n\nTrue\n\n\n\nG_di.has_edge('B', 'A')\n\nFalse\n\n\nЗамість симетричного зв’язку “сусіди”, вузли в орієнтованих графах мають попередників (successors або “in-neighbours”) і наступників (predecessors або “out-neighbours”):\n\nprint('Попередники вершини B:', list(G_di.successors('B')))\n\nprint('Наступники вершини B:', list(G_di.predecessors('B')))\n\nПопередники вершини B: ['D']\nНаступники вершини B: ['A']\n\n\nСпрямовані графи мають вхідні ступені вершини (in-degree) та вихідні ступені вершини (out-degree), які показують кількість ребер, що ведуть до та від даної вершини, відповідно:\n\nG_di.in_degree('A') # у вершину А не входить жодна вершина (шлях)\n\n0\n\n\n\nG_di.out_degree('A') # з вершини А виходять 3 вершини (шляхи)\n\n3\n\n\nУ NetworkX існує декілька алгоритмів компонування, які можуть бути використані для розміщення вузлів графа при візуалізації, в тому числі\n\nnx.spring_layout(): цей алгоритм використовує примусовий підхід до розміщення вершин. Вузли, які з’єднані ребрами, притягуються один до одного, тоді як вузли, які не з’єднані, відштовхуються. Алгоритм намагається мінімізувати енергію системи, регулюючи положення вузлів.\nnx.circular_layout(): алгоритм розміщує вузли рівномірно по колу.\nnx.spectral_layout(): даний алгоритм використовує власні вектори матриці суміжності графа для розміщення вершин. Власні вектори використовуються для проектування вершин у простір нижчої розмірності, а положення вершин потім визначаються шляхом оптимізації функції вартості.\nnx.random_layout(): вершини розміщуються випадковим чином у заданій обмежувальній області.\nnx.shell_layout(): алгоритм фіксує вершини у вигляді концентричних кіл або оболонок, причому вершини в одній і тій же оболонці мають однакову відстань до центру.\nnx.kamada_kawai_layout(): використовується ітераційний оптимізаційний підхід для розміщення вузлів. Алгоритм намагається мінімізувати навантаження на систему, змінюючи положення вузлів.\nnx.fruchterman_reingold_layout(): варіація алгоритму nx.spring_layout(), і використання силового підходу до розміщення вузлів.\n\nКожен алгоритм компонування має свої сильні та слабкі сторони, і вибір найкращого з них залежить від характеристик графа та цілей візуалізації.\n\n\n13.1.2.4 Знакова мережа\n\nSigned_G = nx.Graph()\n\n# додаємо ребра\nSigned_G.add_edge(\"A\",\"B\",sign=\"*\")\nSigned_G.add_edge(\"A\",\"C\",sign=\"-\")\nSigned_G.add_edge(\"A\",\"d\",sign=\"+\")\n\n# вибір алгоритму компонування\npos = nx.random_layout(Signed_G)\n\n# створити словник кольорів ребер на основі знаку кожного ребра\nedge_colors = {'+': 'green', '-': 'red', \"*\":\"black\"}\ncolors = [edge_colors[Signed_G[u][v]['sign']] for u, v in Signed_G.edges()]\n\n# створити словник стилів ребер на основі знаку кожного ребра\nedge_styles = {'+': 'solid', '-': 'dashed', \"*\":\"dashed\"}\nstyles = [edge_styles[Signed_G[u][v]['sign']] for u, v in Signed_G.edges()]\n\nplt.figure(figsize=(6, 4))\n\n# будуємо граф з кольоровими та стилізованими ребрами\nnx.draw_networkx_nodes(Signed_G, pos, node_color='blue')\nnx.draw_networkx_edges(Signed_G, pos, edge_color=colors, style=styles)\nnx.draw_networkx_labels(Signed_G, pos);\n\n\n\n\n\n\n\nРис. 13.6: Знакова мережа з налаштуваннями кольорів на основі знаку кожного ребра\n\n\n\n\n\n\n\n13.1.2.5 Мультиграф\nМультиграф — це тип графа в NetworkX, який допускає декілька ребер між парою вузлів. Іншими словами, MultiGraph може мати паралельні ребра, в той час як стандартний Graph може мати лише одне ребро між будь-якою парою вузлів. Мультиграф — це мережа, в якій декілька ребер можуть з’єднувати одні й ті ж вузли.\n\nMulti_G = nx.MultiGraph()\n\nMulti_G.add_edge(\"A\",\"B\",relation=\"family\",weight=1)\nMulti_G.add_edge(\"A\",\"C\",relation=\"family\",weight=2)\nMulti_G.add_edge(\"A\",\"B\",relation=\"Work\",weight=3)\nMulti_G.add_edge(\"D\",\"B\",relation=\"Work\",weight=1)\nMulti_G.add_edge(\"B\",\"E\",relation=\"Friend\",weight=2)\n\nplt.figure(figsize=(6, 4))\n\nnx.draw_networkx(Multi_G, with_labels=True);\n\n\n\n\n\n\n\nРис. 13.7: Зважений мультиграф\n\n\n\n\n\n\nplt.figure(figsize=(6, 4))\n\n# компонуємо\npos = nx.spring_layout(Multi_G)\n\n# будуємо вузли\nnx.draw_networkx_nodes(Multi_G, pos, node_color='lightblue', node_size=500)\n\n# будуємо ребра\nedge_labels = {}\nfor u, v, d in Multi_G.edges(data=True):\n    if (u, v) in edge_labels:\n        edge_labels[(u, v)] += \"\\n\" + d[\"relation\"] + \": \" + str(d[\"weight\"])\n    else:\n        edge_labels[(u, v)] = d[\"relation\"] + \": \" + str(d[\"weight\"])\n\nnx.draw_networkx_edge_labels(Multi_G, pos, edge_labels=edge_labels)\nnx.draw_networkx_edges(Multi_G, pos, width=1, alpha=0.7)\n\n# будуємо мітки\nnx.draw_networkx_labels(Multi_G, pos, font_size=10, font_family=\"sans-serif\");\n\n\n\n\n\n\n\nРис. 13.8: Зважений мультиграф із покращеною візуалізацією\n\n\n\n\n\n\n# ребра\nlist(Multi_G.edges())\n\n[('A', 'B'), ('A', 'B'), ('A', 'C'), ('B', 'D'), ('B', 'E')]\n\n\n\n# G.edges(data=True)\nlist(Multi_G.edges(data=True))\n\n[('A', 'B', {'relation': 'family', 'weight': 1}),\n ('A', 'B', {'relation': 'Work', 'weight': 3}),\n ('A', 'C', {'relation': 'family', 'weight': 2}),\n ('B', 'D', {'relation': 'Work', 'weight': 1}),\n ('B', 'E', {'relation': 'Friend', 'weight': 2})]\n\n\n\n# конкретно перелічуючи ребра\nlist(Multi_G.edges(data=\"relation\"))\n\n[('A', 'B', 'family'),\n ('A', 'B', 'Work'),\n ('A', 'C', 'family'),\n ('B', 'D', 'Work'),\n ('B', 'E', 'Friend')]\n\n\n\n# певне ребро \nMulti_G.edges[\"A\",\"B\"] # помилка\n\nValueError: not enough values to unpack (expected 3, got 2)\n\n\n\n#натомість\n# атрибути в мультиграфі\n\ndict(Multi_G[\"A\"][\"B\"])\n\n{0: {'relation': 'family', 'weight': 1}, 1: {'relation': 'Work', 'weight': 3}}\n\n\n\nlist(Multi_G.edges(\"B\"))\n\n[('B', 'A'), ('B', 'A'), ('B', 'D'), ('B', 'E')]\n\n\n\nMulti_G[\"A\"][\"B\"][0][\"relation\"]\n\n'family'\n\n\n\n\n13.1.2.6 Двочастковий (bipartite) граф\nДвочастковий граф — це тип графа, в якому вершини можна розбити на дві непересічні множини так, що всі ребра з’єднують вершину з однієї множини з вершиною в іншій множині. Тобто, не існує ребер, які з’єднують вершини всередині однієї множини.\nДвочасткові графи корисні для моделювання відносин між двома різними типами об’єктів, наприклад, покупцями і продавцями на ринку, або акторами і фільмами в кіноіндустрії.\nУ NetworkX ви можете створювати і маніпулювати двосторонніми графами за допомогою модуля bipartite, який надає різні функції і алгоритми для двосторонніх графів. Крім того, існує декілька методів візуалізації, які можна використовувати для відображення двосторонніх графів, наприклад, двосторонній макет, який розташовує вузли у два окремі рядки.\nПриклад акціонерів та акцій:\n\nfrom networkx.algorithms import bipartite\n\n# список акціонерів\nstockholders = ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Harry', 'Ivy', 'John']\n\n# перелік акцій\nstocks = ['AAPL', 'GOOG', 'TSLA', 'AMZN', 'FB', 'MSFT', 'NVDA', 'PYPL', 'NFLX', 'TWTR']\n\n# створити двочастковий граф\nB = nx.Graph()\n\n# додавання вузлів зі списку\nB.add_nodes_from(stockholders, bipartite=0)\nB.add_nodes_from(stocks, bipartite=1)\n\n# додавання ребер випадковим чином\nimport random\nwhile not nx.is_connected(B):\n    B.add_edge(random.choice(stockholders), random.choice(stocks))\n\nplt.figure(figsize=(8, 6))\n\n# будуємо двочастковий граф\npos = nx.bipartite_layout(B, stockholders)\nnx.draw_networkx(B, pos, with_labels=True)\n\n\n\n\n\n\n\nРис. 13.9: Двочасткова мережа акціонерів та їх акцій\n\n\n\n\n\n\n# 2 набори двочасткових графів\nbipartite.sets(B)\n\n({'Alice',\n  'Bob',\n  'Charlie',\n  'David',\n  'Eve',\n  'Frank',\n  'Grace',\n  'Harry',\n  'Ivy',\n  'John'},\n {'AAPL',\n  'AMZN',\n  'FB',\n  'GOOG',\n  'MSFT',\n  'NFLX',\n  'NVDA',\n  'PYPL',\n  'TSLA',\n  'TWTR'})\n\n\nХоча двочасткові графи корисні для представлення повної структури зв’язків “багато-до-багатьох”, іноді простіше працювати зі стандартними односторонніми мережами. Це може бути у випадку, якщо аналіз фокусується на певному типі вузлів, або якщо необхідна методика доступна лише для односторонніх (одномодальних) мереж, або ж методика доступна лише для одномодових мереж, чи мережа зв’язків має занадто багато вузлів для чіткої візуалізації. На щастя, можна створити одномодові мережі з мережі зв’язків за допомогою процесу, який називається “проєкція”.\nОдномодові мережі, побудовані з мереж зв’язків, називаються мережами спільної приналежності, тому що вузли з’єднуються ребрами, якщо вони мають спільні зв’язки. Існує кілька типів проекцій, які використовуються для створення спільної приналежності, але всі вони обертаються навколо однієї і тієї ж ідеї: з’єднання вузлів зі спільним сусідом у вихідній мережі приналежності. Найпростіша можлива проекція — це незважена проекція, яка створює незважене ребро між вузлами з одним або декількома спільними сусідами. Наступний код використовує функцію projected_graph() для проектування мережі акціонерів, що мають спільні акції компаній:\n\n# Лівобічний граф (акціонери) \n# акціонери, які мають спільні акції, пов'язані між собою\n# Лівобічний граф\nplt.figure(figsize=(6, 4))\n\nP = bipartite.projected_graph(B, bipartite.sets(B)[0])\nnx.draw_networkx(P, with_labels=True, node_size=10)\n\n\n\n\n\n\n\nРис. 13.10: Лівобічний граф мережі акціонерів у котрих наявні спільні акції\n\n\n\n\n\nУ такий самий спосіб ми можемо побудувати мережу акцій:\n\n# Правобічний граф\nplt.figure(figsize=(6, 4))\n\nP = bipartite.projected_graph(B,bipartite.sets(B)[1])\nnx.draw_networkx(P, with_labels=True, node_size=10)\n\n\n\n\n\n\n\nРис. 13.11: Лівобічний граф акцій у котрих наявні спільні акціонери\n\n\n\n\n\n\n# Зважений лівобічний граф\n# як багато спільного\n\nplt.figure(figsize=(6, 4))\n\n# краща візуалізація\nP = bipartite.weighted_projected_graph(B,bipartite.sets(B)[0])\npos = nx.circular_layout(P)\n\n# будуємо граф \nnx.draw_networkx_nodes(P, pos)\nnx.draw_networkx_edges(P, pos)\nnx.draw_networkx_labels(P, pos)\n\n# створюємо словник міток ребер\nedge_labels = {(u, v): d['weight'] for u, v, d in P.edges(data=True)}\n\n# будуємо мітки для ребер \nnx.draw_networkx_edge_labels(P, pos, edge_labels=edge_labels);\n\n\n\n\n\n\n\nРис. 13.12: Лівобічний граф мережі акціонерів, де вагові коефіцієнти на ребрах указують на кількість наявних спільних акцій між акціонерами\n\n\n\n\n\n\n\n\n13.1.3 Імпортуємо інформацію про мережу\n\n13.1.3.1 Імпортуємо дані з file.txt та GEXF\nЩоб імпортувати інформацію про мережу до NetworkX, ви можете скористатися однією з декількох функцій, залежно від формату ваших даних. Ось кілька прикладів:\n\nІмпорт з файлу списку граней:\n\nПрипустимо, у вас є файл списку граней, що мають наступне представлення:\nA B\nA C\nB D\nC D\nD E\nВи можете імпортувати цей файл у граф NetworkX функцією read_edgelist():\n\nG = nx.read_edgelist('databases\\lab_13\\Sample1.txt')\n\nplt.figure(figsize=(6, 4))\nnx.draw_networkx(G, with_labels=True)\n\n\n\n\n\n\n\nРис. 13.13: Простий граф, що був зчитаний з текстового файлу за допомогою функції read_edgelist()\n\n\n\n\n\n\nІмпорт з файлу матриці суміжності:\n\nПрипустимо, що у вас є файл матриці суміжності:\n0 1 1 0 0\n1 0 0 1 0\n1 0 0 1 1\n0 1 1 0 1\n0 0 1 1 0\nВи можете імпортувати цей файл у граф NetworkX за допомогою функції from_numpy_array():\n\nadj_matrix = np.loadtxt('databases\\lab_13\\Sample2.txt')\n\nG = nx.from_numpy_array(adj_matrix)\n\nplt.figure(figsize=(6, 4))\nnx.draw_networkx(G, with_labels=True)\n\n\n\n\n\n\n\nРис. 13.14: Простий граф, що був зчитаний з масиву numpy за допомогою функції from_numpy_array()\n\n\n\n\n\n\nІмпорт з файлу GEXF:\n\nЯкщо у вас є файл мережі у форматі GEXF, який є популярним форматом для обміну даних про графи між різними програмними пакетами, ви можете імпортувати його у граф NetworkX за допомогою функції read_gexf:\nПростий граф у форматі GEXF:\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;gexf xmlns=\"http://www.gexf.net/1.3\" version=\"1.3\"&gt;\n  &lt;meta lastmodifieddate=\"2022-10-01\"&gt;\n    &lt;creator&gt;NetworkX&lt;/creator&gt;\n    &lt;description&gt;An example graph in GEXF format&lt;/description&gt;\n  &lt;/meta&gt;\n  &lt;graph mode=\"static\" defaultedgetype=\"undirected\"&gt;\n    &lt;nodes&gt;\n      &lt;node id=\"0\" label=\"Node 0\"/&gt;\n      &lt;node id=\"1\" label=\"Node 1\"/&gt;\n      &lt;node id=\"2\" label=\"Node 2\"/&gt;\n    &lt;/nodes&gt;\n    &lt;edges&gt;\n      &lt;edge id=\"0\" source=\"0\" target=\"1\"/&gt;\n      &lt;edge id=\"1\" source=\"1\" target=\"2\"/&gt;\n      &lt;edge id=\"2\" source=\"2\" target=\"0\"/&gt;\n    &lt;/edges&gt;\n  &lt;/graph&gt;\n&lt;/gexf&gt;\n\nG = nx.read_gexf('databases\\lab_13\\\\basic.gexf')\n\nplt.figure(figsize=(6, 4))\nnx.draw_networkx(G, with_labels=True)\n\n\n\n\n\n\n\nРис. 13.15: Простий граф, що був зчитаний з файлу формату .gexf за допомогою функції read_gexf()\n\n\n\n\n\n\n# Зберігаємо граф у форматі GEXF\nnx.write_gexf(G, 'databases\\lab_13\\Sample3.gexf')\n\n\n\n13.1.3.2 Матриця суміжності\n\nG_mat = np.array([[0, 1, 1, 1, 0, 1, 0, 0, 0, 0],\n                  [1, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n                  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n                  [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n                  [0, 0, 0, 1, 0, 1, 0, 1, 0, 0],\n                  [1, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n                  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n                  [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n                  [0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n                  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]])\n\nG_mat\n\narray([[0, 1, 1, 1, 0, 1, 0, 0, 0, 0],\n       [1, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n       [1, 1, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 1, 0, 1, 0, 1, 0, 0],\n       [1, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n       [0, 0, 0, 0, 0, 1, 0, 0, 0, 1],\n       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]])\n\n\nПеретворення матриці суміжності у граф за допомогою nx.Graph:\n\nG = nx.Graph(G_mat)\n\nplt.figure(figsize=(6, 4))\nnx.draw_networkx(G)\n\n\n\n\n\n\n\nРис. 13.16: Простий граф, що був побудований напряму з матриці суміжності\n\n\n\n\n\n\n\n\n13.1.4 Графостатистичні показники\n\n13.1.4.1 Ступінь вершини\nНезалежно від того, чи представляють вузли людей, місця, комп’ютери або атоми, розташування вузла в структурі мережі тісно пов’язане з роллю, яку він відіграє в загальній системі. Різні структури уможливлюють різні ролі. Отже, кількісно оцінюючи структурні властивості вузла, можна зрозуміти роль, яку відіграє цей вузол. Числові міри, які характеризують мережеві властивості вузла, називаються мірами центральності. Однією з найпростіших мір центральності є ступенева центральність (degree centrality). Ступенева центральність вузла — це просто кількість сусідів, які наявні у вузла. У соціальній мережі ступенева центральність є мірою популярності, і може бути хорошим способом здогадатися, хто влаштовує найкращі вечірки, хто має найбільшу кількість публікацій або хто є монополістом на ринку праці. Ступенева центральність — це досить елементарний приклад, але далі будуть представлені більш складні міри, які часто використовуються в науці складних мереж. Кожна міра центральності кількісно оцінює різний тип важливості і може бути корисною для відповідей на різні типи питань.\nПоказник ступеневої центральності тісно пов’язаний із такою мірою як ступінь вершини в мережі (node degree), яка визначає кількість ребер, з якими з’єднана конкретна досліджувана вершина. У мережі з \\(N\\) вершин і \\(M\\) ребер, ступінь \\(k_i\\) вершини \\(i\\) визначається як\n\\[\nk_i = \\sum_{j=1}^M A_{ij}.\n\\]\n\\(A\\) — матриця суміжності мережі, \\(A_{ij} = 1\\), якщо існує ребро, що з’єднує вершини \\(i\\) та \\(j\\), і \\(A_{ij} = 0\\) в іншому випадку.\nОколицею вершини \\(i\\) називається множина вершин, які безпосередньо з’єднані з \\(i\\)-им ребром. Околиця \\(i\\) позначається як \\(N_i\\) і визначається як\n\\[\nN_i = \\{j \\mid A_{ij} = 1\\}.\n\\]\nТут \\(A\\) — матриця суміжності мережі; \\(A_{ij} = 1\\), якщо існує ребро, що з’єднує вершини \\(i\\) та \\(j\\), і \\(A_{ij} = 0\\) в іншому випадку.\nРозглянемо приклад ступеню вершини на прикладі графа карате-клубу.\n\n\n\n\n\n\nВідомості про граф карате-клубу\n\n\n\nГраф карате-клубу — це соціальна мережа, що представляє дружбу між 34 членами карате-клубу, як це спостерігав Вейн В. Захарі у 1977 році. Кожна вершина графа представляє члена клубу, а кожне ребро — дружбу між двома членами. Граф має 34 вершини та 78 ребер. Карате-клуб є відомим прикладом аналізу соціальних мереж і використовувався для вивчення різних властивостей мережі, таких як структура спільноти і міра центральності. Граф характеризується розколом клубу на дві фракції, очолювані інструкторами клубів: вершина 1 та вершина 34. Цей розкол був спричинений суперечкою між двома лідерами, яка врешті-решт призвела до утворення двох окремих клубів карате\n\n\n\nG_karate = nx.karate_club_graph()\n\n\nplt.figure(figsize=(6, 4))\nnx.draw_networkx(G_karate)\n\n\n\n\n\n\n\nРис. 13.17: Граф карате-клубу\n\n\n\n\n\n\nnode = 2\nneighborhood = list(nx.neighbors(G_karate, node))\nneighborhood\n\n[0, 1, 3, 7, 8, 9, 13, 27, 28, 32]\n\n\n\n# ступінь = кількість сусідів \nlen(neighborhood)\n\n10\n\n\n\n# ступінь вершини\nG_karate.degree(node)\n\n10\n\n\n\n# Усі ступені вершини\ndict(G_karate.degree)\n\n{0: 16,\n 1: 9,\n 2: 10,\n 3: 6,\n 4: 3,\n 5: 4,\n 6: 4,\n 7: 4,\n 8: 5,\n 9: 2,\n 10: 3,\n 11: 1,\n 12: 2,\n 13: 5,\n 14: 2,\n 15: 2,\n 16: 2,\n 17: 2,\n 18: 2,\n 19: 3,\n 20: 2,\n 21: 2,\n 22: 2,\n 23: 5,\n 24: 3,\n 25: 3,\n 26: 2,\n 27: 4,\n 28: 3,\n 29: 4,\n 30: 4,\n 31: 6,\n 32: 12,\n 33: 17}\n\n\n\nplt.figure(figsize=(6, 4))\nplt.hist(sorted(dict(G_karate.degree).values()))\nplt.xlabel(\"Ступінь вершини\")\nplt.ylabel(\"Частота\")\nplt.show();\n\n\n\n\n\n\n\nРис. 13.18: Гістограма ступенів вершини в графі карате-клубу\n\n\n\n\n\nНа Рис. 13.18 видно, що в мережі карате-клубів наявно достатньо багато учасників клубу, хто має один або декілька зв’язків із іншими членами клубу. Також видно, що серед них є ті, хто має більше 15 знайомих. Представниками з такою кількістю зв’язків можуть бути лідери цих клубів.\n\n\n13.1.4.2 Тріадичне закриття\nМіра, представлена в цьому розділі, стосується зв’язків між сусідами вузла, а не самого вузла. Часто буває корисно розглянути, чи мають сусіди вузла тенденцію бути пов’язаними один з одним. У соціальній мережі це питання зводиться до того, щоб запитати, чи товариш вашого товариша є і вашим товаришем одночасно. Ця властивість відома як транзитивність. Результатом таких стосунків є трикутники: три вузли, пов’язані між собою. Тенденція до виникнення таких трикутників називається кластеризацією. Сильна кластеризація часто свідчить про надійність і надлишковість мережі — якщо один ребро зникає, шлях все ще існує через два інших. Кластеризація вимірюється за допомогою коефіцієнта локальної кластеризації, який визначає тенденцію вузлів об’єднуватись у тріади. Глобальний коефіцієнт кластеризації представляє середнє значення по всім локальним кластеризаціям, що були визначені для кожного вузла мережі.\n\n13.1.4.2.1 Коефіцієнт кластеризації\nКоефіцієнт кластеризації вершини \\(i\\) задається формулою:\n\\[\nC_i = \\sum_{j,k}A_{ij}A_{jk}A_{ki} \\Big/ k_i(k_i - 1),\n\\]\nде \\(k_i=\\sum_{j}A_{ij}\\) — кількість ребер, що входять у вершину \\(i\\); \\(A\\) позначає матрицю суміжності.\n\n# локальна кластеризація \nnx.clustering(G, 2)\n\n0\n\n\n\n# список кластеризацій\nnx.clustering(G)\n\n{0: 0.16666666666666666,\n 1: 0.3333333333333333,\n 2: 0,\n 3: 0.3333333333333333,\n 4: 0,\n 5: 0,\n 6: 0,\n 7: 0,\n 8: 0,\n 9: 0}\n\n\n\n\n13.1.4.2.2 Глобальний коефіцієнт кластеризації\nБагато спостережуваних соціальних мереж є більш кластеризованими, ніж це могло б виникнути випадковим чином\nКоефіцієнт кластеризації мережі є середнім значенням коефіцієнтів кластеризації всіх \\(N\\) вузлів:\n\\[\nC = N^{-1}\\sum_{i=1}^{N} C_i.\n\\]\n\n# середній ступінь кластеризації\nnx.average_clustering(G)\n\n0.08333333333333333\n\n\n\n\n13.1.4.2.3 Транзитивність\nТранзитивність — це властивість мережі, яка вимірює ймовірність того, що якщо два вузли мережі мають спільного сусіда, то вони також будуть безпосередньо з’єднані один з одним. Іншими словами, вона вимірює тенденцію до утворення “трикутників” у мережі.\nФормально транзитивність мережі визначається як відношення кількості трикутників у мережі до кількості з’єднаних трійок вузлів (тобто трійок вузлів, які безпосередньо з’єднані один з одним або мають спільного сусіда). У математичній нотації транзитивність мережі позначається як\n\\[\nT = \\sum_{i,k,j=1}^{N}A_{ik}A_{kj}A_{ji} \\Bigg/ \\sum_{i,k,j=1}^{N}A_{ik}A_{ji}.\n\\]\nВисока транзитивність вказує на те, що вузли в мережі мають тенденцію до утворення трикутних кластерів або спільнот, тоді як низька транзитивність вказує на те, що мережа є більш випадковою або децентралізованою структурою. Транзитивність тісно пов’язана з поняттям коефіцієнта кластеризації, який вимірює схильність вузлів до утворення локальних кластерів або спільнот.\n\n#транзитивність\n#transitivity зважує вершини з великим ступенем вершини\nnx.transitivity(G)\n\n0.15789473684210525\n\n\n\n\n\n13.1.4.3 Шлях\nШлях між двома вузлами \\(A\\) та \\(B\\) у мережі - це послідовність вузлів \\(A, X_1, X_2, ..., X_n, B\\) та послідовність ребер \\((A, X_1), (X_1, X_2), ..., (X_n, B)\\), де кожен вузол та ребро у послідовності є суміжним з попереднім та наступним вузлом або ребром у послідовності.\nДовжина шляху — це кількість ребер у ньому. Шлях довжиною 1 — це ребро між двома вершинами, шлях довжиною 2 — послідовність з двох ребер і трьох вершин, і так далі. Найкоротший шлях між двома вершинами — це шлях мінімальної довжини, який їх з’єднує.\n\n# згенерувати усі прості шляхи між вершинами 1 та 3\npaths = nx.all_simple_paths(G, source=1, target=3)\n\n# перетворити генератор у список\nPath_List = [path for path in paths]\n\nprint(\"Список шляхів:\", Path_List)\n\nСписок шляхів: [[1, 0, 3], [1, 0, 5, 4, 3], [1, 3]]\n\n\n\nPath1 = Path_List[0]\n# перевірити, чи є шлях простим у графі\nis_valid = nx.is_simple_path(G, Path1) # Простий шлях - це шлях, який не містить жодної вершини, що повторюється.\nprint(\"Чи є шлях простим?\", is_valid)\n\nЧи є шлях простим? True\n\n\n\n# хибний приклад\nnx.is_simple_path(G, [0,8,5])\n\nFalse\n\n\n\n# формуємо список ребер, що формують шлях\nedge_list = [(Path1[i], Path1[i+1]) for i in range(len(Path1)-1)] # len(Path1)-1 = довжина шляху\nedge_list\n\n[(1, 0), (0, 3)]\n\n\n\n# обчислюємо вагу шляху\nweight = sum(G[u][v]['weight'] for u, v in edge_list if 'weight' in G[u][v])\nprint(\"Вага шляху:\", weight)\n\nВага шляху: 2\n\n\n\n13.1.4.3.1 Геодезична лінія\nГеодезичний шлях між двома вузлами \\(A\\) і \\(B\\) в мережі — це найкоротший шлях, який їх з’єднує. Іншими словами, це шлях з мінімальною кількістю ребер, які потрібно пройти, щоб дістатися з вузла \\(A\\) до вузла \\(B\\). Довжина геодезичного шляху — це кількість ребер у цьому шляху.\n\n# геодезичний шлях = найкоротший шлях \nnx.shortest_path(G, 1, 2)\n\n[1, 0, 2]\n\n\n\n# обчислити найкоротший шлях між двома вузлами\npath = nx.shortest_path(G, source=1, target=3)\n\n# обчислити відповідні ребра шляху\nedges = [(path[i], path[i+1]) for i in range(len(path)-1)]\n\nplt.figure(figsize=(6, 4))\n\n# будуємо граф та шлях \npos = nx.circular_layout(G)\nnx.draw_networkx(G, pos, with_labels=True)\nnx.draw_networkx_edges(G, pos, edgelist=edges, edge_color='r', width=3);\n\n\n\n\n\n\n\nРис. 13.19: Граф із виділеним найкоротшим шляхом між вузлами 1 і 3\n\n\n\n\n\n\n# геодезична довжина \nnx.shortest_path_length(G, 1, 2)\n\n2\n\n\nПошук геодезичного шляху від вузла \\(i\\) до кожного іншого вузла є обчислювально складним, тому нам потрібен ефективний алгоритм для цього.\nТут ми використовуємо пошук у ширину  [2]:\n\n# алгоритм пошуку в ширину\nT = nx.bfs_tree(G, 1)\n\nplt.figure(figsize=(6, 4))\nnx.draw_networkx(T, with_labels=True)\n\n\n\n\n\n\n\nРис. 13.20: Повертає орієнтоване дерево, побудоване на основі пошуку в ширину, починаючи з джерела\n\n\n\n\n\n\n# усі найкоротші шляхи \nnx.shortest_path_length(G, 1) # виводимо словник \n\n{1: 0, 0: 1, 3: 1, 6: 1, 2: 2, 5: 2, 4: 2, 8: 3, 7: 3, 9: 4}\n\n\n\n# середній найкоротший шлях \nnx.average_shortest_path_length(G)\n\n2.4\n\n\n\n\n13.1.4.3.2 Зв’язні компоненти\nУ простій мережі для кожної пари вершин можна знайти шлях, який їх з’єднує. Це і є визначенням зв’язного графа. Ми можемо перевірити цю властивість для заданого графа:\n\nnx.is_connected(G)\n\nTrue\n\n\nНе кожен граф зв’язний:\n\nG_test = nx.Graph()\n\nnx.add_cycle(G_test, (1,2,3))\nG_test.add_edge(4, 5)\n\n\nplt.figure(figsize=(6, 4))\nnx.draw_networkx(G_test, with_labels=True)\n\n\n\n\n\n\n\nРис. 13.21: Розв’язний граф із циклічним шляхом\n\n\n\n\n\n\nnx.is_connected(G_test)\n\nFalse\n\n\nА NetworkX видасть помилку, якщо ви запитаєте шлях між вузлами, якого не існує:\n\nnx.has_path(G_test, 3, 5)\n\nFalse\n\n\n\nnx.shortest_path(G_test, 3, 5)\n\nNetworkXNoPath: No path between 3 and 5.\n\n\nВізуально ми можемо ідентифікувати дві пов’язані компоненти на графі. Давайте перевіримо це:\n\nnx.number_connected_components(G_test)\n\n2\n\n\nФункція nx.connected_components() отримує граф і повертає список наборів імен вершин, по одному такому набору для кожної зв’язної компоненти. Перевірте, чи відповідають дві множини у наступному списку двом зв’язним компонентам на рисунку графа вище:\n\nlist(nx.connected_components(G_test))\n\n[{1, 2, 3}, {4, 5}]\n\n\nЯкщо ви не знайомі з множинами у Python, це колекції елементів без дублікатів. Вони корисні для збору імен вузлів, оскільки імена вузлів повинні бути унікальними. Як і у випадку з іншими колекціями, ми можемо отримати кількість елементів у множині за допомогою функції len:\n\ncomponents = list(nx.connected_components(G_test))\nlen(components[0])\n\n3\n\n\nНас часто цікавить найбільша зв’язна компонента, яку іноді називають ядром мережі. Ми можемо скористатися вбудованою функцією max у Python, щоб отримати найбільший зв’язну компоненту. За замовчуванням функція max у Python сортує дані у лексикографічному (тобто алфавітному) порядку, що не є корисним у даному випадку. Ми хочемо отримати максимальний зв’язаний компонент при сортуванні в порядку його розміру, тому ми передаємо len як ключову функцію:\n\nmax(nx.connected_components(G_test), key=len)\n\n{1, 2, 3}\n\n\nХоча часто достатньо мати лише список назв вершин, іноді нам потрібен власне підграф, що містить найбільш зв’язну вершину. Один із способів отримати її — передати список назв вершин у функцію G.subgraph():\n\ncore_nodes = max(nx.connected_components(G_test), key=len)\ncore = G.subgraph(core_nodes)\n\nplt.figure(figsize=(6, 4))\nnx.draw_networkx(core, with_labels=True)\n\n\n\n\n\n\n\nРис. 13.22: Граф, що представляє підмножину з найбільш зв’язних компонент\n\n\n\n\n\nТі з вас, хто використовує завершення написання коду за допомогою табуляції, також помітять функцію nx.connected_component_subgraphs(). Її також можна використати для отримання основного підграфа, але представлений метод є більш ефективним, якщо вас цікавить найбільша зв’язна компонента.\n\n\n13.1.4.3.3 Направлені шляхи та компоненти\nДавайте поширимо ці ідеї про шляхи та зв’язні компоненти на орієнтовані графи.\n\nD = nx.DiGraph()\nD.add_edges_from([\n    (1,2),\n    (2,3),\n    (3,2), (3,4), (3,5),\n    (4,2), (4,5), (4,6),\n    (5,6),\n    (6,4),\n])\n\nplt.figure(figsize=(6, 4))\nnx.draw_networkx(D, with_labels=True)\n\n\n\n\n\n\n\nРис. 13.23: Простий орієнтовний граф\n\n\n\n\n\nМи знаємо, що в орієнтованому графі ребро з довільної вершини \\(u\\) до довільної вершини \\(v\\) не говорить про те, що існує ребро з \\(v\\) до \\(u\\). Тобто, для направленого графа ми спостерігатимемо асиметрію шляхів. Зверніть увагу, що цей граф має шлях від 1 до 4, але не у зворотному напрямку.\n\nnx.has_path(D, 1, 4)\n\nTrue\n\n\n\nnx.has_path(D, 4, 1)\n\nFalse\n\n\nІнші функції NetworkX, що працюють зі шляхами, також враховують цю асиметрію:\n\nnx.shortest_path(D, 2, 5)\n\n[2, 3, 5]\n\n\n\nnx.shortest_path(D, 5, 2)\n\n[5, 6, 4, 2]\n\n\nОскільки немає ребра з 5 в 3, найкоротший шлях з 5 в 2 не може просто пройти назад по найкоротшому шляху з 2 в 5 — він повинен пройти довшим шляхом через вузли 6 і 4.\nНаправлені мережі мають два типи зв’язності. Сильно зв’язні означають, що між кожною парою вузлів існує спрямований шлях, тобто з будь-якого вузла ми можемо дістатися до будь-якого іншого вузла, дотримуючись спрямованості ребер. Уявіть собі автомобілі на мережі вулиць з одностороннім рухом: вони не можуть їхати проти потоку транспорту.\n\nnx.is_strongly_connected(D)\n\nFalse\n\n\nСлабка зв’язність говорить про те, що між кожною парою вузлів існує шлях, незалежно від напрямку. Подумайте про пішоходів у мережі вулиць з одностороннім рухом: вони ходять по тротуарах, тому їх не хвилює напрямок руху.\n\nnx.is_weakly_connected(D)\n\nTrue\n\n\nЯкщо мережа сильно зв’язана, вона також є і слабко зв’язаною. Зворотне не завжди вірно, як видно з цього прикладу.\nФункція is_connected для неорієнтованих графів видасть помилку, якщо задано орієнтований граф.\n\n# Це призведе до помилки\nnx.is_connected(D)\n\nNetworkXNotImplemented: not implemented for directed type\n\n\nУ випадку направленого графа замість nx.connected_components тепер маємо nx.weak_connected_components та nx.strong_connected_components:\n\nlist(nx.weakly_connected_components(D))\n\n[{1, 2, 3, 4, 5, 6}]\n\n\n\nlist(nx.strongly_connected_components(D))\n\n[{2, 3, 4, 5, 6}, {1}]\n\n\n\n\n\n13.1.4.4 Ексцентриситет\nЕксцентриситет вершини \\(u\\) в мережі — це максимальна відстань між \\(u\\) та будь-якою іншою вершиною мережі. Іншими словами, це максимальна довжина найкоротшого шляху між \\(u\\) та будь-якою іншою вершиною. Ексцентриситет мережі — це максимальний ексцентриситет будь-якого вузла мережі.\n\n# ексцентриситет\n# найбільша відстань між n та всіма іншими вершинами:\nnx.eccentricity(G)\n\n{0: 3, 1: 4, 2: 4, 3: 4, 4: 3, 5: 3, 6: 5, 7: 4, 8: 4, 9: 5}\n\n\n\n# діаметр: max Ексцентриситет між двома вузлами у всій мережі (max max)\nnx.diameter(G)\n\n5\n\n\n\n# Діаметр - максимальний ексцентриситет\nmax(nx.eccentricity(G).values())\n\n5\n\n\n\n# радіус: min Ексцентриситет між двома вузлами у всій мережі (min max)\nnx.radius(G)\n\n3\n\n\n\n# радіус - мінімальний ексцентриситет\nmin(nx.eccentricity(G).values())\n\n3\n\n\n\n# периферія\n# Ексцентриситет=діаметр\nnx.periphery(G)\n\n[6, 9]\n\n\n\n# центр графа: Ексцентриситет = радіус\nnx.center(G)\n\n[0, 4, 5]\n\n\n\n\n13.1.4.5 Центральність\nНезалежно від того, чи представляють вузли людей, місця, комп’ютери або атоми, розташування вузла в структурі мережі тісно пов’язане з роллю, яку він відіграє в загальній системі. Різні структури уможливлюють різні ролі. Отже, кількісно оцінюючи структурні властивості вузла, можна зрозуміти роль, яку відіграє цей вузол. Числові міри, які характеризують мережні властивості вузла, називаються мірами центральності. Центральність часто вводять як міру важливості, але є багато способів, у які вузол може бути важливим. Наприклад, однією з найпростіших мір центральності є ступенева центральність. Ступенева центральність вузла — це просто кількість сусідів, яких він має (у спрямованій мережі є як ступеневі, так і неступеневі сусіди). У соціальній мережі ступенева центральність є мірою популярності.\n\n13.1.4.5.1 Ступенева центральність — ненаправлені графи\nСтупенева центральність — це міра важливості вузла в мережі, що базується на кількості зв’язків, які він має з іншими вузлами. Ступеневу центральність вершини \\(i\\) можна обчислити як \\(C_D(i) = k_i/(n-1)\\), де \\(k_i\\) — ступінь вершини \\(i\\), тобто кількість ребер, інцидентних вершині, а \\(n\\) — загальна кількість вершин у мережі. Знаменник \\(n-1\\) використовується для того, щоб врахувати той факт, що вершина не може бути з’єднана сама з собою.\nСтупенева центральність вузла коливається від 0 до 1, причому більше значення вказує на те, що вузол є більш центральним у мережі. Вузли з високою ступеневою центральністю, як правило, добре пов’язані з іншими вузлами, і їх видалення з мережі може мати значний вплив на її зв’язність.\nРозглянемо деякі показники на прикладі графа карате-клубу.\n\n# Карате-клуб\nG_karate = nx.karate_club_graph()\nG_karate = nx.convert_node_labels_to_integers(G_karate, first_label=1)\n\n\nplt.figure(figsize=(6, 4))\n\n# Встановіть положення вузлів за допомогою конструктора Камада-Каваї\npos = nx.kamada_kawai_layout(G_karate)\n\n# Будуємо граф з червоними вузлами для вузла 0 (інструктор клубу) і вузла 33 (член клубу): тепер це 1 і 34.\nred_nodes = [1, 34]\nnode_colors = ['red' if node in red_nodes else 'blue' for node in G_karate.nodes()]\nnx.draw_networkx_nodes(G_karate, pos, node_color=node_colors)\nnx.draw_networkx_edges(G_karate, pos)\n\n# Будуємо мітки для вузлів\nnx.draw_networkx_labels(G_karate, pos);\n\n\n\n\n\n\n\nРис. 13.24: Граф карате-клубу з виокремлини лідерами двох фракцій\n\n\n\n\n\n\n# ступеневі центральності\ndegCent = nx.degree_centrality(G_karate)\ndegCent\n\n{1: 0.48484848484848486,\n 2: 0.2727272727272727,\n 3: 0.30303030303030304,\n 4: 0.18181818181818182,\n 5: 0.09090909090909091,\n 6: 0.12121212121212122,\n 7: 0.12121212121212122,\n 8: 0.12121212121212122,\n 9: 0.15151515151515152,\n 10: 0.06060606060606061,\n 11: 0.09090909090909091,\n 12: 0.030303030303030304,\n 13: 0.06060606060606061,\n 14: 0.15151515151515152,\n 15: 0.06060606060606061,\n 16: 0.06060606060606061,\n 17: 0.06060606060606061,\n 18: 0.06060606060606061,\n 19: 0.06060606060606061,\n 20: 0.09090909090909091,\n 21: 0.06060606060606061,\n 22: 0.06060606060606061,\n 23: 0.06060606060606061,\n 24: 0.15151515151515152,\n 25: 0.09090909090909091,\n 26: 0.09090909090909091,\n 27: 0.06060606060606061,\n 28: 0.12121212121212122,\n 29: 0.09090909090909091,\n 30: 0.12121212121212122,\n 31: 0.12121212121212122,\n 32: 0.18181818181818182,\n 33: 0.36363636363636365,\n 34: 0.5151515151515151}\n\n\n\n# сортування за ступеневою центральністю\nsorted_degcent = {k: v for k, v in sorted(degCent.items(), key=lambda item: item[1], reverse=True)}\nsorted_degcent\n\n{34: 0.5151515151515151,\n 1: 0.48484848484848486,\n 33: 0.36363636363636365,\n 3: 0.30303030303030304,\n 2: 0.2727272727272727,\n 4: 0.18181818181818182,\n 32: 0.18181818181818182,\n 9: 0.15151515151515152,\n 14: 0.15151515151515152,\n 24: 0.15151515151515152,\n 6: 0.12121212121212122,\n 7: 0.12121212121212122,\n 8: 0.12121212121212122,\n 28: 0.12121212121212122,\n 30: 0.12121212121212122,\n 31: 0.12121212121212122,\n 5: 0.09090909090909091,\n 11: 0.09090909090909091,\n 20: 0.09090909090909091,\n 25: 0.09090909090909091,\n 26: 0.09090909090909091,\n 29: 0.09090909090909091,\n 10: 0.06060606060606061,\n 13: 0.06060606060606061,\n 15: 0.06060606060606061,\n 16: 0.06060606060606061,\n 17: 0.06060606060606061,\n 18: 0.06060606060606061,\n 19: 0.06060606060606061,\n 21: 0.06060606060606061,\n 22: 0.06060606060606061,\n 23: 0.06060606060606061,\n 27: 0.06060606060606061,\n 12: 0.030303030303030304}\n\n\n\n# ступенева центральність вузла\n\ndegCent[34]\n\n0.5151515151515151\n\n\n\n# відобразити мережу з розмірами вершин на основі їх ступеневої центральності\nplt.figure(figsize=(6, 4))\n\n# створити список розмірів вершин на основі ступеневої центральності\nnode_sizes = [10000*v*v for v in degCent.values()]\n\n# будуємо граф\nnx.draw_networkx(G_karate, with_labels=True, node_size=node_sizes,pos=nx.spring_layout(G_karate))\n\n\n\n\n\n\n\nРис. 13.25: Граф карате-клубу зі збільшеними вершинами на основі їх ступеневої центральності\n\n\n\n\n\n\n# кольори на основі ступеневої центральності\nnode_colors = [v for v in degCent.values()]\n\nplt.figure(figsize=(6, 4))\n# будуємо граф\nnx.draw_networkx(G_karate, \n                 with_labels=True, \n                 node_size=node_sizes,\n                 pos=nx.spring_layout(G_karate), \n                 node_color=node_colors, \n                 cmap=plt.cm.PuBu)\n\n# PuBu розшифровується як \"Pu\" (фіолетовий) - \"Bu\" (синій), \n# і це послідовна карта кольорів, яка варіюється від світло-фіолетового до темно-синього.\n\n\n\n\n\n\n\nРис. 13.26: Граф карате-клубу з виокремленими вершинами на основі їх ступеневої центральності за допомогою різної палітри кольорів\n\n\n\n\n\n\n\n13.1.4.5.2 Ступенева центральність — направлені графи\n\n# направлений граф \nG = nx.DiGraph()\n\nG.add_edge(\"A\",\"B\")\nG.add_edge(\"A\",\"D\")\nG.add_edge(\"A\",\"C\")\nG.add_edge(\"B\",\"D\")\n\nplt.figure(figsize=(6, 4))\n# будуємо вузли з мітками\nnx.draw_networkx(G, with_labels=True)\n\n\n\n\n\n\n\nРис. 13.27: Приклад направленого графа\n\n\n\n\n\n\n# вхідний ступінь вершини\nindegCent = nx.in_degree_centrality(G)\nindegCent\n\n{'A': 0.0,\n 'B': 0.3333333333333333,\n 'D': 0.6666666666666666,\n 'C': 0.3333333333333333}\n\n\n\n# вихідний\noutdegCent = nx.out_degree_centrality(G)\noutdegCent\n\n{'A': 1.0, 'B': 0.3333333333333333, 'D': 0.0, 'C': 0.0}\n\n\n\n# конкретна вершина\noutdegCent[\"A\"]\n\n1.0\n\n\n\n\n13.1.4.5.3 Ступінь близькості\nМіра, відома як ступінь близькості, є однією з найстаріших мір центральності, що використовується в мережній науці, запропонована соціологом Алексом Бавеласом у 1950 році. Близькість визначається як зворотна величина до віддаленості. Що таке віддаленість? Більш зрозуміло, віддаленість вузла — це сума відстаней між цим вузлом і всіма іншими вузлами. Отже, вузол з високою центральністю близькості знаходиться буквально поруч з іншими вузлами. Центральність вузла вимірює, наскільки швидко він може поширювати інформацію або вплив по всій мережі, оскільки вузли з меншою середньою відстанню до всіх інших вузлів можуть спілкуватися більш ефективно. Крім того, вузли з високим показником центральності часто розташовані в центрі мережі, і їх видалення може мати значний вплив на зв’язність мережі.\nСтупінь близькості вузла \\(i\\) можна обчислити як \\(C_C(i) = ( \\sum_{j \\neq i} d_{ij} )^{-1}\\), де \\(d_{ij}\\) — найкоротша відстань між вузлами \\(i\\) та \\(j\\). Ступінь близькості вузла коливається від 0 до 1, причому більше значення вказує на меншу середню відстань до всіх інших вузлів мережі.\nУ наступному прикладі використовується функція NetworkX closeness_centrality() для обчислення значень центральності для мережі карате клубу та відображення 10 найближчих один до одного каратистів:\n\ncloseness = nx.closeness_centrality(G_karate)\nsorted(closeness.items(), key=lambda x:x[1], reverse=True)[:10]\n\n[(1, 0.5689655172413793),\n (3, 0.559322033898305),\n (34, 0.55),\n (32, 0.5409836065573771),\n (9, 0.515625),\n (14, 0.515625),\n (33, 0.515625),\n (20, 0.5),\n (2, 0.4852941176470588),\n (4, 0.4647887323943662)]\n\n\n\n# намалювати мережу з розмірами вершин на основі їх ступеня близькості\n\n# створити список розмірів вершин на основі ступеня близькості\nnode_sizes = [3000*v*v for v in closeness.values()]\n\n# кольори на основі ступеневої близькості\nnode_colors = [v for v in closeness.values()]\n\n# будуємо граф\nplt.figure(figsize=(6, 4))\nnx.draw_networkx(G_karate, \n                 with_labels=True, \n                 node_size=node_sizes, \n                 pos=nx.spring_layout(G_karate), \n                 node_color=node_colors, \n                 cmap=plt.get_cmap('plasma'))\n\n\n\n\n\n\n\nРис. 13.28: Граф карате-клубу з виокремлини вершинами на основі їх ступеня близькості\n\n\n\n\n\n\n\n13.1.4.5.4 Ступінь посередництва\nУ популярній дитячій грі “Телефон” один гравець починає з того, що шепоче повідомлення іншому, той шепоче це повідомлення іншому і так далі. Врешті-решт, останній гравець промовляє повідомлення вголос. Як правило, фінальне повідомлення не має нічого спільного з початковим. Кожного разу, коли повідомлення передається від людини до людини, воно може змінюватися, можливо, через те, що його неправильно почули, а можливо, через те, що його навмисно змінили. У більш складних соціальних мережах, таких як організації та громадські рухи, особи, які з’єднують різні частини мережі, мають найбільші можливості фільтрувати, посилювати та змінювати інформацію. Таких людей називають брокерами, а ребра, що з’єднують віддалені частини мережі, — мостами. Важливість таких вузлів і ребер не обмежується соціальними мережами. У потокових мережах — таких як залізниці, водопроводи та телекомунікаційні системи — вузли що з’єднують віддалені частини мережі, можуть діяти як вузькі місця, обмежуючи обсяг потоку. Виявлення таких вузьких місць дає змогу збільшити їхню пропускну здатність і захистити їх від збоїв та атак. Мости і брокери важливі, тому що вони знаходяться між різними частинами мережі. Відповідно, тип центральності, який використовується для визначення мостів і брокерів називається cтупенем посередництва.\nСтупінь посередництва — це міра того, наскільки вузол лежить на найкоротших шляхах між іншими вузлами мережі. Ступінь посередництва для вузла \\(i\\) можна обчислити як \\(C_B(i) = \\sum_{s \\neq i \\neq t} \\sigma_{st}(i)/\\sigma_{st}\\), в якій \\(s\\) і \\(t\\) — два вузли мережі, \\(\\sigma_{st}\\) — загальна кількість найкоротших шляхів між \\(s\\) і \\(t\\), а \\(\\sigma_{st}(i)\\) — кількість найкоротших шляхів між \\(s\\) і \\(t\\), які проходять через вузол \\(i\\).\nСтупінь посередництва змінюється від 0 до 1, причому більше значення вказує на більшу кількість найкоротших шляхів, що проходять через вершину. Вузли з високим значенням центральності часто розташовані на “мостах” між різними кластерами або спільнотами в мережі, і їх видалення може мати значний вплив на зв’язність мережі.\nСтупінь посередництва базується на припущенні, що чим більше найкоротших шляхів проходить через вершину (або ребро), тим більше вона виступає в ролі брокера (або моста). Для ступеня посередництва знаходять найкоротші шляхи між кожною парою вузлів. Значення ступеня посередництва для вузла або ребра — це просто кількість цих шляхів, що проходять через нього. На наступній діаграмі показано приклад мережі та розраховані значення посередництва для кожної вершини та ребра. Для кожної пари вершин показано найкоротший шлях (за винятком тривіальних шляхів довжиною 1). Посередництво вузла — це сума шляхів, які проходять через цей вузол. Посередництво ребра — це кількість нетривіальних шляхів, які проходять через це ребро, плюс 1 для самого ребра.\n\n\n\n\n\n\nРис. 13.29: Усі нетривіальні найкоротші шляхи та отримані центри посередництва\n\n\n\nСтупінь посередництва між вузлами легко обчислюється в NetworkX за допомогою функції betweenness_centrality(). Ця функція повертає словник, який зіставляє позначення вузлів зі значеннями посередництва. Якщо аргумент normalized має значення True (за замовчуванням), значення ступеня посередництва ділиться на кількість пар вузлів, що може бути корисним для порівняння значень посередництва, що мають різні масштаби. Якщо аргумент endpoints має значення True (за замовчуванням False), то кінцеві точки шляху будуть включені в розрахунок посередництва.\n\nbtwnCent = nx.betweenness_centrality(G_karate, endpoints = False)\nsorted(btwnCent.items(), key=lambda x:x[1], reverse=True)[:10]\n\n[(1, 0.43763528138528146),\n (34, 0.30407497594997596),\n (33, 0.145247113997114),\n (3, 0.14365680615680618),\n (32, 0.13827561327561325),\n (9, 0.05592682780182781),\n (2, 0.053936688311688304),\n (14, 0.04586339586339586),\n (20, 0.03247504810004811),\n (6, 0.02998737373737374)]\n\n\n\n# відобразити мережу з розмірами вершин на основі їх ступеня посередництва\n\n# створити список розмірів вершин на основі ступеня посередництва\nnode_sizes = [10000*v*v for v in btwnCent.values()]\n\n# кольори на основі ступеня посередництва\nnode_colors = [v for v in btwnCent.values()]\n\n# будуємо граф\nplt.figure(figsize=(6, 4))\nnx.draw_networkx(G_karate, with_labels=True, \n        node_size=node_sizes, \n        pos=nx.spring_layout(G_karate), \n        node_color=node_colors, \n        cmap=plt.get_cmap('viridis'))\n\n\n\n\n\n\n\nРис. 13.30: Граф карате-клубу з виокремлини вершинами на основі їх ступеня посередництва вершин\n\n\n\n\n\nВидно, що високим рівнем посередництва характеризуються вершини 1, 34 і 33. Високий рівень посередництва між ними свідчить про те, що ці особи є важливими інформаційними посередниками в клубі карате. Можливо, вони є найбільш вправними каратистами.\nСтупінь посередництва для ребер — це міра того, наскільки ребро лежить на найкоротших шляхах між іншими ребрами в мережі. Посередництво ребра \\(e\\) можна обчислити так:\n\\[\nC_B(e) = \\sum_{s \\neq e \\neq t} \\sigma_{st}(e) / \\sigma_{st}.\n\\]\n\\(s\\) і \\(t\\) — дві вершини мережі, \\(\\sigma_{st}\\) — загальна кількість найкоротших шляхів між \\(s\\) і \\(t\\), а \\(\\sigma_{st}(e)\\) — кількість найкоротших шляхів між \\(s\\) і \\(t\\), які проходять через ребро \\(e\\).\nСтупінь посередництва ребра змінюється від 0 до 1, причому більше значення вказує на більшу кількість найкоротших шляхів, які проходять через ребро. Ребра з високою посередництвом часто розташовані на “мостах” між різними кластерами або спільнотами в мережі, і їх видалення може мати значний вплив на зв’язність мережі.\n\nbtwnCent_edge = nx.edge_betweenness_centrality(G_karate, normalized=True)\nsorted(btwnCent_edge.items(), key=lambda x:x[1], reverse=True)[:10] \n\n[((1, 32), 0.1272599949070537),\n ((1, 7), 0.07813428401663695),\n ((1, 6), 0.07813428401663694),\n ((1, 3), 0.0777876807288572),\n ((1, 9), 0.07423959482783014),\n ((3, 33), 0.06898678663384543),\n ((14, 34), 0.06782389723566191),\n ((20, 34), 0.05938233879410351),\n ((1, 12), 0.058823529411764705),\n ((27, 34), 0.0542908072319837)]\n\n\n\n# візуалізувати мережу з розмірами вершин на основі ступеня посередництва їх ребер\n\n# кольори ребер на основі ступеня посередництва ребер\nedge_colors = [v for v in btwnCent_edge.values()]\nedge_widths = [v*100 for v in btwnCent_edge.values()]\n\n# будуємо граф\nplt.figure(figsize=(6, 4))\nnx.draw_networkx(G_karate, \n        with_labels=True, \n        pos=nx.spring_layout(G_karate), \n        edge_color=edge_colors, \n        cmap=plt.get_cmap('winter'), \n        width=edge_widths)\n\n\n\n\n\n\n\nРис. 13.31: Граф карате-клубу з виокремлини ребрами на основі їх ступеня посередництва ребер\n\n\n\n\n\nЯкщо розглядати, наприклад, топ 3 ребер із найбільшим ступенем посередництва, ми побачимо, що, як правило, найкраща комунікація проходить у тренера з учнями під номерами 32, 7, 6, 3 тощо.\n\n\n13.1.4.5.5 Ступінь впливовості\nУявіть, що у вас є важливе повідомлення, яке потрібно донести до цілої групи (наприклад, до вашого роботодавця або школи), але ви можете передати його лише одній людині. Кому б ви це сказали? Ви б хотіли знайти когось, хто має хороші зв’язки з усією мережею. Ви можете спробувати звернутися до людини з найвищою ступеневою центральністю (найбільшою кількістю друзів). Недоліком такого підходу є те, що її друзі можуть бути не дуже добре пов’язані з рештою мережі. Наприклад, у гіпотетичній компанії директор з продажу в окремому регіоні може знати найбільше людей, але не знати, як зв’язатися з іншими відділами чи регіонами. Замість нього краще знайти когось, хто має тісні зв’язки з іншими людьми, які мають тісні зв’язки, наприклад, генерального директора (або, що більш ймовірно, його помічника). Таких людей іноді називають хабами, тому що, подібно до центру колеса зі спицями, вони з’єднують між собою багато різних точок. Цю концепцію високозв’язних хабів добре відображає показник, який називається ступенем впливовості.\nСтупінь впливовості вершини \\(i\\) можна визначити через головний власний вектор матриці суміжності \\(\\mathbf{A}\\) мережі:\n\\[\n\\mathbf{Av} = \\lambda \\mathbf{v},\n\\]\nде \\(\\mathbf{v}\\) — власний вектор, що відповідає найбільшому власному значенню \\(\\lambda\\). Ступінь впливовості вершини \\(i\\) задається \\(i\\)-им елементом \\(\\mathbf{v}\\).\nСтупінь впливовості вузла коливається від 0 до 1, причому більше значення вказує на більшу важливість вузла та його сусідів у мережі. Вузли з високим ступенем впливовості часто розташовані в центрі мережі і добре пов’язані з іншими сильно пов’язаними вузлами, і їх видалення може мати значний вплив на зв’язність мережі.\n\neigenvector_centrality = nx.eigenvector_centrality_numpy(G_karate)\nsorted(eigenvector_centrality.items(), key=lambda x:x[1], reverse=True)[:10]\n\n[(34, 0.37336347029148326),\n (1, 0.35549144452456677),\n (3, 0.3171925044864315),\n (33, 0.30864421979104745),\n (2, 0.26595991955249165),\n (9, 0.22740390712540015),\n (14, 0.22647272014248124),\n (4, 0.21117972037789026),\n (32, 0.19103384140654367),\n (31, 0.17475830231435283)]\n\n\n\n# відображаємо мережу з розмірами вершин на основі їх ступеня впливовості\n\n# створити список розмірів вершин на основі ступеня впливовості\nnode_sizes = [10000*v*v for v in eigenvector_centrality.values()]\n\n# кольори на основі ступеневої впливовості\nnode_colors = [v for v in eigenvector_centrality.values()]\n\n# будуємо граф\nplt.figure(figsize=(6, 4))\nnx.draw_networkx(G_karate, \n        with_labels=True, \n        node_size=node_sizes, \n        pos=nx.spring_layout(G_karate), \n        node_color=node_colors, \n        cmap=plt.get_cmap('Purples'))\n\n\n\n\n\n\n\nРис. 13.32: Граф карате-клубу з виокремлини вершини на основі їх ступеня впливовості\n\n\n\n\n\n\n\n\n\n13.1.5 Широкомасштабний опис мереж\nШирокомасштабні структури можуть сильно відрізнятися від мережі до мережі. Ці відмінності часто вказують на різні типи мереж (наприклад, соціальні та технологічні). Широкомасштабні структури також можуть мати важливі наслідки для функціональних властивостей, таких як як стійкість до збоїв і атак. Розглянемо аналіз структурних показників для мереж різних типів.\nЯк ви вже могли переконатися на прикладі графа карате-клубу, NetworkX надає декілька вбудованих наборів мережних даних, які можна використовувати для тестування та експериментів. Ці набори даних доступні в самій бібліотеці NetworkX і можуть бути завантажені за допомогою функцій, які починаються з префікса nx., за яким слідує назва набору даних.\nОсь кілька прикладів вбудованих мережних наборів даних у NetworkX:\n\nnx.karate_club_graph() — повертає мережу Zachary’s Karate Club, соціальну мережу карате-клубу, де кожен вузол представляє члена клубу, а кожне ребро представляє дружні стосунки між членами;\nnx.les_miserables_graph() — повертає мережу персонажів роману Віктора Гюго “Знедолені”, де кожен вузол представляє персонажа роману, а кожне ребро представляє спільну появу двох персонажів у главі;\nnx.davis_southern_women_graph() — повертає мережу соціальних взаємодій між жінками у містечку на півдні США в 1930-х роках, де кожен вузол представляє жінку, а кожне ребро — соціальні стосунки між двома жінками.\n\nЦе лише кілька прикладів вбудованих мережних наборів даних у NetworkX. Ви можете знайти більше інформації про доступні набори даних та їх використання в документації NetworkX.\n\n# генеруємо першу мережу\nG_karate = nx.karate_club_graph()\nmr_hi = 0\njohn_a = 33\n\n# генеруємо другу мережу \nG_novel = nx.les_miserables_graph()\n\n# генеруємо третю мережу \nG_woman = nx.davis_southern_women_graph()\n\nНаступний код візуалізує три приклади мереж:\n\nfig, ax = plt.subplots(1, 3, figsize=(8, 5))\n\nax[0].set_title(\"Карате\")\nnx.draw_networkx(G_karate, node_size=0, with_labels=False, ax=ax[0])\nax[1].set_title(\"Роман\")\nnx.draw_networkx(G_novel, node_size=0, with_labels=False, ax=ax[1])\nax[2].set_title(\"Жінки\")\nnx.draw_networkx(G_woman, node_size=0, with_labels=False, ax=ax[2])\n\nplt.tight_layout()\n\n\n\n\n\n\n\nРис. 13.33: Мережі карате-клубу, персонажів роману та соціальної взаємодії між жінками в містечку на півдні США в 1930-х роках\n\n\n\n\n\n\n13.1.5.1 Діаметр і найкоротший шлях\nМережі можуть бути охарактеризовані відповідно до розподілу довжини найкоротшого шляху. Наведена нижче функція будує гістограму всіх найкоротших шляхів у мережі:\n\ndef path_length_histogram(G, title=None):\n    # знаходимо довжини шляхів\n    length_source_target = dict(nx.shortest_path_length(G))\n    # конвертуємо словник словників до звичайного списку\n    all_shortest = sum([\n    list(length_target.values())\n    for length_target\n    in length_source_target.values()],\n    [])\n    # розраховуємо цілочисельні біни\n    high = max(all_shortest)\n    bins = [-0.5 + i for i in range(high + 2)]\n    # будуємо гістограму\n    plt.hist(all_shortest, bins=bins, rwidth=0.8)\n    plt.title(title)\n    plt.xlabel(\"Відстань\")\n    plt.ylabel(\"Підрахунок\")\n\nТепер давайте порівняємо розподіл довжин шляхів для трьох мереж:\n\n# Створюємо рисунок\nplt.figure(figsize=(8, 5))\n# Будуємо гістограми найкоротших шляхів\nplt.subplot(1, 3, 1)\npath_length_histogram(G_karate, title=\"Карате\")\nplt.subplot(1, 3, 2)\npath_length_histogram(G_novel, title=\"Роман\")\nplt.subplot(1, 3, 3)\npath_length_histogram(G_woman, title=\"Жінки\")\n\nplt.tight_layout()\n\n\n\n\n\n\n\nРис. 13.34: Гістограми найкоротших шляхів у мережах карате-клубу, персонажів роману та соціальної взаємодії між жінками в містечку на півдні США в 1930-х роках\n\n\n\n\n\nУсі три графи мають достатньо малі найкоротші шляхи. Соціальні мережі, як правило, мають короткі шляхи, що відомо як феномен малого світу.\nХоча розподіл повної довжини шляху є інформативним, він є дещо громіздким, тому корисно використовувати агреговані показники. Однією з таких мір є середня довжина найкоротшого шляху, яку можна обчислити наступним чином:\n\nprint(\"Середній найкоротший шлях для карате-клубу: \", nx.average_shortest_path_length(G_karate))\nprint(\"Середній найкоротший шлях для роману: \", nx.average_shortest_path_length(G_novel))\nprint(\"Середній найкоротший шлях для жінок: \", nx.average_shortest_path_length(G_woman))\n\nСередній найкоротший шлях для карате-клубу:  2.408199643493761\nСередній найкоротший шлях для роману:  2.6411483253588517\nСередній найкоротший шлях для жінок:  2.306451612903226\n\n\n\n\n\n\n\n\nПопередження\n\n\n\nУ роз’єднаній на дві або або більше компонентів мережі без ребра між ними середня довжина шляху стає нескінченною. Цю проблему можна вирішити кількома способами, наприклад, використання гармонічного, а не арифметичного середнього, або усереднення середнього значення найкоротших шляхів у межах кожної зв’язної компоненти. Який метод є доречним, залежить від типу мережі, що аналізується\n\n\nКрім того, розмір мережі може бути охарактеризований найбільшою довжиною шляху довжиною, яка називається діаметром. Діаметри трьох прикладів мереж можна знайти за допомогою функції diameter():\n\nprint(\"Діаметр для карате-клубу: \", nx.diameter(G_karate))\nprint(\"Діаметр для роману: \", nx.diameter(G_novel))\nprint(\"Діаметр для жінок: \", nx.diameter(G_woman))\n\nДіаметр для карате-клубу:  5\nДіаметр для роману:  5\nДіаметр для жінок:  4\n\n\nЯк ми можемо бачити результати доволі схожі на попередні. На відміну від середньої довжини найкоротшого шляху, діаметр залежить лише від одного шляху. Як наслідок, один викид може значно збільшити діаметр. Однак у такому разі діаметр може бути гарним показником найгіршої довжини шляху.\n\n\n13.1.5.2 Вимірювання стійкості мережі\nСтійкість — це здатність системи протистояти збоям і атакам. Наприклад, в електромережі стійкість означає продовження подачі електроенергії, коли лінія електропередач або генератор вийшли з ладу. У дорожньому русі це може означати можливість перенаправляти автомобілі, коли вулиця перекрита через аварію.\nСтійкість — це фундаментальна властивість мережі, оскільки вона зазвичай досягається за допомогою резервних шляхів. Коли один шлях більше не доступний, інші все ще можуть бути використані.\nНайпростішим (і найгрубішим) показником стійкості є щільність мережі: частка можливих ребер, які існують. Чим більше ребер у мережі, тим більше надлишкових шляхів існує між її вузлами. Наступний код використовує функцію density() для обчислення цього значення:\n\nprint(\"Щільність для карате-клубу: \", nx.density(G_karate))\nprint(\"Щільність для роману: \", nx.density(G_novel))\nprint(\"Щільність для жінок: \", nx.density(G_woman))\n\nЩільність для карате-клубу:  0.13903743315508021\nЩільність для роману:  0.08680792891319207\nЩільність для жінок:  0.17943548387096775\n\n\nМережа зазвичай вважається розрідженою, якщо кількість ребер близька до \\(N\\) (кількість вузлів), і щільною, якщо кількість ребер близька до \\(N^2\\).\nМожна бачити, що найбільш стійкою (щільною) серед усіх трьох графів є мережа жінок.\n\n\n13.1.5.3 Найменші розрізи\nБільш складні показники відмовостійкості базуються на концепції найменших розрізів. Найменший розріз або min-cut — це кількість вузлів (або ребер), які потрібно видалити, щоб розділити мережу на дві незв’язані частини. Найменші розрізи можна знайти або між двома конкретними вузлами, або над усіма парами вузлів.\nУ NetworkX найменший розріз між двома вузлами знаходять за допомогою функції minimum_st_node_cut(). Зауважте, що ця функція знаходиться у пакеті connectivity і має бути імпортована окремо на додачу до базового пакету networkx. Наступний код знаходить мінімальну довжину шляху між містером Хі та Джоном А. у мережі карате-клубу:\n\nimport networkx.algorithms.connectivity as nxcon\nnxcon.minimum_st_node_cut(G_karate, mr_hi, john_a)\n\n{2, 8, 13, 19, 30, 31}\n\n\nПопередній результат говорить про те, що вузли 2, 8, 12, 19, 30, 31 потрібно видалити, щоб розділити мережу на дві половини, одна з яких містить містера Хі, а інша — Джона А.\nАналогічно, найменший розріз ребер:\n\nnxcon.minimum_st_edge_cut(G_karate, mr_hi, john_a)\n\n{(0, 8),\n (0, 31),\n (1, 30),\n (2, 8),\n (2, 27),\n (2, 28),\n (2, 32),\n (9, 33),\n (13, 33),\n (19, 33)}\n\n\nЯкщо потрібно знати лише розмір найменшого розрізу, можна скористатися функціями node_connectivity() або edge_connectivity() базового пакету networkx. У наступному прикладі обчислюються ці значення для мережі карате-клубу:\n\nnx.node_connectivity(G_karate, mr_hi, john_a)\n\n6\n\n\n\nnx.edge_connectivity(G_karate, mr_hi, john_a)\n\n10\n\n\n\n\n13.1.5.4 Зв’язність\nНайменші розрізи можуть бути використані для визначення показників зв’язності для всієї мережі. Ці міри дуже корисні для кількісної оцінки стійкості мережі.\nЗв’язність вузлів — це найменший мінімальний розріз між усіма парами вузлів. Зв’язність ребер визначається аналогічно. Фактичні значення розрізів між вузлами та ребрами можна знайти за допомогою пакету connection:\n\nnxcon.minimum_node_cut(G_karate)\n\n{0}\n\n\n\nnxcon.minimum_edge_cut(G_karate)\n\n{(11, 0)}\n\n\nЗв’язність можна обчислити за допомогою функцій node_connectivity() та edge_connectivity(), не вказуючи вихідні та цільові вузли. У наступному прикладі обчислюється зв’язність вузлів для трьох прикладів мереж:\n\nnx.node_connectivity(G_karate)\n\n1\n\n\n\nnx.node_connectivity(G_novel)\n\n1\n\n\n\nnx.node_connectivity(G_woman)\n\n2\n\n\nЗдається, що всі ці мережі, окрім мережі жінок, можна роз’єднати, видаливши лише один вузок. Для мережі жінок потребується видалити два вузли.\nПопередня міра зв’язності знаходить розмір найменшого мінімального розрізу, але його видалення не вплине на всі шляхи в мережі. Після видалення вузла або ребра мережа буде розділена, але в кожній половині вузли все ще будуть з’єднані один з одним.\nКращий показник надійності можна знайти, усереднивши зв’язність по всіх вузлах або ребрах за допомогою функцій average_node_connectivity() і average_edge_connectivity(). Зауважте, що обчислення цих значень може зайняти багато часу, навіть для невеликих мереж. Наступний код обчислює середню зв’язність вузлів для досліджуваних мереж:\n\nprint(\"Середня зв'язність для карате-клубу: \", nx.average_node_connectivity(G_karate))\nprint(\"Середня зв'язність для роману: \", nx.average_node_connectivity(G_novel))\nprint(\"Середня зв'язність для жінок: \", nx.average_node_connectivity(G_woman))\n\nСередня зв'язність для карате-клубу:  2.2174688057040997\nСередня зв'язність для роману:  2.2624743677375254\nСередня зв'язність для жінок:  3.7399193548387095\n\n\nМережа каратистів та персонажів роману доволі подібні один до одного по зв’язності, але мережа жінок представляється найбільш стійкою або організованою.\n\n\n13.1.5.5 Централізація та нерівномірність\nМережі також можна класифікувати за ступенем централізації — наскільки вони зосереджені в одному або декількох вузлах. Нерівномірний розподіл є більш централізованим. Наприклад, найбільш централізованою мережею є мережа, всі вузли якої під’єднані до одного вузла-хабу. Наступний код будує гістограми ступенів впливовості для кожної з мереж:\n\n# Функція для побудови гістограми\ndef centrality_histogram(x, title=None):\n    plt.hist(x, density=True)\n    plt.title(title)\n    plt.xlabel(\"Впливовість\")\n    plt.ylabel(\"Підрахунок\")\n\n# Створення рисунку\nplt.figure(figsize=(8, 5))\n# Розрахунок центральностей для кожного графа\nplt.subplot(1, 3, 1)\ncentrality_histogram(\nnx.eigenvector_centrality(G_karate).values(), title=\"Карате\")\nplt.subplot(1, 3, 2)\ncentrality_histogram(\nnx.eigenvector_centrality(G_novel).values(),\ntitle=\"Роман\")\nplt.subplot(1, 3, 3)\ncentrality_histogram(\nnx.eigenvector_centrality(G_woman).values(), title=\"Жінки\")\n\nplt.tight_layout()\n\n\n\n\n\n\n\nРис. 13.35: Гістограми ступенів впливовості в мережах карате-клубу, персонажів роману та соціальної взаємодії між жінками в містечку на півдні США в 1930-х роках\n\n\n\n\n\nЗ представлених гістограм видно, що найвищі значення впливовості приходяться на мережу жінок. Найгіршою за впливовістю є мережа персонажів роману.\nВиміряти нерівномірність набору значень можна за допомогою ентропії Шеннона. Чим більш рівномірно розподіленим є набір чисел, тим вища його ентропія. Наступна функція повертає ентропію списку чисел:\n\nimport math\ndef entropy(x):\n    # Нормалізація\n    total = sum(x)\n    x = [xi / total for xi in x]\n    H = sum([-xi * math.log2(xi) for xi in x])\n    return H\n\nОбчислення ентропії ступенів впливовості у кожній з мереж дає наступний результат:\n\nprint(\"Ентропія ступенів впливовості для карате-клубу: \", \n      entropy(nx.eigenvector_centrality(G_karate).values()))\nprint(\"Ентропія ступенів впливовості для роману: \", \n      entropy(nx.eigenvector_centrality(G_novel).values()))\nprint(\"Ентропія ступенів впливовості для жінок: \", \n      entropy(nx.eigenvector_centrality(G_woman).values()))\n\nЕнтропія ступенів впливовості для карате-клубу:  4.842401948329853\nЕнтропія ступенів впливовості для роману:  5.52075429881287\nЕнтропія ступенів впливовості для жінок:  4.858808158743919\n\n\nНайбільш рівномірно розподіленою в даному випадку представляється мережа персонажів роману. Мережі карате-клубу та жінок мають трохи вищий ступінь централізації.\nУ соціальних мережах не всі стосунки є рівними. У соціології міцність стосунків вимірюється поняттям міцність зв’язності. У цьому контексті зв’язність — це певний вид міжособистісних стосунків, а міцність — це будь-яка міра того, наскільки інтенсивними чи близькими є ці стосунки (зв’язності).\nУ 1973 році соціолог Марк Грановеттер описав важливість слабких зв’язків для зближення різних спільнот. Якщо всі зв’язки всередині спільноти сильні, то будь-які зв’язки між спільнотами мають бути слабкими. Він назвав це явище силою слабких зв’язків. З’єднуючи різні спільноти, слабкі зв’язки дають змогу знаходити інформацію з віддалених частин мережі. Але як виміряти силу зв’язностей?\n\n\n13.1.5.6 Сила зв’язності\nУ мережі карате-клубу немає ніякої додаткової інформації про міцність ребер, але є відповідні властивості цих ребер, які можна обчислити, наприклад, сила зв’язності. Сила зв’язності зростає зі збільшенням кількості сусідів, які мають спільні вершини. Це мотивовано спостереженням, що близькі друзі, як правило, мають більше спільних друзів, і це часто може дати уявлення про структуру соціальної мережі. Наступний код обчислює силу зв’язку, використовуючи метод neighbors() для пошуку сусідів вузлів:\n\ndef tie_strength(G, v, w):\n    # Отримуємо сусідів вершин v та w у G\n    v_neighbors = set(G.neighbors(v))\n    w_neighbors = set(G.neighbors(w))\n    # Повернути розмір заданої зв'язності\n    return 1 + len(v_neighbors & w_neighbors)\n\nТут ми визначили міцність зв’язку як кількість спільних сусідів плюс один. Чому плюс один? Нульова вага умовно означає відсутність ребра, тому без додаткової одиниці ребра між вершинами, які не мають спільних сусідів, не вважатимуться ребрами.\n\nG = nx.karate_club_graph()\n\n# Надаємо інформацію про те, хто в якому клубі \n# опинився після розділення клубу\nmember_club = [\n    0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n    0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n    1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n    1, 1, 1, 1]\n\nnx.set_node_attributes(G, dict(enumerate(member_club)), 'club')\n\n# Знаходимо внутрішні та зовнішні ребра\nfor v, w in G.edges:\n # Перебираємо пари вершин\n # Встановлюємо 'True', якщо вершини в одному кластері (клубі)\n if G.nodes[v][\"club\"] == G.nodes[w][\"club\"]:\n    G.edges[v, w][\"internal\"] = True\n else:\n    G.edges[v, w][\"internal\"] = False\n\n# Внутріші - каратисти знаходяться в одному клубі й підтримують зв'язок\ninternal = [e for e in G.edges if G.edges[e][\"internal\"]]\n\n# Зовнішні - каратисти в різних клубах, але продовжують підтримувати зв'язок\nexternal = [e for e in G.edges if ~G.edges[e][\"internal\"]]\n\nНаступний код обчислює силу зв’язності кожного ребра і зберігає її в змінну strength:\n\nstrength = dict(\n ((v,w), tie_strength(G, v, w))\n for v, w in G.edges())\n\n\n\n13.1.5.7 Мостовий проліт\nМіцність зв’язків також можна оцінити кількісно, розглядаючи ефект видалення ребра з мережі. Вузли, з’єднані ребром, завжди знаходяться на відстані в 1 крок один від одного (у незваженій мережі). Але, якщо це ребро видалити, його кінцеві точки можуть знаходитись на відстані в 2 кроки, і навіть до зовсім не з’єднаних між собою. Цю концепцію відображає мостовий проліт — відстань між кінцевими точками ребра, якщо це ребро видалити. Ребра з великим прольотом з’єднують віддалені частини мережі, тому їх можна вважати слабкими зв’язками, незважаючи на те, що вони відіграють важливу роль.\nНаступний код обчислює довжину кожного ребра в мережі карате-клубу:\n\ndef bridge_span(G):\n    # Отримуємо список ребер\n    edges = G.edges()\n    # Створюємо копію графа\n    G = nx.Graph(G)\n    # Створюємо словник для збереження результату\n    result = dict()\n    for v, w in edges:\n        # Тимчасово видаляємо ребро \n        G.remove_edge(v, w)\n        # Знаходимо нову відстань між двома вузлами \n        # після видалення ребра\n        try:\n            d = nx.shortest_path_length(G, v, w)\n            result[(v, w)] = d\n        except nx.NetworkXNoPath:\n            result[(v, w)] = float('inf')\n        # Відновлюємо ребро\n        G.add_edge(v, w)\n    return result\n\n\nspan = bridge_span(G)\n\n\n\n13.1.5.8 Порівняння міцності та прольоту\nРозглянемо 10 найміцніших і 10 найслабших ребер у мережі карате-клубу. Наступний код виводить ці ребра:\n\n# Упорядковуємо ребра за силою зв'язності\nordered_edges = sorted(strength.items(), key=lambda x: x[1])\nprint('Ребро\\t Міцність\\t Проліт\\t Внутрішній зв\\'язок')\n# Виводимо 10 найміцніших\nfor e, edge_strength in ordered_edges[:10]:\n    print('{:10}{}\\t\\t{}\\t{}'.format(\n    str(e), edge_strength, span[e], G.edges[e]['internal']))\nprint('...')\n# Виводимо 10 найслабших\nfor e, edge_strength in ordered_edges[-10:]:\n    print('{:10}{}\\t\\t{}\\t{}'.format(\n    str(e), edge_strength, span[e], G.edges[e]['internal']))\n\nРебро    Міцність    Проліт  Внутрішній зв'язок\n(0, 11)   1     inf True\n(0, 31)   1     3   False\n(1, 30)   1     3   False\n(2, 9)    1     3   False\n(2, 27)   1     3   False\n(2, 28)   1     3   False\n(9, 33)   1     3   True\n(13, 33)  1     3   False\n(19, 33)  1     3   False\n(23, 25)  1     3   True\n...\n(8, 32)   4     2   True\n(23, 33)  4     2   True\n(29, 33)  4     2   True\n(1, 2)    5     2   True\n(1, 3)    5     2   True\n(2, 3)    5     2   True\n(0, 2)    6     2   True\n(0, 3)    6     2   True\n(0, 1)    8     2   True\n(32, 33)  11        2   True\n\n\nРезультат показує, що ребра з низькою міцністю і великим прольотом, як правило, є зовнішніми, з’єднуючи членів клубу, які розкололися на різні клуби-відколи. З іншого боку, ребра з високою міцністю і малим прольотом є внутрішніми, вони з’єднують членів клубу, які залишилися разом після розколу.\n\n\n13.1.5.9 Спектральні міри складності\nСпектром графа \\(G\\) називається множина власних значень матриці, що відповідає даному графу. Відомі декілька підходів встановлення зв’язку між графом \\(G\\) та його спектром. Для випадку регулярних графів (якими є графи часових рядів фондових індексів) можна показати, що різні види спектрів еквівалентні, тобто містять однакову кількість інформації щодо структури графа \\(G\\).\nМи вже згадували, що одним із способів представлення графа у вигляді матриці є матриця суміжності. Матриця Лапласа (Laplacian matrix) \\(L\\) — також є одним видів подання графа. Вона може бути використана для розрахунку кількості остовних дерев для графа. Для знаходження матриці Лапласа використовують формулу \\(L=D-A\\), де \\(D\\) — діагональна матриця:\n\\[\nd_{ij} = \\begin{cases}\n            d_i, & i=j,\\\\\n            0, & i \\neq j,\n        \\end{cases}\n\\]\nде \\(d_i\\) — ступінь відповідної вершини графа. Отже,\n\\[\nl_{ij} = \\begin{cases}\n    d_i, & i=j, \\\\\n    -1, & i \\neq j \\, \\text{і} \\, v_i \\, \\text{суміжна з} \\, v_j, \\\\\n    0 & \\text{в іншому випадку}.\n\\end{cases}\n\\]\nАлгебраїчна зв’язність графа (Algebraic connectivity) — друге найменше власне значення матриці Лапласа. Це власне значення більше нуля тоді і тільки тоді, коли граф зв’язний. Величина цього значення відображає, наскільки зв’язним є даний граф, і використовується при аналізі надійності та синхронізації мереж. Бібліотека NetworkX містить метод algebraic_connectivity() для обчислення даного показника. Бібліотека також надає змогу розрахувати нормалізовану матрицю Лапласа. Сенс нормалізації полягає в тому, що вершина з великим ступенем вершини, яку також називають важкою вершиною, призводить до того, що в матриці Лапласа з’являється великий діагональний елемент, який домінує у властивостях матриці. Нормалізація спрямована на те, щоб зробити вплив таких вершин більш рівним впливу інших вершин шляхом ділення елементів матриці Лапласа на ступені вершин. Щоб уникнути ділення на нуль, ізольовані вершини з нульовими ступенями виключаються з процесу нормалізації.\n\nprint(\"Алгебраїчна зв'язність для карате-клубу: \", nx.algebraic_connectivity(G_karate, normalized=True, method='tracemin_lu'))\nprint(\"Алгебраїчна зв'язність для роману: \", nx.algebraic_connectivity(G_novel, normalized=True, method='tracemin_lu'))\nprint(\"Алгебраїчна зв'язність для жінок: \", nx.algebraic_connectivity(G_woman, normalized=True, method='tracemin_lu'))\n\nАлгебраїчна зв'язність для карате-клубу:  0.1100741920065786\nАлгебраїчна зв'язність для роману:  0.06737737553000267\nАлгебраїчна зв'язність для жінок:  0.20797214796917615\n\n\nМожемо бачити, що найбільш зв’язним у даному випадку представляється саме граф жінок. Тобто спілкування та кооперація між ними залишається найбільш тісною.\nЕнергія графа (graph energy) — це сума абсолютних значень власних значень матриці суміжності графа. Нехай \\(G\\) є граф з \\(n\\) вершинами. Передбачається, що \\(G\\) — простий, тобто він не містить петлі чи паралельних ребер. Нехай \\(A\\) — матриця суміжності графа \\(G\\) і \\(\\lambda_i\\), \\(i=1,...,n\\) — власні значення матриці \\(A\\). Тоді енергія графа визначається як:\n\\[\nE(G) = \\sum_{i=1}^{n}\\left| \\lambda_i \\right|.\n\\]\nВбудованого методу в NetworkX для визначення енергії графа немає, але ми доволі запросто можемо розрахувати спектр власних значень матриці суміжності, а потім скористатися формулою вище. Власні значення матриці \\(A\\) можна знайти за допомогою методу adjacency_spectrum(). Далі визначимо наступну функцію для розрахунку енергії графа:\n\ndef graph_energy(G): \n\n    adj_spectrum = nx.adjacency_spectrum(G) # спектр власних значень матриці суміжності\n    graph_en = np.sum(np.abs(adj_spectrum))\n\n    return graph_en\n\nТепер розрахуємо енергію для кожного досліджуваного графа:\n\nprint(\"Енергія графа карате-клубу: \", graph_energy(G_karate))\nprint(\"Енергія графа роману: \", graph_energy(G_novel))\nprint(\"Енергія графа жінок: \", graph_energy(G_woman))\n\nЕнергія графа карате-клубу:  153.22817810462595\nЕнергія графа роману:  460.4651813130984\nЕнергія графа жінок:  51.820121985616545\n\n\nНайвище значення енергії графа вказує на найвищу складність мережі або на найвищий ступінь централізованості деяких вузлів. Для наших графів видно, що найвища енергія приходить саме граф персонажів роману. Тобто, тут є декілька персонажів, на які приходить найбільша кількість зв’язків (діалогів) у порівнянні з іншими персонажами.\nСпектральний розрив (spectral gap) — різниця між найбільшим і другим за величиною власними значеннями, надає інформацію про те, як швидко досягається синхронний стан. Можемо визначити й прорахувати наступну функцію:\n\ndef spectral_gap(G):\n    adj_spectrum = nx.adjacency_spectrum(G)\n    sorted_adj_spectrum = np.sort(adj_spectrum.real)\n    spec_gap = sorted_adj_spectrum[-1] - sorted_adj_spectrum[-2]\n\n    return spec_gap\n\nprint(\"Спектральний розрив для карате-клубу: \", spectral_gap(G_karate))\nprint(\"Спектральний розрив для роману: \", spectral_gap(G_novel))\nprint(\"Спектральний розрив для жінок: \", spectral_gap(G_woman))\n\nСпектральний розрив для карате-клубу:  4.5812458234061815\nСпектральний розрив для роману:  16.25810678675367\nСпектральний розрив для жінок:  2.3618098280049002\n\n\nСпектральний радіус є найбільшим за модулем власним значенням:\n\\[\nr(A) = \\max_{\\lambda \\in Spec(A)} \\left| \\lambda \\right|,\n\\]\nде \\(Spec(A)\\) — спектр власних значень матриці суміжності. Для розрахунків визначимо наступну функцію:\n\ndef spectral_radius(G): \n    adj_spectrum = nx.adjacency_spectrum(G).real\n    spec_rad = np.max(np.abs(adj_spectrum))\n\n    return spec_rad\n\nprint(\"Спектральний радіус для карате-клубу: \", spectral_radius(G_karate))\nprint(\"Спектральний радіус для роману: \", spectral_radius(G_novel))\nprint(\"Спектральний радіус для жінок: \", spectral_radius(G_woman))\n\nСпектральний радіус для карате-клубу:  21.68756590395423\nСпектральний радіус для роману:  65.02628035526055\nСпектральний радіус для жінок:  6.741908124910328\n\n\nСпектральний момент. Для визначення \\(k\\)-ого спектрального моменту використовують матрицю суміжності. Визначимо її наступним чином:\n\\[\nm_k(A)=\\frac{1}{n}\\sum_{i=1}^{n}\\lambda_{i}^{k},\n\\]\nде \\(\\lambda_i\\) — власні значення матриці суміжності \\(A\\), \\(n\\) — кількість вершин графа \\(G\\). Значення \\(k\\) у нашому випадку випадку буде дорівнювати 3. Тобто, будемо обчислювати спектральний момент 3-го порядку. Визначимо наступну функцію для розрахунку даного показника:\n\ndef spectral_moment(G):\n\n    adj_spectrum = nx.adjacency_spectrum(G).real\n    spec_mom_3 = np.mean(adj_spectrum ** 3)\n\n    return spec_mom_3\n\nprint(\"Спектральний момент для карате-клубу: \", spectral_moment(G_karate))\nprint(\"Спектральний момент для роману: \", spectral_moment(G_novel))\nprint(\"Спектральний момент для жінок: \", spectral_moment(G_woman))\n\nСпектральний момент для карате-клубу:  321.3529411764726\nСпектральний момент для роману:  4325.688311688315\nСпектральний момент для жінок:  9.703349235223868e-14\n\n\nОстанні показники говорять по те, що персонажі роману характеризуються найвищим ступенем складності в порівнянні з іншими графами. Ми показали, що достатня кількість вузлів має досить невисокий найкоротший шлях, але може мати гіршу щільність зв’язності вузлів або рівнорозподіленності ступеня впливовості.\n\n\n13.1.5.10 Проблема малого світу\nУ 1967 році соціальні психологи Джеффрі Треверс і Стенлі Мілґрем надіслали листи групам людей у Вічіті, штат Канзас, та Омасі, штат Небраска. Вони також обрали одну цільову особу в штаті Массачусетс. Кожному отримувачу листа було доручено переслати його знайомому, який, найімовірніше, знав цільову людину. Багато листів дійшли до адресата, і дослідники змогли з’ясувати, скільки кроків було зроблено для цього. Середня кількість кроків становила шість, звідси і поширена фраза “шість ступенів відокремлення”.\n\n\n13.1.5.11 Кільцеві мережі\nЯк правило, більшість знайомих людини — це люди, які живуть у тій самій місцевості. Якби кожна людина була знайома лише з тими, хто живе поруч, то можна було б очікувати, що для того, щоб надіслати повідомлення з Канзасу до Массачусетсу, знадобилося б більше шести кроків, оскільки кожен крок міг би подолати лише невелику відстань. Таку мережу можна змоделювати як кільце: вузли, розташовані по колу, причому кожен вузол з’єднаний з найближчими \\(k/2\\) вузлами з кожного боку. Наступний приклад створює та візуалізує чотирикільце за допомогою функції watts_strogatz_graph() про яку ми ще поговоримо.\n\nG_small_ring = nx.watts_strogatz_graph(16, 4, 0)\npos = nx.circular_layout(G_small_ring)\n\nplt.figure(figsize=(6, 4))\nnx.draw_networkx(G_small_ring, pos=pos, with_labels=False)\n\n\n\n\n\n\n\nРис. 13.36: Кільцеве представлення графа Воттса-Строгаца\n\n\n\n\n\nЩоб з’єднати два вузли в попередньому прикладі, потрібно пройти по краю кола, пропускаючи щонайбільше кожен другий вузол. Навіть у цій дуже маленькій мережі типова мережна відстань є досить великою.\nНаступний код знаходить середній найкоротший шлях і середню кластеризацію в більш реалістичному 10-ти кільцевому графі з 4000 вузлів:\n\nG_ring = nx.watts_strogatz_graph(4000, 10, 0)\nnx.average_shortest_path_length(G_ring)\n\n200.45011252813202\n\n\n\nnx.average_clustering(G_ring)\n\n0.6666666666666546\n\n\nЦя мережа має в середньому 200 кроків розділення, що набагато більше, ніж шість! Вона також має досить великий середній коефіцієнт кластеризації 0.67, що показує, що сусіди вузла мають тенденцію бути пов’язаними один з одним.\n\n\n13.1.5.12 Випадкові мережі\nЩоб дослідити цю таємницю, розглянемо інший тип мережі. У цій мережі ми починаємо з \\(k\\)-кільця, але випадковим чином переставляємо кінцеві точки кожного ребра. В результаті отримаємо мережу з тією ж кількістю вузлів і ребер, але з випадковою структурою, що демонструється наступним кодом:\n\nG_small_random = nx.watts_strogatz_graph(16, 4, 1)\npos = nx.circular_layout(G_small_random)\n\nplt.figure(figsize=(6, 4))\nnx.draw_networkx(G_small_random, pos=pos, with_labels=False)\n\n\n\n\n\n\n\nРис. 13.37: Кільцеве представлення графа Воттса-Строгаца з випадковим перев’язуванням ребер\n\n\n\n\n\nТепер давайте розглянемо властивості перев’язаного 10-ти кільцевого графа з 4000 вузлів:\n\nG_random = nx.watts_strogatz_graph(4000, 10, 1)\nnx.average_shortest_path_length(G_random)\n\n3.86820767691923\n\n\n\nnx.average_clustering(G_random)\n\n0.0022108393949531726\n\n\nСередній найкоротший шлях дуже близький до реальної соціальної мережі, але середня кластеризація тепер майже 0. Поки що моделі, які ми бачили, досягають коротких шляхів або високої кластеризації, але не того й іншого разом.\n\n\n13.1.5.13 Мережа Воттса-Строгаца\nПроблема малого світу полягає в тому, як люди, що живуть на великій відстані один від одного, можуть бути пов’язані короткими шляхами, навіть якщо їхні зв’язки є локальними. Дункан Воттс і Стівен Строгац розробили клас мереж для пояснення такої поведінки. Мережі починаються як \\(k\\)-кільця: вузли, розміщені по колу, кожен з яких з’єднаний з найближчими \\(k\\) сусідами. Потім, з ймовірністю \\(p\\), ребра кожного вузла перев’язуються з іншим випадково обраним вузлом. Ці перестановки створюють короткі шляхи по всій мережі. Навіть невелика кількість коротких шляхів значно скорочує відстані між вузлами мережі, вирішуючи проблему малого світу. Фактично, це саме те, що робить функція watts_strogatz_graph(), яку ми використовували, а третій параметр задає частку ребер, які потрібно перезв’язати. Наступний код обчислює середній найкоротший шлях і середню кластеризацію для діапазону ймовірностей перезв’язування:\n\npath = []\nclustering = []\n# Пробуємо список імовірностей перезв'язування\np = [10**(x) for x in range(-6, 1)]\nfor p_i in p:\n    path_i = []\n    clustering_i =[]\n    # Створюємо 10 моделей для кожної ймовірності\n    for n in range(10):\n        G = nx.watts_strogatz_graph(1000, 10, p_i)\n        path_i.append(nx.average_shortest_path_length(G))\n        clustering_i.append(nx.average_clustering(G))\n    # Усереднюємо показники для кожного значення p_i\n    path.append(sum(path_i) / len(path_i))\n    clustering.append(sum(clustering_i) / len(clustering_i))\n\nРезультати наступного коду зберігаються у списках path та clustering. Використовуючи функцію semilogx() з matplotlib.pyplot, наступний код візуалізує, як ці значення змінюються при зміні ймовірності перев’язування від 0 до 1:\n\nplt.figure(figsize=(6, 4))\n\nplt.semilogx(p, [x / path[0] for x in path], label=r'$L_{mean} / L_0$')\nplt.semilogx(p, [x / clustering[0] for x in clustering], label=r'$C_{mean} / C_0$')\nplt.tick_params(axis='both', which='major', labelsize=16)\nplt.xlabel(r'Імовірність перезв\\'язування $p$', fontsize=16)\nplt.legend(fontsize=16); \n\n\n\n\n\n\n\nРис. 13.38: Зміна відносного середнього найкоротшого шляху та коефіцієнту кластеризації від імовірності перев’язування графа\n\n\n\n\n\nЯк ми вже бачили, зі збільшенням кількості перезв’язувань, як середня кластеризація, так і середній найкоротший шлях зменшуються. Однак цікава річ відбувається при проміжних значеннях. Довжина шляху стає коротшою при дуже низьких значеннях перезв’язування, в той час як зменшення кластеризації відбувається лише при більших значеннях перезв’язування. Іншими словами, перезв’язування дуже малої частки ребер створює “мости”, які з’єднують віддалені частини мережі і різко скорочують середній найкоротший шлях, не змінюючи при цьому кластеризацію. Можна сказати, що найкращий тип мереж це той, що зберігає як частку впорядкованості, так і частку випадковості.\nДалі можемо подивитись, як виглядає мережа Воттса й Строгаца при наступних імовірностях: \\(p=0\\), \\(p=0.1\\) та \\(p=1\\).\n\nplt.figure(figsize=(8, 4))\nfor i, p in enumerate([0.0, 0.1, 1.0]):\n    # Генеруємо граф\n    G = nx.watts_strogatz_graph(12, 6, p)\n    # Будуємо рисунок\n    plt.subplot(1, 3, i + 1)\n    pos = nx.circular_layout(G)\n    nx.draw_networkx(G, pos=pos)\n    plt.title(\"p = {:0.1f}\".format(p))\n    \n\n\n\n\n\n\n\nРис. 13.39: Представлення графа Воттса й Строгаца при різних імовірностях перев’язування ребер\n\n\n\n\n\nУ деяких випадках перезв’язування може призвести до того, що дві компоненти в мережі Воттса-Строгаца будуть роз’єднані. Роз’єднана мережа може бути непотрібним ускладненням. Мережа Ньюмана-Воттса-Строгаца — це варіант, який гарантує, що отримана мережа буде зв’язною. Вона схожа на оригінальну версію, але залишає копію оригінального ребра на місці кожного ребра, що перезв’язується. Такі мережі можна створювати за допомогою функції newman_watts_strogatz_graph(), як показано нижче:\n\nplt.figure(figsize=(8, 4))\nfor i, p in enumerate([0.0, 0.1, 1.0]):\n    \n    G = nx.newman_watts_strogatz_graph(12, 6, p)\n\n    plt.subplot(1, 3, i + 1)\n    pos = nx.circular_layout(G)\n    nx.draw_networkx(G, pos=pos)\n    plt.title(\"p = {:0.1f}\".format(p))\n\n\n\n\n\n\n\nРис. 13.40: Представлення графа Ньюмана-Воттса-Строгаца при різних імовірностях перезв’язування ребер\n\n\n\n\n\n\n\n13.1.5.14 Степеневі закони та переважне приєднання\nВід інтернету до поїздок в аеропорт, багато мереж характеризуються кількома вузлами з великою кількістю зв’язків і багатьма вузлами з дуже малою кількістю зв’язків. Такі мережі характеризуються важкими хвостами, тому що при побудові гістограми ступенів вузлів, вузли з високим рівнем зв’язності утворюють хвіст.\nІснує багато способів генерування мереж з важким хвостом, але одним з найпоширеніших є модель переважного приєднання Барабаші-Альберт. Модель переважного приєднання імітує процеси, в яких багаті стають багатшими. Кожного разу, коли додається новий вузол, він випадковим чином з’єднується з існуючими вузлами, причому більш вірогідним є з’єднання з вузлами високого ступеня.\nУ NetworkX функція barabasi_albert_graph(), яка генерує мережі переважного приєднання. У наступному коді показано приклад такої мережі з 35 вузлами:\n\nG_preferential_35 = nx.barabasi_albert_graph(35, 1)\npos = nx.spring_layout(G_preferential_35, k=0.1)\nplt.figure(figsize=(6, 4))\nnx.draw_networkx(G_preferential_35, pos)\n\n\n\n\n\n\n\nРис. 13.41: Представлення графа переважного приєднання Барабаші-Альберт при 35 вузлах\n\n\n\n\n\nСтруктура мережі переважного приєднання ще більш очевидна при більшій кількості вузлів. У наступному прикладі використовується 1000 вузлів:\n\nG_preferential_1000 = nx.barabasi_albert_graph(1000, 1)\npos = nx.spring_layout(G_preferential_1000)\nplt.figure(figsize=(6, 4))\nnx.draw_networkx(G_preferential_1000, pos, node_size=0, with_labels=False)\n\n\n\n\n\n\n\nРис. 13.42: Представлення графа переважного приєднання Барабаші-Альберт при 1000 вузлах\n\n\n\n\n\nВажкі хвости цих мереж можна побачити, побудувавши їхні ступеневі розподіли. Наступна функція будує розподіл ступенів мережі:\n\ndef plot_degree_hist(G, title):\n    \"\"\"Функція для побудови розподілу ступенів вершин мережі\"\"\"\n    plt.hist(dict(nx.degree(G)).values(), bins=range(1, 11))\n    plt.xlabel('Ступінь')\n    plt.ylabel('Щільність')\n    plt.title(title)\n\nВикористовуючи цю функцію, наступний код візуалізує розподіл ступенів для 35-вузлової та 1000-вузлових мереж переважного приєднання:\n\nplt.figure(figsize=(8, 5))\nax = plt.subplot(1, 2, 1)\nplot_degree_hist(G_preferential_35, '35 вузлів')\nfor spine in ax.spines.values():\n    spine.set_visible(True)\nax = plt.subplot(1, 2, 2)\nfor spine in ax.spines.values():\n    spine.set_visible(True)\nplot_degree_hist(G_preferential_1000, '1000 вузлів')\nplt.tight_layout()\n\n\n\n\n\n\n\nРис. 13.43: Гістограми ступенів вершини графів переважного приєднання при 35 та 1000 вершинах\n\n\n\n\n\nМережі з переважним приєднанням мають одну цікаву властивість: вони масштабно-інваріантні. Розподіл ступенів у масштабоінваріантних мережах підпорядковується степеневому закону, що призводить до схожої структури на різних масштабах. Один із способів побачити це — порівняти попередні гістограми. Незважаючи на дуже різні масштаби, вони мають схожу форму. Розподіл ступенів вершин можна описати степеневою функцією виду:\n\\[\nP(k) \\propto k^{-\\gamma},\n\\]\nде \\(k\\) — ступінь вузла, \\(P(k)\\) — ймовірність того, що вузол має ступінь \\(k\\), і \\(\\gamma\\) — показник степеневого закону. Показник \\(\\gamma\\) зазвичай знаходиться в діапазоні від 2 до 3 для більшості реальних мереж.\nРозподіл ступенів степеневого закону має важливі наслідки для структури та функцій мереж. Наприклад, мережі зі степеневим розподілом часто є більш надійними і стійкими до випадкових збоїв, але більш вразливими до цілеспрямованих атак на вузли з високим ступенем.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Лабораторна робота № 13</span>"
    ]
  },
  {
    "objectID": "lab_13.html#хід-роботи",
    "href": "lab_13.html#хід-роботи",
    "title": "13  Лабораторна робота № 13",
    "section": "13.2 Хід роботи",
    "text": "13.2 Хід роботи\nТепер давайте проведемо порівняльний аналіз графів різної складності з використанням деяких із зазначених показників. За допомогою бібліотеки NetworkX розглянемо наступні типи графів:\n\nлінійний граф — path_graph();\nциклічний граф — cycle_graph();\nграф-зірка — star_graph();\nграф Ердеша-Реньї — erdos_renyi_graph();\nграф малого світу — watts_strogatz_graph();\nграф переважного приєднання — barabasi_albert_graph().\n\nВізуалізуємо кожен із зазначених графів:\n\nfig, axes = plt.subplots(3, 2, figsize=(10, 8))\n\n# лінія \naxes[0, 0].set_title('Лінія')\nline_graph = nx.path_graph(100)\npos_line_graph = nx.spring_layout(line_graph, k=0.15, iterations=100)\nnx.draw_networkx(line_graph, pos=pos_line_graph, node_size=10, with_labels=False, ax=axes[0, 0])\n\n# коло \naxes[0, 1].set_title('Коло')\ncycle_graph = nx.cycle_graph(100)\npos_cycle_graph = nx.circular_layout(cycle_graph)\nnx.draw_networkx(cycle_graph, pos=pos_cycle_graph, node_size=10, with_labels=False, ax=axes[0, 1])\n\n# зірка\naxes[1, 0].set_title('Зірка')\nstar_graph = nx.star_graph(100)\npos_star_graph = nx.spring_layout(star_graph, k=0.15, iterations=100)\nnx.draw_networkx(star_graph, pos=pos_star_graph, node_size=10, with_labels=False, ax=axes[1, 0])\n\n# Ердеша-Реньї\naxes[1, 1].set_title('Ердеш-Реньї')\nerdos_renyi_graph = nx.erdos_renyi_graph(100, 0.01)\npos_erdos_renyi_graph = nx.circular_layout(erdos_renyi_graph)\nnx.draw_networkx(erdos_renyi_graph, pos=pos_erdos_renyi_graph, node_size=10, with_labels=False, ax=axes[1, 1])\n\n# Малий світ \naxes[2, 0].set_title('Малий світ')\nsmall_world_graph = nx.watts_strogatz_graph(100, 30, 0.01, seed=32)\npos_small_world_graph = nx.spring_layout(small_world_graph, k=0.15, iterations=100)\nnx.draw_networkx(small_world_graph, pos=pos_small_world_graph, node_size=10, with_labels=False, ax=axes[2, 0])\n\n# Переважне приєднання \naxes[2, 1].set_title('Переважне приєднання')\nbarabasi_albert_graph = nx.barabasi_albert_graph(100, 30, seed=32)\npos_barabasi_albert_graph = nx.spring_layout(barabasi_albert_graph, k=0.15, iterations=100)\nnx.draw_networkx(barabasi_albert_graph, pos=pos_barabasi_albert_graph, node_size=10, with_labels=False, ax=axes[2, 1])\n\n\n\n\n\n\n\nРис. 13.44: Візуалізація графів: лінія, коло, зірка, Ердеш-Реньї, Малий світ і Переважне приєднання\n\n\n\n\n\nКожен із даних графів може різнитись за своїми спектральними і топологічними властивостями: деякі можуть мати вищий ступінь кластеризації, ступеня вершини, посередництва тощо. Розглянемо як ранжується ступінь складності кожного графа за досліджуваними нами показниками.\nСпочатку збережемо кожен із побудованих графів до одного масиву для ітеративного проведення розрахунків по кожному з них:\n\ngraphs = [line_graph, cycle_graph, star_graph, erdos_renyi_graph, small_world_graph, barabasi_albert_graph] # графи\nlabels = ['Лінія', 'Коло', 'Зірка', 'Ердеш-Реньї', 'Малий світ', 'Переважне приєднання']                    # їх мітки\ncolors = ['b', 'purple', 'red', 'green', 'pink', 'black']\nlinestyles = ['-', '-', '--', '--', ':', '-']\nmarkers = ['d', 'v', '*', 's', 'H', 'o']\n\n\n13.2.1 Спектральні міри складності\nТепер виконаємо розрахунки спектральних мір складності для кожного графа:\n\nalgebraic_connect_vals = np.zeros(6) \nenergy_vals = np.zeros(6)\nspec_gap_vals = np.zeros(6)\nspec_mom_vals = np.zeros(6)\n\nfor i, graph in enumerate(graphs):\n    algebraic_connect_vals[i] = nx.algebraic_connectivity(graph, normalized=False, method='tracemin_lu')\n    energy_vals[i] = graph_energy(graph)\n    spec_gap_vals[i] = spectral_gap(graph)\n    spec_mom_vals[i] = spectral_moment(graph)\n\nnum_rep = 30\nalgebraic_connect_vals = np.repeat(algebraic_connect_vals[:, np.newaxis], num_rep, axis=1)\nenergy_vals = np.repeat(energy_vals[:, np.newaxis], num_rep, axis=1)\nspec_gap_vals = np.repeat(spec_gap_vals[:, np.newaxis], num_rep, axis=1)\nspec_mom_vals = np.repeat(spec_mom_vals[:, np.newaxis], num_rep, axis=1)\n\nВиведемо результат:\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\n# Зв'язність\naxes[0, 0].set_title('Алгебраїчна зв\\'язність')\nfor i in range(len(graphs)):\n    axes[0, 0].plot(algebraic_connect_vals[i], \n                    color=colors[i], \n                    marker=markers[i], \n                    linestyle=linestyles[i], \n                    label=labels[i])\naxes[0, 0].legend(fontsize=10)\n\n\n# Енергія \naxes[0, 1].set_title('Енергія графа')\nfor i in range(len(graphs)):\n    axes[0, 1].plot(energy_vals[i], \n                    color=colors[i], \n                    marker=markers[i], \n                    linestyle=linestyles[i], \n                    label=labels[i])\naxes[0, 1].legend(fontsize=10)\n\n# Розрив\naxes[1, 0].set_title('Спектральний розрив')\nfor i in range(len(graphs)):\n    axes[1, 0].plot(spec_gap_vals[i], \n                    color=colors[i], \n                    marker=markers[i], \n                    linestyle=linestyles[i], \n                    label=labels[i])\naxes[1, 0].legend(fontsize=10)\n\n# Момент \naxes[1, 1].set_title('Спектральний момент')\nfor i in range(len(graphs)):\n    axes[1, 1].plot(spec_mom_vals[i], \n                    color=colors[i], \n                    marker=markers[i], \n                    linestyle=linestyles[i], \n                    label=labels[i])\naxes[1, 1].legend(fontsize=10)\n\nplt.show(); \n\n\n\n\n\n\n\nРис. 13.45: Спектральні властивості канонічних і модельних графів: алгебраїчної зв’язності, енергії графа, спектрального розриву та спектрального моменту\n\n\n\n\n\nЗ рисунку (Рис. 13.45) можна побачити наступне:\n\nпо-перше, усі спектральні показники залишаються найбільшими саме для графа переважного приєднання, що представляється найбільш складним серед усіх інших графів;\nпо друге, згідно динаміки спектральних показників, найпростішими серед усіх графів є граф лінії, зірки та Ердеша-Реньї. Для лінії зберігається зв’язок тільки між парами послідовних вершин. Для зірки зберігається зв’язок усіх вершин із центром, але самі вони не пов’язані один із одним;\nпо третє, граф малого світу залишається другим за складністю майже за всіма показниками, окрім спектрального розриву. Спектральний розрив говорить, що граф зірки є трохи складнішим за граф малого світу. Це може бути обумовлене тим, що для зірки ми спостерігаємо достатньо високий ступінь централізації.\n\n\n\n13.2.2 Топологічні міри\nРозрахуємо для досліджуваних графів топологічні міри складності. В якості прикладу розглянемо такі міри як\n\nмаксимальний ступінь вершини (\\(d_{max}\\));\nглобальний коефіцієнт кластеризації (\\(C\\));\nсередній ступінь посередництва (\\(B_{mean}\\));\nсередня довжина найкоротшого шляху (\\(L_{mean}\\)).\n\n\nmax_degree_vals = np.zeros(6) \nglobal_clust_vals = np.zeros(6)\nmean_betweenness_vals = np.zeros(6)\nmean_path_vals = np.zeros(6)\n\nfor i, graph in enumerate(graphs):\n    max_degree_vals[i] = max(dict(graph.degree()).values())\n    global_clust_vals[i] = nx.average_clustering(graph)\n    mean_betweenness_vals[i] = np.mean(list(nx.betweenness_centrality(graph).values()))\n    mean_path_vals[i] = np.mean([nx.average_shortest_path_length(C) for C in \n                                 (graph.subgraph(c).copy() for c in nx.connected_components(graph))])\n\nnum_rep = 30\nmax_degree_vals = np.repeat(max_degree_vals[:, np.newaxis], num_rep, axis=1)\nglobal_clust_vals = np.repeat(global_clust_vals[:, np.newaxis], num_rep, axis=1)\nmean_betweenness_vals = np.repeat(mean_betweenness_vals[:, np.newaxis], num_rep, axis=1)\nmean_path_vals = np.repeat(mean_path_vals[:, np.newaxis], num_rep, axis=1)\n\nВиводимо результат:\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\naxes[0, 0].set_title('Макс. ступінь вершини')\n\nfor i in range(len(graphs)):\n    axes[0, 0].plot(max_degree_vals[i], \n                    color=colors[i], \n                    marker=markers[i], \n                    linestyle=linestyles[i], \n                    label=labels[i])\naxes[0, 0].legend(fontsize=10)\n\n# Енергія \naxes[0, 1].set_title('Глобальний коефіцієнт кластеризації')\nfor i in range(len(graphs)):\n    axes[0, 1].plot(global_clust_vals[i], \n                    color=colors[i], \n                    marker=markers[i], \n                    linestyle=linestyles[i], \n                    label=labels[i])\naxes[0, 1].legend(fontsize=10)\n\n# Розрив\naxes[1, 0].set_title('Середній ступінь посередництва')\nfor i in range(len(graphs)):\n    axes[1, 0].plot(mean_betweenness_vals[i], \n                    color=colors[i], \n                    marker=markers[i], \n                    linestyle=linestyles[i], \n                    label=labels[i])\naxes[1, 0].legend(fontsize=10)\n\n# Момент \naxes[1, 1].set_title('Середня довжина найкоротшого шляху')\nfor i in range(len(graphs)):\n    axes[1, 1].plot(mean_path_vals[i], \n                    color=colors[i], \n                    marker=markers[i], \n                    linestyle=linestyles[i], \n                    label=labels[i])\naxes[1, 1].legend(fontsize=10)\n\nplt.show(); \n\n\n\n\n\n\n\nРис. 13.46: Топологічні міри складності канонічних і модельних графів: максимального значення ступеня вершини, глобального коефіцієнту кластеризації, середнього ступеня посередництва та середньої довжини найкоротшого шляху\n\n\n\n\n\nНа рисунку (Рис. 13.46) можна побачити наступне:\n\nпо-перше, найбільшим максимальним ступенем вершини характеризується саме граф-зірка, центр якої з’єднаний абсолютно з усіма вершинами мережі. Другим по ступенем концентрованності йде граф переважного приєднання, що, як ми вже зазначали, є найкращою моделлю реальних соціальних систем. До найпростіших можна віднести графи лінії, кола та Ердеша-Реньї;\nпо-друге, глобальний коефіцієнт кластеризації вказує на те, що найвищий ступінь кластеризації спостерігається саме для графа малого світу. Закономірно за ним іде граф переважного приєднання. Найпростішими знову виявляються графи Ердеша-Реньї, лінії, кола та, цього разу, зірки. Для зірки навіть візуально видно, що всі вершини мають тенденцію слідувати тільки за однією конкретною;\nпо-третє, середній ступінь посередництва є найнижчим для зірки, графа Ердеша-Реньї, малого світу та переважного приєднання. Для цих мереж передача інформації від одного вузла до іншого не займає значну частку часу. Для лінії та кола від одного кінця графа до іншого може знадобитися досить великий проміжок часу для передачі інформації. Схожа ситуація спостерігається й для середньої довжини найкоротшого шляху, оскільки міра посередництва на пряму залежить від значення найкоротшого шляху від одного вузла до іншого.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Лабораторна робота № 13</span>"
    ]
  },
  {
    "objectID": "lab_13.html#завдання-для-самостійної-роботи",
    "href": "lab_13.html#завдання-для-самостійної-роботи",
    "title": "13  Лабораторна робота № 13",
    "section": "13.3 Завдання для самостійної роботи",
    "text": "13.3 Завдання для самостійної роботи\n\nПроаналізуйте аналогічно інші з розрахованих мір складності як спектральних, так і топологічних\nВкажіть і аргументуйте, які з них, на вашу думку, кількісно описують складність досліджуваних мереж?\nПобудуйте залежність різних мережних показників для часового ряду реального світу по аналогії з двома попередніми рисунками\n\n\n\n\n\n[1] E. L. Platt, Network Science with Python and NetworkX Quick Start Guide: Explore and Visualize Network Data Effectively (Packt Publishing, 2019).\n\n\n[2] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein, Introduction to Algorithms, Fourth Edition (MIT Press, 2022).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Лабораторна робота № 13</span>"
    ]
  },
  {
    "objectID": "lab_14.html",
    "href": "lab_14.html",
    "title": "14  Лабораторна робота № 14",
    "section": "",
    "text": "14.1 Теоретичні відомості\nУ попередній роботі ми ввели поняття мір складності для найпростіших графів та поширених мережних моделей, порівняли деякі з мір складності. У даній роботі ми продемонструємо сучасні методи перетворення часових рядів у мережу (граф) з подальшим дослідженням відповідних спектральних і топологічних мір складності. Ми також покажемо, що вказані міри можна співставляти з динамікою вихідного часового ряду (звідси графодинаміка) і якщо вони є інформативними щодо можливих змін власне ряду, то їх можливо використовувати для побудови індикаторів характерної динаміки складних систем.\nБільшість складних систем інформують про свою структурну та динамічну природу, генеруючи послідовність певних характеристик, що можна представити часовими рядами. Останніми роками розроблено цікаві алгоритми перетворення часових рядів у мережу, що дозволяє розширити діапазон відомих характеристик часових рядів навіть до мережних  [1–3]. Останнім часом було запропоновано декілька підходів до перетворення часових послідовностей у складні мережеподібні відображення. Ці методи можна умовно розділити на три класи  [4]. Перший базується на вивченні “видимості” послідовних значень часового ряду і називається графом видимості (Visibility Graph, VG)  [4,5].\nДругий аналізує взаємне наближення різних відрізків часової послідовності і використовує техніку рекурентного аналізу  [4] (див. лабораторні 2 і 3). Рекурентна діаграма відображає існуючу повторюваність фазових траєкторій у вигляді бінарної матриці, елементами якої є одиниці або нулі, залежно від того, чи є близькими (рекурентними) із заданою точністю чи ні обрані точки фазового простору динамічної системи. Рекурентна діаграма легко трансформується в матрицю суміжності, за якою розраховуються спектральні та топологічні характеристики графа  [6].\nНарешті, якщо в основу формування зв’язків елементів графа покласти кореляційні відношення між ними, то отримаємо кореляційний граф  [4]. Для побудови та аналізу властивостей кореляційного графа необхідно сформувати з кореляційної матриці матрицю суміжності. Для цього необхідно ввести величину, яка для кореляційного поля буде слугувати відстанню між корельованими агентами. Така відстань може бути представлена як \\(d_{ij}=\\sqrt{2(1-C_{ij})}\\), де \\(C_{ij}\\) — це коефіцієнт кореляції між двома активами. Так, якщо коефіцієнт кореляції між двома активами значний, то відстань між ними невелика, і, починаючи з певного критичного значення \\(d_{cr}\\), активи можна вважати зв’язаними на графі. Для матриці суміжності це означає, що вони є суміжними на графі. В іншому випадку активи не є суміжними. У цьому випадку умова зв’язності графа є обов’язковою умовою.\nОсновною метою таких методів є точне відтворення інформації, що зберігається в часових рядах, в альтернативній математичній структурі, щоб згодом можна було використовувати потужні інструменти теорії графів для характеристики часових рядів з іншої точки зору з метою подолання розриву між нелінійним аналізом часових рядів, динамічних систем і теорією графів.\nУ даній роботі розглянемо лише алгоритм графа видимості (див. опис у лаб. 11).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Лабораторна робота № 14</span>"
    ]
  },
  {
    "objectID": "lab_14.html#теоретичні-відомості",
    "href": "lab_14.html#теоретичні-відомості",
    "title": "14  Лабораторна робота № 14",
    "section": "",
    "text": "14.1.1 Пакет ts2vg\nДля подальшої побудови класичного VG або його горизонтального аналогу, ми будемо використовувати бібліотеку ts2vg. Пакет ts2vg надає високопродуктивну реалізацію алгоритму для побудови графів видимості з даних часових рядів, вперше представленого Лукасом Лакасою та ін. у 2008 році  [5].\nГрафи видимості та деякі з їхніх властивостей (наприклад, степеневі розподіли) обчислюються швидко та ефективно навіть для часових рядів з мільйонами спостережень. Для обчислення графів використовується ефективний алгоритм “розділяй і володарюй”, коли це можливо  [7].\n\n14.1.1.1 Встановлення\nОстання випущена версія ts2vg доступна на PyPI і може бути легко встановлена шляхом запуску наступної команди:\n\n!pip install ts2vg\n\n\n\n14.1.1.2 Підтримувані типи графів\n\n14.1.1.2.1 Основні типи\n\nКласичний граф видимості  [5] (ts2vg.NaturalVG).\nГоризонтальний граф видимості  [8] (ts2vg.HorizontalVG).\n\n\n\n14.1.1.2.2 Доступні варіації\n\nЗважений граф видимості (через параметр weighted).\nНаправлений граф видимості (через параметр directed).\nПараметричний граф видимості  [9] (через параметри min_weight та max_weight).\nГраф обмеженої проникної видимості  [10,11] (через параметр penetrable_limit).\n\nЗверніть увагу, що кілька варіантів графів можна комбінувати і використовувати одночасно. Із більш детальною документацією можна ознайомитись на сайті бібліотеки ts2vg.\n\n\n14.1.1.2.3 Сумісність з іншими бібліотеками\nОтримані графи можуть бути легко перетворені в графові об’єкти з інших поширених графових бібліотек Python, таких як igraph, NetworkX та SNAP для подальшого аналізу.\nДля цього передбачені наступні методи:\n\nas_igraph();\nas_network();\nas_snap().",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Лабораторна робота № 14</span>"
    ]
  },
  {
    "objectID": "lab_14.html#хід-роботи",
    "href": "lab_14.html#хід-роботи",
    "title": "14  Лабораторна робота № 14",
    "section": "14.2 Хід роботи",
    "text": "14.2 Хід роботи\nСпочатку імпортуємо необхідні модулі для подальшої роботи:\n\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport numpy as np\nimport neurokit2 as nk\nimport yfinance as yf\nimport pandas as pd\nimport networkx as nx\nimport scienceplots\n\nfrom sklearn import preprocessing\nfrom tqdm import tqdm\nfrom ts2vg import NaturalVG, HorizontalVG\nfrom scipy.spatial import distance\n\n%matplotlib inline\n\nІ виконаємо налаштування рисунків для виведення:\n\nplt.style.use(['science', 'notebook', 'grid']) # стиль, що використовуватиметься\n                                               # для виведення рисунків\n\nparams = {\n    'figure.figsize': (8, 6),         # встановлюємо ширину та висоту рисунків за замовчуванням\n    'font.size': 22,                  # розмір фонтів рисунку\n    'lines.linewidth': 2,             # товщина ліній\n    'axes.titlesize': 'small',        # розмір титулки над рисунком\n    'axes.labelsize': 22,             # розмір підписів по осям\n    'legend.fontsize': 22,            # розмір легенди\n    'xtick.labelsize': 22,            # розмір розмітки по осі Ох\n    'ytick.labelsize': 22,            # розмір розмітки по осі Ох\n    \"font.family\": \"sans-serif\",      # сімейство стилів підписів \n    \"font.serif\": [\"Times\"],          # стиль підпису\n    'savefig.dpi': 300                # якість збережених зображень\n}\n\nplt.rcParams.update(params)           # оновлення стилю згідно налаштувань\n\nРозглянемо можливість використання графодинамічних показників у якості індикаторів або індикаторів-передвісників кризових явищ. Для прикладу завантажимо часовий ряд криптовалютного індексу Біткоїна за весь часовий період, що надається веб-ресурсом yfinance:\n\nsymbol = 'BTC-USD'          # Символ індексу\n\ndata = yf.download(symbol)  # вивантажуємо дані\ntime_ser = data['Adj Close'].copy()     # зберігаємо саме ціни закриття\ndate_in_num = mdates.date2num(time_ser.index)\n\nxlabel = 'time, days'    # підпис по вісі Ох \nylabel = symbol          # підпис по вісі Оу\n\n[*********************100%%**********************]  1 of 1 completed\n\n\n\n\n\n\n\n\nУвага\n\n\n\nВиконайте цей блок, якщо хочете зчитати дані не з Yahoo! Finance, а із власного файлу. Зрозуміло, що й аналіз результатів, і висновки залежать від того, з яким рядом ми працюємо\n\n\n\n\nsymbol = 'sMpa11'                  # Символ індексу\n\npath = \"databases\\sMpa11.txt\"      # шлях зчитування файлу\ndata = pd.read_csv(path,           # зчитування даних \n                   names=[symbol])\ntime_ser = data[symbol].copy()     # копіюємо значення кривої \n                                   # \"напруга-видовження\" до окремої змінної\n\ndate_in_num = mdates.date2num(time_ser.index)\n\nxlabel = r'$\\varepsilon$'          # підпис по вісі Ох \nylabel = symbol                    # підпис по вісі Оу\n\n\nВиведемо досліджуваний ряд:\n\nfig, ax = plt.subplots()                   # Створюємо порожній графік\nax.plot(time_ser.index, time_ser.values)   # Додаємо дані до графіка\nax.legend([symbol])                        # Додаємо легенду\nax.set_xlabel(xlabel)                      # Встановимо підпис по вісі Ох\nax.set_ylabel(ylabel)                      # Встановимо підпис по вісі Oy\n\nplt.xticks(rotation=45)                    # оберт позначок по осі Ох на 45 градусів\n\nplt.savefig(f'{symbol}.jpg')               # Зберігаємо графік \nplt.show();                                # Виводимо графік\n\n\n\n\n\n\n\nРис. 14.1: Динаміка щоденних значень індексу Біткоїна\n\n\n\n\n\nЯк і до цього, визначимо функцію для перетворення ряду (його стандартизації або знаходження прибутковостей). Для цього оголосимо функцію transformation(), що прийматиме на вхід часовий сигнал, тип ряду, і повертатиме його перетворення. Як показували попередні дослідження авторів, вихідне представлення часового ряду надає найбільш інформативне представлення для побудови графа. Тим не менш, ми допускаємо, що, наприклад, прибутковості фізичного сигналу можуть мати краще графове представлення, тому і визначаємо цю функцію в даній роботі.\n\ndef transformation(signal, ret_type):\n\n    for_graph = signal.copy()\n\n    if ret_type == 1:       # Зважаючи на вид ряду, виконуємо\n                            # необхідні перетворення\n        pass\n    elif ret_type == 2:\n        for_graph = for_graph.diff()\n    elif ret_type == 3:\n        for_graph = for_graph.pct_change()\n    elif ret_type == 4:\n        for_graph = for_graph.pct_change()\n        for_graph -= for_graph.mean()\n        for_graph /= for_graph.std()\n    elif ret_type == 5: \n        for_graph = for_graph.pct_change()\n        for_graph -= for_graph.mean()\n        for_graph /= for_graph.std()\n        for_graph = for_graph.abs()\n    elif ret_type == 6:\n        for_graph -= for_graph.mean()\n        for_graph /= for_graph.std()\n\n    for_graph = for_graph.dropna().values\n\n    return for_graph\n\nПовертаємо той самий вихідний сигнал.\nДалі задаємо параметри досліджуваного графа. Для подальших розрахунків ми будемо використовувати одні й ті ж самі значення часового вікна, кроку й типу ряду.\n\nsignal = time_ser.copy()\nret_type = 1            # вид ряду: 1 - вихідний, \n                        # 2 - детрендований (різниця між теп. значенням та попереднім)\n                        # 3 - прибутковості звичайні, \n                        # 4 - стандартизовані прибутковості, \n                        # 5 - абсолютні значення (волатильності)\n                        # 6 - стандартизований ряд\n\nfor_graph = transformation(signal, ret_type) # перетворення сигналу\n\nwindow = 250            # розмір вікна\ntstep = 1               # крок вікна\ngraph_type = 'classic'  # тип графу: classic, horizontal\n\nlength = len(time_ser)\n\n\n14.2.1 Побудова графа\nОскільки побудова графа для всього часового ряду може зайняти досить великий проміжок часу, ми будемо будувати граф видимості лише для його фрагменту. Для цього визначимо параметри index_begin та index_end, які будуть вказувати на початок відліку побудови та кінець. Для класичного графа видимості маємо:\n\nindex_begin = 1700\nindex_end = 2800\n\ndate = date_in_num[index_begin:index_end]\n\nif graph_type == 'classic':\n    g = NaturalVG(directed=None).build(for_graph[index_begin:index_end], xs=date)\n    pos1 = g.node_positions()\n    nxg = g.as_networkx()\nif graph_type == 'horizontal':\n    g = HorizontalVG(directed=None).build(for_graph[index_begin:index_end], xs=date)\n    pos1 = g.node_positions()\n    nxg = g.as_networkx()\n\ngraph_plot_options = {\n    'with_labels': False,\n    'node_size': 0,\n    'node_color': [(0, 0, 0, 1)],\n    'edge_color': [(0, 0, 0, 0.15)],\n}\n\n\nfig, ax = plt.subplots(1, 2, figsize=(15, 8))\n\nnx.draw_networkx(nxg, ax=ax[0], pos=pos1, **graph_plot_options)\nax[0].tick_params(bottom=True, labelbottom=True)\nax[0].plot(time_ser.index[index_begin:index_end], for_graph[index_begin:index_end], label=fr\"{ylabel}\")\nax[0].set_title(f'Зв\\'язки видимості для {ylabel}', pad=10)\nax[0].set_xlabel(xlabel)\nax[0].set_ylabel(f\"{ylabel}\")\nax[0].legend(loc='upper right')\nax[0].tick_params(axis='x', labelrotation=45)\n\n\nax[1].set_title(f'Графове представлення для {symbol}', pad=10)\n\n# визначаємо позицію вузлів на графі\npos2 = nx.spring_layout(nxg, k=0.15, iterations=100)\n\n# розраховуємо ступеневу центральність\ndegCent = nx.degree_centrality(nxg)\n\n# створити список розмірів вершин на основі ступеневої центральності\nnode_sizes = [v*100 for v in degCent.values()]\n\n# кольори вузлів на основі їх ступеневої центральності\nnode_colors = [v for v in degCent.values()]\n\n# будуємо граф\nnx.draw_networkx(nxg, ax=ax[1], pos=pos2,\n                node_size=node_sizes,  \n                node_color=node_colors,\n                with_labels=False,\n                cmap=plt.get_cmap('plasma'))\n\n# присвоюємо мінімальне та максимальне значення \n# ступеневої центральності для побудови теплової шкали\nvmin = np.asarray(list(degCent.values())).min()\nvmax = np.asarray(list(degCent.values())).max()\n\nsm = plt.cm.ScalarMappable(cmap=plt.get_cmap('plasma'), \n                           norm=plt.Normalize(vmin=vmin, vmax=vmax))\ncb = plt.colorbar(sm, ax=ax[1])\ncb.set_label('Ступенева центральність')\n\nplt.savefig(f\"Time_ser_connections_symbol={symbol}_idx_beg={index_begin}_\\\n            idx_end={index_end}_sertype={ret_type}_network_type={graph_type}.jpg\", bbox_inches=\"tight\", dpi=1000)\n\n\n\n\n\n\n\nРис. 14.2: Графік зв’язків видимості на основі природного VG напередодні крахів 21-го року на ринку Біткоїна та графове представлення цього фрагмента\n\n\n\n\n\nЯк ми можемо бачити з представленого рисунку, три послідовних зростання та спадання ціни BTC у 2021-2022 роках характеризуються доволі високим ступенем видимості в передкризовий період. Також дані піки утворюють орієнтовно 3 кластери із високою ступеневою центральністю. Крахові події на криптовалютному ринку можна розглядати як графи переважного приєднання, де, можливо, ключову роль у цих підйомах та спадах можуть відігравати один або декілька “китів” ринку, котрі чинять найбільший вплив на ринок і спрямовують вектор уваги всіх трейдерів у тому чи іншому напрямі.\n\n\n14.2.2 Віконна процедура\nДалі будемо спостерігати за тим, як змінюються властивості мережі з плином часу. Для цього використаємо добре знайому нам процедуру рухомого вікна. У рамках цієї процедури дослідимо графодинаміку як спектральних, так і топологічних показників.\nДля побудови парної динаміки конкретного індикатора та досліджуваного ряду визначимо функцію plot_pair:\n\ndef plot_pair(x_values, \n              y1_values,\n              y2_values,  \n              y1_label, \n              y2_label,\n              x_label, \n              file_name, clr=\"magenta\"):\n\n    fig, ax = plt.subplots()\n\n    ax2 = ax.twinx()\n\n    ax2.spines.right.set_position((\"axes\", 1.03))\n\n    p1, = ax.plot(x_values, \n                  y1_values, \n                  \"b-\", label=fr\"{y1_label}\")\n    p2, = ax2.plot(x_values,\n                   y2_values, \n                   color=clr, \n                   label=y2_label)\n\n    ax.set_xlabel(x_label)\n    ax.set_ylabel(f\"{y1_label}\")\n\n    ax.yaxis.label.set_color(p1.get_color())\n    ax2.yaxis.label.set_color(p2.get_color())\n\n    tkw = dict(size=2, width=1.5)\n\n    ax.tick_params(axis='x', rotation=35, **tkw)\n    ax.tick_params(axis='y', colors=p1.get_color(), **tkw)\n    ax2.tick_params(axis='y', colors=p2.get_color(), **tkw)\n\n\n    ax2.legend(handles=[p1, p2])\n\n    plt.savefig(file_name + \".jpg\")\n        \n    plt.show();\n\n\n14.2.2.1 Спектральні характеристики\nСпектральна теорія графів базується на вивченні властивостей графів через власні значення або власні вектори матриці суміжності \\(A\\) або матриці Лапласа \\(L\\)  [12].\nНагадаємо, що стандартна матриця Лапласа для графа \\(G\\) визначається як\n\\[\nL = D - A,\n\\tag{14.1}\\]\n\\(D\\) — діагональна матриця \\(G\\), де \\(i\\)-ий діагональний елемент є ступенем вершини \\(i\\) в \\(G\\)  [13], а \\(A\\) — матриця суміжності \\(G\\). У цій роботі ми представляємо спектральні характеристики для нормованої матриці Лапласа  [14], яка визначається як\n\\[\n\\hat{L} = D^{-1/2}LD^{-1/2}.\n\\tag{14.2}\\]\nЯкщо \\(\\lambda\\) — власне значення \\(\\hat{L}\\), тоді \\(\\lambda \\in [0, 2]\\)  [12]; тобто, нормалізуючи матрицю Лапласа, ми нормалізуємо власні значення.\n\nAlgebraicCon = []\nGraphEnergy = []\nSpecMoment_3 = []\nSpecRadius = []\nSpecGap = []\nNaturalConnectivity = []\n\n\nfor i in tqdm(range(0,length-window,tstep)):\n    # відбираємо фрагмент\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n    \n    if graph_type == 'classic':\n        g = NaturalVG(directed=None).build(fragm)\n        pos = g.node_positions()\n        nxg = g.as_networkx()\n    if graph_type == 'horizontal':\n        g = HorizontalVG(directed=None).build(fragm)\n        pos = g.node_positions()\n        nxg = g.as_networkx()\n    \n    # спектр власних значень матриці суміжності\n    adj_spectrum = nx.adjacency_spectrum(nxg).real\n\n    # сортуємо власні значення в порядку зростання\n    sorted_adj_spectrum = np.sort(adj_spectrum)\n    \n    # розраховуємо алгебраїчну зв'язність\n    alg_con = nx.algebraic_connectivity(nxg, normalized=True, method='tracemin_lu') \n\n    # розраховуємо енергію графа\n    graph_en = np.sum(np.abs(adj_spectrum))\n\n    # розраховуємо спектральний розрив\n    spec_gap = sorted_adj_spectrum[-1] - sorted_adj_spectrum[-2]\n\n    # розраховуємо спектральний радіус\n    spec_rad = np.max(np.abs(adj_spectrum))\n\n    # розраховуємо спектральний момент\n    spec_mom_3 = np.mean(adj_spectrum ** 3)\n\n    # розраховуємо природню зв'язність\n    nat_con = np.log(np.mean(np.exp(adj_spectrum)))\n    \n    AlgebraicCon.append(alg_con)\n    GraphEnergy.append(graph_en)\n    SpecRadius.append(spec_rad)\n    SpecGap.append(spec_gap)\n    SpecMoment_3.append(spec_mom_3)\n    NaturalConnectivity.append(nat_con)\n\n100%|██████████| 3207/3207 [02:56&lt;00:00, 18.19it/s]\n\n\nЗберігаємо абсолютні значення у текстовому документі. Також готуємо мітки для рисунків та назви збережених мір:\n\nind_names = ['algebraic_conn', 'graph_energy', 'spectral_radius', \n             'spectral_grap', 'spectral_moment_3', 'natural_connectivity']\n\nindicators = [AlgebraicCon, GraphEnergy, SpecRadius, \n              SpecGap, SpecMoment_3, NaturalConnectivity]\n\nmeasure_labels = [r'$\\lambda_2$', r'$E$', r'$R$', r'$\\delta$', r'$m_3$', r'$N_c$']\n\nfile_names = []\n\nfor i in range(len(ind_names)):\n    name = f\"{ind_names[i]}_symbol={symbol}_wind={window}_step={tstep}_seriestype={ret_type}_graph_type={graph_type}\"\n    np.savetxt(name + \".txt\", indicators[i])\n    file_names.append(name)\n\n\n14.2.2.1.1 Алгебраїчна зв’язність\nЩодо власних значень матриці Лапласа, однією з основних характеристик, яку ми можемо отримати, є алгебраїчна зв’язність \\(\\lambda_2\\) графа, яка відповідає другому найменшому власному значенню матриці. Цей показник відображає кількість роз’єднаних компонент. Для незв’язного графа \\(\\lambda_2\\) буде дорівнювати нулю, а для графа з вищою щільністю зв’язків \\(\\lambda_2\\) буде більшим. Використовуючи цей показник, можливо визначити відмовостійкість і синхронізованість досліджуваної системи.\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          indicators[0], \n          ylabel, \n          measure_labels[0],\n          xlabel,\n          file_names[0],\n          clr=\"magenta\")\n\n\n\n\n\n\n\nРис. 14.3: Динаміка індексу BTC та алгебраїчної зв’язності\n\n\n\n\n\nНа Рис. 14.3 видно, що \\(\\lambda_2\\) зростає в передкризові періоди часу, що говорить про зростання ступеня синхронізованності між трейдерами ринку в дані періоди часу. Мережа крипторинку набуває все більшої корельованості та стійкості. Подібна динаміка може вказувати на зростання узгодженності між великими гравцями ринку щодо своїх подальших дій на Біткоїні.\n\n\n14.2.2.1.2 Енергія графа\nЗ власних значень матриці суміжності \\(A\\) з \\(G\\) можна визначити таку міру, як енергія графа \\(E(G)\\)  [15,16]:\n\\[\nE = E(G) = \\sum_{i=1}^{N}\\left| \\lambda_i \\right|.\n\\tag{14.3}\\]\nПодібно до \\(\\lambda_2\\), ми маємо повністю роз’єднаний граф, коли \\(E(G)=0\\). Для кожного \\(\\lambda_i &gt; 0\\) існує багато ребер \\(e_{ij}\\), які визначають високу та ефективну зв’язність \\(G\\).\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          indicators[1], \n          ylabel, \n          measure_labels[1],\n          xlabel,\n          file_names[1],\n          clr=\"crimson\")\n\n\n\n\n\n\n\nРис. 14.4: Динаміка індексу BTC та енергії графа\n\n\n\n\n\nРис. 14.4 демонструє, що в періоди відносної стабільності \\(E\\) залишається на досить низькому рівні, що вказує на роз’єднаність трейдерів ринку в подібні періоди. Як покупці, так і продавці діють досить некорельовано. У передкризові періоди енергія починає зростати, що вказує на зростання ефективності роботи між гравцями ринку та їх зв’язності.\n\n\n14.2.2.1.3 Cпектральний радіус\nКрім наведених вище мір, можна визначити такі міри, як спектральний радіус, яка є найбільшим абсолютним власним значенням матриці \\(A\\):\n\\[\nR = R(G) = \\max_{1\\leq i \\leq N}\\left| \\lambda_i \\right|.\n\\tag{14.4}\\]\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          indicators[2], \n          ylabel, \n          measure_labels[2],\n          xlabel,\n          file_names[2],\n          clr=\"orange\")\n\n\n\n\n\n\n\nРис. 14.5: Динаміка індексу BTC та спектрального радіуса\n\n\n\n\n\nНа даному рисунку (Рис. 14.5) видно, що спектральний радіус зростає в кризові й передкризові періоди, що вказує на зростання корельованості графа Біткоїна та синхронізованості дій трейдерів.\n\n\n14.2.2.1.4 Cпектральний розрив\nПроранжувавши власні значення матриці суміжності \\(G\\) у неспадаючому порядку, тобто \\(\\lambda_1 \\leq \\lambda_2 \\leq ... \\leq \\lambda_n\\), ми можемо визначити таку міру, як спектральний розрив:\n\\[\n\\delta = \\delta(G) = \\lambda_{n} - \\lambda_{n-1}\n\\tag{14.5}\\]\nдля якого \\(\\lambda_n\\) — перше найбільше власне значення \\(\\hat{L}\\), а \\(\\lambda_{n-1}\\) друге найбільше власне значення. Спектральний розрив показує швидкість синхронізації в досліджуваній мережі. Чим він більший, тим більш взаємопов’язаними є вершини і тим складнішим є граф.\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          indicators[3], \n          ylabel, \n          measure_labels[3],\n          xlabel,\n          file_names[3],\n          clr=\"darkgreen\")\n\n\n\n\n\n\n\nРис. 14.6: Динаміка індексу BTC та спектрального розриву\n\n\n\n\n\nРис. 14.6 демонструє, що спектральний розрив також є показником синхронізованності ринку в передкризові період. Однак, із динаміки даного показника можна сказати, що в моменти криз найбільшу кількість інформації починає нести найбільше власне значення матриці Лапласа. Можна припустити, що друге і третє також можуть слугувати в якості індикаторів крахових подій, але найбільше власне значення в даному випадку представляється найкращим рішенням.\n\n\n14.2.2.1.5 Cпектральний момент\nСпектральною мірою складності, яку ми також хотіли б представити є \\(k\\)-й спектральний момент. Для невід’ємного цілого числа \\(k\\), \\(k\\)-ий спектральний момент визначається як\n\\[\nm_k = m_k(G) = \\sum_{i=1}^{N}\\lambda_{i}^{k},\n\\tag{14.6}\\]\nде \\(m_k\\) дорівнює кількості замкнутих обходів довжини \\(k\\)  [17]. Кількість замкнутих обходів є важливим показником для вимірювання складності системи. Як було показано в роботі Ву та ін.  [18], використовуючи кількість замкнутих обходів всієї довжини, ми можемо виміряти складність графа та надлишковість альтернативних найкоротших шляхів. Отже, більші значення \\(m_k\\) відповідають більшій складності мережі. Для подальших обчислень ми обрали \\(k=3\\).\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          indicators[4], \n          ylabel, \n          measure_labels[4],\n          xlabel,\n          file_names[4],\n          clr=\"chocolate\")\n\n\n\n\n\n\n\nРис. 14.7: Динаміка індексу BTC та спектрального моменту\n\n\n\n\n\nЗ динаміки \\(m^3\\) видно, що найбільш суттєвим ступенем синхронізованності характеризувалась предкризова динаміка 2018, середини 2019 і 2021 років. У ці періоди часу ми мали найбільшу кількість досить високих власних значень матриці Лапласа, а одже й достатньо високий ступінь синхронізації ринку в зазначені періоди.\n\n\n14.2.2.1.6 Cпектральна природна зв’язність\nЮнь та ін.  [19] запропонували вимірювати “середнє власне значення” спектра суміжності графа \\(G\\). Було запропоновано називати цей показник природною зв’язністю або природним власним значенням:\n\\[\nN_c = N_c(G) = \\ln{\\left( \\frac{1}{N}\\sum_{i=1}^{N}\\exp{\\lambda_i} \\right)}.\n\\tag{14.7}\\]\nЕстрада  [20], Ву та ін.  [18] показали, що (14.7) є чутливою та надійною мірою стійкості мережі.\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          indicators[5], \n          ylabel, \n          measure_labels[5],\n          xlabel,\n          file_names[5],\n          clr=\"black\")\n\n\n\n\n\n\n\nРис. 14.8: Динаміка індексу BTC та спектральної природної зв’язності\n\n\n\n\n\nНа Рис. 14.8 видно, що показник природної зв’язності зростає у передкризові періоди часу. Тобто, даний показник можна використовувати в якості індикатора або індикатора-передвісника крахових подій на ринку Біткоїна. Особливо характерним є зростання ступеня синхронізованності ринку напередодні 2018 року, що може вказувати на початкові стадії зміцнення стійкості криптовалютної мережі. Досить високий ступінь синхронізованності ринку можна спостерігати напередодні середини 2019 та початку 2021 років.\n\n\n\n14.2.2.2 Топологічні міри центральності\nІснує багато способів кількісно оцінити важливість вершини або ребра з точки зору певного мережного атрибуту, відображаючи таким чином топологію складної мережі.\n\nDegreeMax = []\nGlobalEigenvectorCentrality = []\nGlobalClosenessCentrality = []\nGlobalInformationCentrality = []\nGlobalBetweennessCentrality = []\nGlobalHarmonicCentrality = []\n\n\nfor i in tqdm(range(0,length-window,tstep)):\n    # відбираємо фрагмент\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n    \n    if graph_type == 'classic':\n        g = NaturalVG(directed=None).build(fragm)\n        pos = g.node_positions()\n        nxg = g.as_networkx()\n    if graph_type == 'horizontal':\n        g = HorizontalVG(directed=None).build(fragm)\n        pos = g.node_positions()\n        nxg = g.as_networkx()\n    \n    # максимальний ступінь вершини\n    deg_max = max(dict(nxg.degree()).values())\n\n    # середній ступінь впливовості\n    glob_eigenvector_centrality = np.mean(list(nx.eigenvector_centrality_numpy(nxg).values()))\n\n    # середній ступінь близькості\n    glob_closeness_centrality = np.mean(list(nx.closeness_centrality(nxg).values()))\n\n    # середній ступінь інформаційності\n    glob_information_centrality = np.mean(list(nx.information_centrality(nxg).values()))\n\n    # максимальний ступінь посередництва\n    glob_betweenness_centrality = np.max(list(nx.betweenness_centrality(nxg).values()))\n\n    # середній ступінь гармонійності\n    glob_harm_centrality = np.mean(list(nx.harmonic_centrality(nxg).values()))\n\n    DegreeMax.append(deg_max)\n    GlobalEigenvectorCentrality.append(glob_eigenvector_centrality)\n    GlobalClosenessCentrality.append(glob_closeness_centrality)\n    GlobalInformationCentrality.append(glob_information_centrality)\n    GlobalBetweennessCentrality.append(glob_betweenness_centrality)\n    GlobalHarmonicCentrality.append(glob_harm_centrality)\n\n100%|██████████| 3207/3207 [25:34&lt;00:00,  2.09it/s]\n\n\nЗберігаємо абсолютні значення у текстовому документі. Також готуємо мітки для рисунків та назви збережених:\n\nind_names = ['DegreeMax', 'GlobalEigenvectorCentrality', 'GlobalClosenessCentrality', \n             'GlobalInformationCentrality', 'GlobalBetweennessCentrality', 'GlobalHarmonicCentrality']\n\nindicators = [DegreeMax, GlobalEigenvectorCentrality, GlobalClosenessCentrality, \n              GlobalInformationCentrality, GlobalBetweennessCentrality, GlobalHarmonicCentrality]\n\nmeasure_labels = [r'$D_{max}$', r'$X$', r'$C$', r'$I$', r'$B$', r'$GHc$']\n\nfile_names = []\n\nfor i in range(len(ind_names)):\n    name = f\"{ind_names[i]}_symbol={symbol}_wind={window}_step={tstep}_seriestype={ret_type}_graph_type={graph_type}\"\n    np.savetxt(name + \".txt\", indicators[i])\n    file_names.append(name)\n\n\n14.2.2.2.1 Максимальний ступінь вершини\nСтупінь вершини або ступенева центральність є концептуально найпростішою метрикою для опису характеристик зв’язку однієї вершини в складній мережі. Вона може бути представлена у вигляді\n\\[\nd_i = \\sum_{j=1}^{N}A_{ij},\n\\tag{14.8}\\]\nде \\(d_i\\) підраховує кількість \\(j\\)-их ребер, що інцидентні вершині \\(i\\).\nОкрім ступеня конкретної вершини, ми можемо визначити вершину з найбільшою кількістю інцидентних ребер. Кількість таких вершин можемо позначити як \\(D_{max}\\):\n\\[\nD_{max} = \\max_{i=1,...,N}d_i.\n\\tag{14.9}\\]\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          indicators[0], \n          ylabel, \n          measure_labels[0],\n          xlabel,\n          file_names[0],\n          clr=\"magenta\")\n\n\n\n\n\n\n\nРис. 14.9: Динаміка індексу BTC та максимального ступеня вершини\n\n\n\n\n\nНа даному рисунку (Рис. 14.9) видно, що максимальний ступінь вершини починає зростати в кризові та передкризові періоди, що вказує на зростання центральності одного або декількох вузлів. Можна припустити, що один або декілька трейдерів ринку починають концентрувати на собі увагу всіх інших діячів дотичних до ринку Біткоїна.\n\n\n14.2.2.2.2 Середній ступінь впливовості\nСтупінь впливовості обчислює оцінку важливості вузла шляхом додавання впливовостей його сусідів. Впливовість для вузла \\(i\\) — це \\(i\\)-й елемент власного вектора \\(x\\), пов’язаний з власним значенням \\(\\lambda\\) максимального модуля, який є додатним. Такий власний вектор \\(x\\) визначається з точністю до мультиплікативної константи рівнянням\n\\[\n\\lambda x^{T} = x^{T}A,\n\\tag{14.10}\\]\nде \\(A\\) — матриця суміжності графа \\(G\\). Наведене вище рівняння еквівалентне наступному:\n\\[\n\\lambda x_i = \\sum_{j \\to i}x_j.\n\\tag{14.11}\\]\nТобто, додавання ступенів впливовості попередників вершини \\(i\\) дає ступінь впливовості \\(i\\), помножену на \\(\\lambda\\). У випадку неорієнтованих графів \\(x\\) також розв’язує знайоме рівняння \\(Ax =\\lambda x\\).\nЗа теоремою Перрона-Фробеніуса  [21], якщо \\(G\\) сильно зв’язний, то існує єдиний власний вектор \\(x\\), і всі його елементи строго додатні.\nЯкщо \\(G\\) не є сильно зв’язним, то може існувати декілька лівих власних векторів, пов’язаних з \\(\\lambda\\), причому деякі з їх елементів можуть дорівнювати нулю.\n\n\n\n\n\n\nПримітка\n\n\n\nСтупінь впливовості або центральність за власним вектором було введено Ландау  [22] для шахових турнірів. Пізніше його знову відкрив Вей  [23], а потім популяризував Кендалл  [24] в контексті спортивного рейтингу. Берге ввів загальне визначення для графів, заснованих на соціальних зв’язках  [25]. Бонаcіч  [26] знову ввів центральність власного вектора і зробив її популярною в аналізі зв’язків.\nЦя функція обчислює лівий домінуючий власний вектор, що відповідає додаванню впливовості попередників: це звичайний підхід. Щоб додати центральність наступників, спочатку переверніть граф за допомогою G.reverse().\nЦя реалізація використовує SciPy sparse eigenvalue solver (ARPACK) для пошуку найбільшої пари власне значення/власний вектор за допомогою ітерацій Арнольді  [27]\n\n\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          indicators[1], \n          ylabel, \n          measure_labels[1],\n          xlabel,\n          file_names[1],\n          clr=\"crimson\")\n\n\n\n\n\n\n\nРис. 14.10: Динаміка індексу BTC та середнього ступеня впливовості\n\n\n\n\n\nСередній ступінь впливовості характеризується зростанням у передкризові періоди, як правило, спадом під час криз. Подібно до спектральних показників, середній ступінь впливовсті вказує на зростання одного або декількох власних значень матриці суміжності графа та значущості власних векторів центральних вузлів торгівельного графа Біткоїна.\n\n\n14.2.2.2.3 Середній ступінь близькості\nУ мережі відстань \\(l_{ij}\\) між вузлом \\(i\\) та вузлом \\(j\\) позначає кількість ребер, які з’єднують найкоротший шлях між цими двома вузлами. Спираючись на поняття довжини найкоротшого шляху між двома вузлами, ми можемо надати різні міри, які характеризують зв’язність всієї мережі. Однією з таких мір є центральність або середній ступінь близькості зв’язку між вершиною \\(i\\) та всіма іншими вершинами\n\\[\nc_i = (N-1)\\Bigg/\\left( \\sum_{j=1}^{N}l_{ij} \\right),\n\\tag{14.12}\\]\nщо надають зворотнє середнє по всім найкоротшим шляхам від \\(i\\) до всіх вузлів \\(j\\).\nСереднє арифметичне значення ступеня близькості для кожного \\(i\\)-го вузла дає нам глобальний (середній) ступінь близькості:\n\\[\nC = \\frac{1}{N}\\sum_{i=1}^{N}c_i.\n\\tag{14.13}\\]\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          indicators[2], \n          ylabel, \n          measure_labels[2],\n          xlabel,\n          file_names[2],\n          clr=\"orange\")\n\n\n\n\n\n\n\nРис. 14.11: Динаміка індексу BTC та середнього ступеня близькості\n\n\n\n\n\nЯк можна бачити з представленого рисунку (Рис. 14.11), глобальний ступінь близькості зростає в кризові та передкризові періоди, що вказує на спад довжини найкоротших шляхів у графі видимості криптовалютного ринку. Даний показник зростав напередодні останніх чотирьох років. Це вказує на те, що передодні досліджуваних торгівельних років між трейдерами зростав ступінь синхронізованності їх дій.\n\n\n14.2.2.2.4 Середній ступінь інформаційності\nДля визначення центральності будь-якого вузла \\(i\\) пропонується спочатку визначити його інформаційну зв’язність з іншими вузлами, тобто \\(\\left\\{I_{ij} | j=1,..., N \\right\\}\\). Середнє гармонійне значення інформації щодо шляху від вузла \\(i\\) до інших вузлів буде використовуватися для визначення ступеня інформаційності вузла \\(i\\). Зокрема, якщо \\(I_i\\) пов’язано з центральністю або інформаційністю вузла \\(i\\), то\n\\[\n\\hat{I_i} = \\left( \\frac{1}{N}\\sum_{j=1}^{N}\\frac{1}{I_{ij}} \\right)^{-1}.\n\\tag{14.14}\\]\nЗгідно зі Стівенсоном та Зеленом  [28], ступінь інформаційності можна обчислити шляхом інвертування простої матриці. Перш за все, ми визначаємо \\(N \\times N\\) матрицю \\(B=\\left\\{{b_{ij}} \\right\\}\\), де\n\\[\nb_{ij} = \\begin{cases}\n        0 & \\text{якщо} \\,\\, i \\,\\, та \\,\\, j \\,\\, суміжні,\\\\\n        1 & \\text{інакше},\n    \\end{cases}\n\\tag{14.15}\\]\nі \\(b_{ii}=1+d_i\\), де \\(d_i\\) ступінь вершини \\(i\\).\nДалі, визначивши матрицю \\(C = \\left\\{ c_{ij} \\right\\} = B^{-1}\\), ми можемо розрахувати \\(I_{ij}\\) згідно рівняння\n\\[\nI_{ij} = (c_{ii} + c_{jj} - 2c_{ij})^{-1}.\n\\tag{14.16}\\]\nЕлемент \\(\\sum_{j=1}^{N} 1/I_{ij}\\) у рівнянні (14.14) можна переписати наступним чином:\n\\[\n\\sum_{j=1}^{N}c_{ii} + c_{jj} - 2c_{ij} = Nc_{ii} + T - 2R,\n\\tag{14.17}\\]\nде \\(T = \\sum_{j=1}^{N}c_{jj}\\) і \\(R = \\sum_{j=1}^{N}c_{ij}\\).\nОтже, ступінь інформаційності вузла \\(i\\) може бути представлений як\n\\[\nI_i = \\left[ (Nc_{ii} + T - 2R)/N \\right]^{-1} = \\left[ c_{ii} + (T-2R)/N \\right]^{-1}.\n\\tag{14.18}\\]\nСхожим чином, для вимірювання глобального ступеня інформаційності ми розглядаємо середнє арифметичне локального ступеня інформаційності:\n\\[\n\\hat{I} = \\frac{1}{N}\\sum_{i=1}^{N}I_i.\n\\tag{14.19}\\]\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          indicators[3], \n          ylabel, \n          measure_labels[3],\n          xlabel,\n          file_names[3],\n          clr=\"darkgreen\")\n\n\n\n\n\n\n\nРис. 14.12: Динаміка індексу BTC та середнього ступеня впливовості\n\n\n\n\n\nНа (Рис. 14.12) видно, що глобальний ступінь інформаційності зростає в передкризові періоди, що є індикатором зростання ефективності передачі інформації між трейдерами ринку та зростання детермінованності динаміки ринку.\n\n\n14.2.2.2.5 Максимальний ступінь посередництва\nІншою часто досліджуваною характеристикою вершин на основі шляхів є ступінь посередництва, яка вимірює частку всіх найкоротших шляхів у мережі, що проходять від \\(i\\) до \\(j\\) через вершину \\(k\\). Для загальної кількості найкоротших шляхів між вершинами \\(i\\) та \\(j\\), позначених як \\(\\sigma(i,j)\\), і найкоротших шляхів, що проходять через дану вершину \\(k(\\sigma(i, j | k))\\), ступінь посередництва можна визначити як\n\\[\nb_k = \\sum_{i,j=1; i,j \\neq k}^{N} \\sigma(i, j | k) / \\sigma(i, j).\n\\tag{14.20}\\]\nЩоб віднайти найбільшу кількість інформації, що проходить через конкретний \\(k\\)-й, ми вимірюємо максимальний ступінь посередництва, розглядаючи кожен \\(k\\)-ий вузол:\n\\[\nB = \\max_{i=1,...,N}b_{i}.\n\\tag{14.21}\\]\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          indicators[4], \n          ylabel, \n          measure_labels[4],\n          xlabel,\n          file_names[4],\n          clr=\"chocolate\")\n\n\n\n\n\n\n\nРис. 14.13: Динаміка індексу BTC та середнього ступеня посередництва\n\n\n\n\n\nЯк можна бачити, показник максимального ступеня посередництва спадає в передкризові періоди, що вказує на спад кількості посередників через яких може проходити інформація стосовно подальної динаміки Біткоїна. Це говорить про те, що на ринку зв’являються один або декілька трейдерів на котрих концентрується увага майже всіх інших, і зв’язок усіх трейдерів із найбільш впливомими може здійснюватись в один або декілька найкоротших шляхів.\n\n\n14.2.2.2.6 Середній ступінь гармонійності\nМарчіорі та Латора  [29] запропонували міру, подібну до (14.12), яка називається ступенем гармонійності. Для заданого вузла \\(j\\) вона може бути визначена як\n\\[\nHc_{j} = \\sum_{i=1, i \\neq j}^{N} \\left( l_{ij} \\right)^{-1},\n\\tag{14.22}\\]\nде \\((l_{ij})^{-1}=0\\), якщо між вузлами \\(i\\) та \\(j\\) немає шляху. Середній ступінь гармонійності визначається через середнє арифметичне локальних ступенів гармонійності.\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          indicators[5], \n          ylabel, \n          measure_labels[5],\n          xlabel,\n          file_names[5],\n          clr=\"black\")\n\n\n\n\n\n\n\nРис. 14.14: Динаміка індексу BTC та середнього ступеня гармонійності\n\n\n\n\n\n\n\n\n14.2.2.3 Асортативність\nАсортативність означає тенденцію в мережі до з’єднання вузлів з подібними властивостями, тоді як диассортативність виявляється у з’єднанні вузлів з різнорідними властивостями. Реальні мережі можуть демонструвати різну асортативність. Соціальні мережі, такі як взаємодії між вченими або корпоративними директорами, зазвичай мають позитивну асортативність. З іншого боку, технологічні та біологічні мережі, такі як електромережі, Інтернет, білкові взаємодії, нейронні мережі та харчові мережі, зазвичай виявляють негативну асортативність.\nДалі буде представлено декілька показників асортативності для передчасної ідентифікації криптовалютних криз.\n\nAssortativity = []\nAvgDegreeConnectivity = []\n\n\nfor i in tqdm(range(0,length-window,tstep)):\n    # відбираємо фрагмент\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n    \n    if graph_type == 'classic':\n        g = NaturalVG(directed='left_to_right').build(fragm)\n        pos = g.node_positions()\n        nxg_dir = g.as_networkx()\n    if graph_type == 'horizontal':\n        g = HorizontalVG(directed='left_to_right').build(fragm)\n        pos = g.node_positions()\n        nxg_dir = g.as_networkx()\n    \n    # розрахунок асортативності\n    assort = nx.degree_pearson_correlation_coefficient(nxg_dir)\n\n    # середня степенева зв'язність\n    avg_deg_con = np.mean(list(nx.average_degree_connectivity(nxg_dir, source=\"in\", target=\"in\").values()))\n\n    Assortativity.append(assort)\n    AvgDegreeConnectivity.append(avg_deg_con)\n\n 14%|█▍        | 465/3207 [00:04&lt;00:26, 103.14it/s]100%|██████████| 3207/3207 [00:31&lt;00:00, 102.21it/s]\n\n\n\nind_names = ['Assortativity', 'AvgDegreeConnectivity']\n\nindicators = [Assortativity, AvgDegreeConnectivity]\n\nmeasure_labels = [r'$r$', r'$\\langle d_{nn}^{w} \\rangle$']\n\nfile_names = []\n\nfor i in range(len(ind_names)):\n    name = f\"{ind_names[i]}_symbol={symbol}_wind={window}_step={tstep}_seriestype={ret_type}_graph_type={graph_type}\"\n    np.savetxt(name + \".txt\", indicators[i])\n    file_names.append(name)\n\n\n14.2.2.3.1 Середня ступенева зв’язність\nСередня ступенева зв’язність \\(d_{nn}(d)\\) для вершин зі ступенем \\(d\\) є ще однією мірою, яка використовується для дослідження структури мереж  [30]. Оскільки вона може бути виражена як \\(d_{nn}(d) = \\sum_{d'}d^{'}P(d^{'}|d)\\), де \\(P(d^{'}| 𝑑)\\) — умовна ймовірність того, що дана вершина зі ступенем \\(d\\) пов’язана з вершиною зі ступенем \\(d^{'}\\). Ця величина виражає кореляцію між ступенями зв’язаних вершин  [31]. За відсутності кореляцій між ступенями, \\(𝑃(𝑑^{'}| 𝑑)\\) не залежить від \\(𝑑\\), а також від середнього ступеня найближчих сусідів, тобто \\(d_{nn}(𝑑)=\\text{const}\\)  [30]. За наявності кореляцій поведінка \\(d_{nn}(d)\\) визначає два загальні класи мереж. Якщо \\(d_{nn}(d)\\) є зростаючою функцією від \\(d\\), тоді вершини з високим (низьким) ступенем мають більшу ймовірність бути пов’язаними з вершинами з вищим (нижчим) ступенем. Ця властивість у різних галузях науки називається асортативним змішуванням  [32]. Навпаки, спадна поведінка \\(d_{nn}(d)\\) визначає дизасортативне змішування, в тому сенсі, що вершини з високим (низьким) ступенем мають більшість сусідів з низьким (високим) ступенем вершин.\nМіру такої сортативності чи дизасортативності для сусідів певної вершини \\(i\\) можна визначити як середню ступеневу зв’язність (середньозважений ступінь найближчого сусіда):\n\\[\nd_{i}^{w} = \\frac{1}{s_i}\\sum_{j=1}^{N}A_{ij}w_{ij}d_{j},\n\\tag{14.23}\\]\nде \\(s_i = \\sum_{j=1}^{N}A_{ij}w_{ij}\\) — це “сила” \\(i\\)-го вузла; \\(A_{ij}\\) — це елемент матриці суміжності \\(A\\); \\(w_{ij}\\) — це вага ребра \\(e_{ij}\\) (у нашому випадку вона дорівнює 1); \\(d_j\\) представляє ступінь вершини \\(j\\)-го сусіда.\nЗагалом, це рівняння вимірює ступінь тяжіння сусідів з високим або низьким ступенем вершини один до одного відносно величини фактичних взаємодій.\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          indicators[1], \n          ylabel, \n          measure_labels[1],\n          xlabel,\n          file_names[1],\n          clr=\"darkorange\")\n\n\n\n\n\n\n\nРис. 14.15: Динаміка індексу BTC та середньої ступеневої зв’язності\n\n\n\n\n\nЯк можна бачити з даного рисунка (Рис. 14.15), середня ступенева зв’язність зростає в передкризові періоди, що вказує на поступове зростання ступеня тяготіння вершин з високою степеневою центральністю до вершин із ще вищою центральністю.\n\n\n14.2.2.3.2 Ступінь асортативності\nІнша форма асортативного змішування залежить від однієї або декількох скалярних властивостей вершин мережі. Для його обчислення ми визначаємо матрицю \\(e_{ij}\\), яка задовольняє правилам додавання: \\(\\sum_{ij}e_{ij}=1\\), \\(\\sum_{j}e_{ij}=a_i\\), \\(\\sum_{i}e_{ij}=b_j\\), де \\(a_i\\) та \\(b_j\\) — частки ребер, які починаються та закінчуються у вершинах \\(i\\) та \\(j\\). Розрахувавши коефіцієнт кореляції Пірсона, можна визначити ступінь асортативності  [32]. Таким чином, цей коефіцієнт асортативності обчислюється як\n\\[\nr = \\sum_{xy}xy(e_{xy} - a_{x}b_{y}) \\Big/ \\sigma_{a}\\sigma_{b},\n\\tag{14.24}\\]\nа \\(\\sigma_{a}\\) та \\(\\sigma_{b}\\) визначають стандартні відхилення розподілів \\(a_x\\) та \\(b_y\\); \\(-1 \\leq r \\leq 1\\), де \\(r&lt;0\\) вказує на вищу дизасортативність, \\(r&gt;0\\) демонструє вищу асортативність, а \\(r=0\\) говорить про відсутність асортативності між вершинами.\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          indicators[0], \n          ylabel, \n          measure_labels[0],\n          xlabel,\n          file_names[0],\n          clr=\"darkgreen\")\n\n\n\n\n\n\n\nРис. 14.16: Динаміка індексу BTC та середнього ступеня асортативності\n\n\n\n\n\nНа Рис. 14.16 видно, що коефіцієнт асортативності спадає в передкризові періоди, що вказує на дизасортативну поведінку ринку в ці моменти часу: вершини з малим ступенем зв’язності й централізованості тяжіють до вершин, що характеризуються високим ступенем посередництва, гармонійності, інформаційності, близькості тощо. Як уже зазначалося, дизасортативність, що властива передкризовим періодам Біткоїна, характерна і як для реальних соціальних мереж, так і для складних біологічних мереж.\n\n\n\n14.2.2.4 Кластеризація\nУ теорії графів, коефіцієнт кластеризації вказує на те, наскільки вузли у графі мають тенденцію групуватися. Дослідження показують, що у більшості реальних мереж, зокрема, у соціальних мережах, вузли зазвичай утворюють компактні групи з високою кількістю зв’язків між ними.\nДля подальшого аналізу розглянемо показники транзитивності, глобальної тріадної і квадратичної кластеризацій.\n\nTransitivity = []\nAvgClustering = []\nAvgSquareClustering = []\n\n\nfor i in tqdm(range(0,length-window,tstep)):\n    # відбираємо фрагмент\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n    \n    if graph_type == 'classic':\n        g = NaturalVG(directed=None).build(fragm)\n        pos = g.node_positions()\n        nxg = g.as_networkx()\n    if graph_type == 'horizontal':\n        g = HorizontalVG(directed=None).build(fragm)\n        pos = g.node_positions()\n        nxg = g.as_networkx()\n\n    # транзитивність\n    trans = nx.transitivity(nxg)\n\n    # глобальний коефіцієнт кластеризації\n    avg_clust = nx.average_clustering(nxg)\n\n    # коефіцієнт квадратичної кластеризації \n    avg_sqr_clust = np.mean(list(nx.square_clustering(nxg).values()))\n    \n    Transitivity.append(trans)\n    AvgClustering.append(avg_clust)\n    AvgSquareClustering.append(avg_sqr_clust)\n\n  1%|          | 34/3207 [00:05&lt;09:40,  5.47it/s]100%|██████████| 3207/3207 [18:20&lt;00:00,  2.91it/s]\n\n\n\nind_names = ['AvgClustering', 'Transitivity', 'AvgSquareClustering']\n\nindicators = [AvgClustering, Transitivity, AvgSquareClustering]\n\nmeasure_labels = [r'$\\langle C_3 \\rangle$', r'$T$', r'$\\langle C_4 \\rangle$']\n\nfile_names = []\n\nfor i in range(len(ind_names)):\n    name = f\"{ind_names[i]}_symbol={symbol}_wind={window}_step={tstep}_seriestype={ret_type}_graph_type={graph_type}\"\n    np.savetxt(name + \".txt\", indicators[i])\n    file_names.append(name)\n\n\n14.2.2.4.1 Коефіцієнт глобальної кластеризації\nДля того, щоб охарактеризувати щільність зв’язків між сусідами вершини \\(i\\), ми можемо використати коефіцієнт локальної кластеризації:\n\\[\nC_{i}^{3} = \\sum_{k,j=1}^{N}A_{ik}A_{kj}A_{ji} \\Bigg/ d_i(d_i - 1),\n\\tag{14.25}\\]\nде чисельник позначає кількість закритих трикутників, що містять вершину \\(i\\).\nМи можемо розглядати глобальний коефіцієнт кластеризації, як середнє арифметичне локального коефіцієнта кластеризації трикутників  [33]:\n\\[\n\\langle C^{3} \\rangle = \\frac{1}{N}\\sum_{i=1}^{N}C_{i}^{3},\n\\tag{14.26}\\]\nщо вимірює середню схильність системи до утворення трикутних кластерів.\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          indicators[0], \n          ylabel, \n          measure_labels[0],\n          xlabel,\n          file_names[0],\n          clr=\"magenta\")\n\n\n\n\n\n\n\nРис. 14.17: Динаміка індексу BTC та глобального коефіцієнта кластеризації\n\n\n\n\n\nНа Рис. 14.17 видно, що в абсолютних значеннях глобальний коефіцієнт тріадної кластеризації залишається на достатньо високому рівні, що говорить про досить високий ступінь кластеризації трейдерів криптовалютного ринку. Локально, в передкризові періоди, видно, що \\(\\langle C^{3} \\rangle\\) спадає, що говорить про локалізовану руйнацію кластеризованих групувань трейдерів і зростання їх тяжіння до одного або декількох гравців ринку.\n\n\n14.2.2.4.2 Транзитивність\nУ випадку дуже неоднорідних степенів, тобто безмасштабних мереж, де лише кілька вершин мають високу степінь, а інші — низьку (\\(d_i &lt; 2\\)), вершини з низькою степенню будуть брати участь переважно в обчисленні локального коефіцієнта кластеризації, що може призвести до недооцінки трикутних кластерів у мережі. Баррат і Вайгт  [34] запропонували альтернативний підхід для подолання такої проблеми, який отримав назву транзитивності  [35]:\n\\[\nT = \\sum_{k,j=1}^{N}A_{ik}A_{kj}A_{ji} \\Bigg/ \\sum_{i,k,j=1}^{N}A_{ik}A_{ji}.\n\\tag{14.27}\\]\nУ реальних мережах ми можемо зіткнутися з випадками, коли зв’язані сусіди в мережі можуть утворювати різні кліки (форми кластеризації). Класичний коефіцієнт локальної кластеризації, який вимірює імовірність знаходження трикутників, зазвичай відповідає одностороннім мережам. Однак він не може бути сформований у двосторонніх мережах  [36,37]. Складні структури односторонніх, двосторонніх і багатосторонніх мереж реальної системи можуть призвести до утворення кластерів набагато вищого порядку.\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          indicators[1], \n          ylabel, \n          measure_labels[1],\n          xlabel,\n          file_names[1],\n          clr=\"crimson\")\n\n\n\n\n\n\n\nРис. 14.18: Динаміка індексу BTC і транзитивності\n\n\n\n\n\nПоказник транзитивності працює подібно до \\(\\langle C^{3} \\rangle\\). Однак, на відміну від \\(\\langle C^{3} \\rangle\\), він надає куди більше сигналів про подальшу крахову поведінку на ринку Біткоїна. Видно, що на ринку зберігається досить висока частка трикутних кліків, які стають неповними в передкризові періоди, на що і вказує спадання \\(T\\).\n\n\n14.2.2.4.3 Коефіцієнт квадратичної кластеризації\nПодібно до \\(C_{i}^{3}\\), який є класичним коефіцієнтом локальної кластеризації, було запропоновано кількісно оцінити коефіцієнт кластеризації \\(C_{i}^{4}\\)  [38], який відповідає ймовірності знайти “квадратний” кластер, утворений сусідами вузла \\(i\\). Тобто, що два сусіди вузла \\(i\\) мають спільного сусіда, відмінного від \\(i\\). Для кожної вершини \\(i\\) вона може бути обчислена як\n\\[\nC_{i}^{4} = \\frac{\\sum_{k=1}^{d_i}\\sum_{j=k+1}^{d_i}q_i(k, j)}{\\sum_{k=1}^{d_i}\\sum_{j=k+1}^{d_i}[a_{i}(k, j) + q_i(k, j)]},\n\\tag{14.28}\\]\nде \\(q_i(k, j)\\) представляє кількість спостережуваних квадратних кластерів; \\(a_i(k, j) = \\left( d_k - (1+q_{i}(k, j) + \\theta_{ki}) \\right) + \\left( d_{j} - (1 + q_{i}(k, j) + \\theta_{kj}) \\right)\\); \\(\\theta_{kj}=1\\) якщо \\(k\\) і \\(j\\) є зв’язними і 0 у зворотньому випадку  [39]. Схожим чином до 14.26 ми можемо визначити глобальний коефіцієнт квадратичної кластеризації як\n\\[\n\\langle C^{4} \\rangle = \\frac{1}{N}\\sum_{i=1}^{N}C_{i}^{4}.\n\\tag{14.29}\\]\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          indicators[2], \n          ylabel, \n          measure_labels[2],\n          xlabel,\n          file_names[2],\n          clr=\"orange\")\n\n\n\n\n\n\n\nРис. 14.19: Динаміка індексу BTC і коефіцієнта квадратичної кластеризації\n\n\n\n\n\nРис. 14.19 демонструє, що глобально Біткоїн містить куди меншу частку квадратичних кластерів у порівнянні з тріадними. Локально ми спостерігаємо подібну до попередніх показників динаміку: \\(\\langle С_4 \\rangle\\) спадає в передкризовий період і поступово зростає в посткризовий. Можна зробити таке саме припущення, що й до цього: у передкризові періоди трейдери починають поступово ізолюватися від аналітики один одного і спрямовувати свою увагу на дії одного або декількох найбільш впливових груп. Хоча їх кластеризація спадає, але дії залишаються узгодженими згідно тієї інформації, що доходить до них із зовні.\n\n\n\n14.2.2.5 Зв’язність\nУ математиці зв’язний граф — це граф, у якого кількість ребер наближається до максимально можливої (коли кожна пара вершин з’єднана одним ребром). І навпаки, розріджений граф містить лише невелику кількість ребер. Точне визначення того, який граф вважати зв’язним або розрідженим, є неоднозначним. Отже, визначення щільності графа може змінюватись у залежності від контексту задачі.\n\nDensity = []\n\n\nfor i in tqdm(range(0,length-window,tstep)):\n    # відбираємо фрагмент\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n    \n    if graph_type == 'classic':\n        g = NaturalVG(directed=None).build(fragm)\n        pos = g.node_positions()\n        nxg = g.as_networkx()\n    if graph_type == 'horizontal':\n        g = HorizontalVG(directed=None).build(fragm)\n        pos = g.node_positions()\n        nxg = g.as_networkx()\n    \n    # розрахунок щільності\n    dens = nx.density(nxg)\n\n    Density.append(dens)\n\n 60%|█████▉    | 1914/3207 [00:06&lt;00:03, 387.58it/s]100%|██████████| 3207/3207 [00:10&lt;00:00, 320.53it/s]\n\n\n\nind_names = ['Density']\n\nindicators = [Density]\n\nmeasure_labels = [r'$\\rho$']\n\nfile_names = []\n\nfor i in range(len(ind_names)):\n    name = f\"{ind_names[i]}_symbol={symbol}_wind={window}_step={tstep}_seriestype={ret_type}_graph_type={graph_type}\"\n    np.savetxt(name + \".txt\", indicators[i])\n    file_names.append(name)\n\n\n14.2.2.5.1 Щільність\nЩільність графа може допомогти визначити, наскільки густо заселений різними ребрами представлений граф. Чим вона вища, тим більшою є зв’язність досліджуваного графа. Її можна обчислити як\n\\[\n\\rho = E \\big/ E_{max},\n\\tag{14.30}\\]\nде \\(E\\) дорівнює кількості ребер у \\(G\\), а \\(E_{max}=N(N-1)/2\\) — це максимальна кількість ребер у простому ненаправленому графі.\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          indicators[0], \n          ylabel, \n          measure_labels[0],\n          xlabel,\n          file_names[0],\n          clr=\"black\")\n\n\n\n\n\n\n\nРис. 14.20: Динаміка індексу BTC і показника щільності\n\n\n\n\n\nНа рисунку можна бачити, що глобальна зв’язність ринку залишається досить низькою (\\(\\rho&lt;0.10\\)), що говорить про недостатньо високий рівень зв’язності між теперішніми та минулими вузлами цінових коливань ринку Біткоїна. Віконна динаміка \\(\\rho\\) вказує на те, що в передкризовий момент часу ступінь щільності зв’язків учасників ринку зростає, що робить граф Біткоїна більш стійким.\n\n\n\n14.2.2.6 Міри відстані\nНа основі довжини найкоротшого шляху графа ми можемо отримати безліч інших показників його ефективності або віддаленості його вершин від центру зв’язності досліджуваного графа.\n\nDiameter = []\nRadius = []\n\n\nfor i in tqdm(range(0,length-window,tstep)):\n    # відбираємо фрагмент\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n    \n    if graph_type == 'classic':\n        g = NaturalVG(directed=None).build(fragm)\n        pos = g.node_positions()\n        nxg = g.as_networkx()\n    if graph_type == 'horizontal':\n        g = HorizontalVG(directed=None).build(fragm)\n        pos = g.node_positions()\n        nxg = g.as_networkx()\n    \n    # розрахунок діаметра\n    diameter = nx.diameter(nxg)\n    \n    # розрахунок радіуса\n    rad = nx.radius(nxg)\n    \n    Diameter.append(diameter)\n    Radius.append(rad)\n\n  2%|▏         | 49/3207 [00:05&lt;05:49,  9.05it/s]100%|██████████| 3207/3207 [06:00&lt;00:00,  8.89it/s]\n\n\n\nind_names = ['Diameter', 'Radius']\n\nindicators = [Diameter, Radius]\n\nmeasure_labels = [r'$diam$', r'rad']\n\nfile_names = []\n\nfor i in range(len(ind_names)):\n    name = f\"{ind_names[i]}_symbol={symbol}_wind={window}_step={tstep}_seriestype={ret_type}_graph_type={graph_type}\"\n    np.savetxt(name + \".txt\", indicators[i])\n    file_names.append(name)\n\n\n14.2.2.6.1 Діаметр\nЗауважимо, що найкоротший шлях, який є характеристикою відстані між досліджуваними вершинами \\(i\\) та \\(j\\), може бути використаний для характеристики загального розміру мережі. Величина, яка визначає найбільшу відстань між вершиною \\(i\\) та будь-якою іншою вершиною, називається ексцентриситетом:\n\\[\n\\varepsilon(i) = \\max_{j}l_{ij}.\n\\tag{14.31}\\]\nРозмір мережі можна охарактеризувати в термінах діаметру і визначити як\n\\[\ndiam = \\max_{i}\\varepsilon(i) = \\max_{i}\\max_{j}l_{ij}.\n\\tag{14.32}\\]\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          indicators[0], \n          ylabel, \n          measure_labels[0],\n          xlabel,\n          file_names[0],\n          clr=\"magenta\")\n\n\n\n\n\n\n\nРис. 14.21: Динаміка індексу BTC і діаметра мережі\n\n\n\n\n\nНа Рис. 14.21 видно, що діаметр графа спадає в передкризовий період, що говорить про зближення верхньої границі графа до його центру. Тобто, інформація, що проходитиме на криптовалютному ринку від одного трейдера до іншого, займатиме набагато менше кроків. Іншими словами, у передкризові періоди трейдери все менше покладаються на посередників із різноманітних новинних ресурсів і більше часу відводять на пряме опрацювання торгівельних закономірностей на ринку.\n\n\n14.2.2.6.2 Радіус\nТаким чином, діаметр — це найбільша (максимальна) довжина шляху в мережі. Отже, ми можемо визначити найменший ексцентриситет досліджуваної мережі, який називається радіусом:\n\\[\nrad = \\min_{i}\\varepsilon(i) = \\min_{i}\\max_{j}l_{ij}.\n\\tag{14.33}\\]\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          indicators[1], \n          ylabel, \n          measure_labels[1],\n          xlabel,\n          file_names[1],\n          clr=\"crimson\")\n\n\n\n\n\n\n\nРис. 14.22: Динаміка індексу BTC і радіуса мережі\n\n\n\n\n\nОскільки радіус графа це найменший ексцентриситет мережі, а діаметр є найбільшим, можна зробити подібний висновок. Якщо придивитися, то можна помітити, що радіус представляє приблизно в двічі меншу за діаметр версію, але тренд цих обох індикаторів ідентичний.\n\n\n\n14.2.2.7 Ефективність\nУ галузі мережної науки ефективність мережі в передачі інформації, яку також називають комунікаційною ефективністю, є ключовою метрикою. Це поняття ґрунтується на припущенні, що чим далі один від одного знаходяться два вузли в мережі, тим менш ефективною стає їхня комунікація. Ефективність можна аналізувати як на локальному, так і на глобальному рівнях мережі. На глобальному рівні оцінюється загальний обмін інформацією по всій мережі, де інформаційні потоки протікають паралельно. На локальному рівні вимірюється стійкість мережі до збоїв у менших масштабах. Зокрема, локальна ефективність вузла i відображає, наскільки ефективно його сусіди обмінюються інформацією за його відсутності.\n\nLocalEfficiency = []\nGlobalEfficiency = []\n\n\nfor i in tqdm(range(0,length-window,tstep)):\n    # відбираємо фрагмент\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n    \n    if graph_type == 'classic':\n        g = NaturalVG(directed=None).build(fragm)\n        pos = g.node_positions()\n        nxg = g.as_networkx()\n    if graph_type == 'horizontal':\n        g = HorizontalVG(directed=None).build(fragm)\n        pos = g.node_positions()\n        nxg = g.as_networkx()\n    \n    # розрахунок локальної ефективності\n    local_eff = nx.local_efficiency(nxg)\n\n    # розрахунок глобальної ефективності\n    glob_eff = nx.global_efficiency(nxg)\n    \n    LocalEfficiency.append(local_eff)\n    GlobalEfficiency.append(glob_eff)\n\n  1%|          | 18/3207 [00:05&lt;16:36,  3.20it/s]100%|██████████| 3207/3207 [39:56&lt;00:00,  1.34it/s]  \n\n\n\nind_names = ['LocalEfficiency', 'GlobalEfficiency']\n\nindicators = [LocalEfficiency, GlobalEfficiency]\n\nmeasure_labels = [r'$E_{loc}$', r'$E_{glob}$']\n\nfile_names = []\n\nfor i in range(len(ind_names)):\n    name = f\"{ind_names[i]}_symbol={symbol}_wind={window}_step={tstep}_seriestype={ret_type}_graph_type={graph_type}\"\n    np.savetxt(name + \".txt\", indicators[i])\n    file_names.append(name)\n\n\n14.2.2.7.1 Глобальна ефективність\nВизначення поведінки в малому світі згідно з  [29] можна подати в термінах ефективності \\(E\\) мережі. Ефективність \\(\\varepsilon_{ij}\\) між вершинами \\(i\\) та \\(j\\) визначається як \\(1/l_{ij}\\). Коли \\(l_{ij}=\\infty\\) і, послідовно, якщо \\(1/l_{ij}=0\\), \\(i\\) і \\(j\\) вважаються роз’єднаними. Відповідно до формалізму ефективності, вона може бути кількісно визначена як для глобальних, так і для локальних масштабів \\(G\\). Латора та Марчіорі підкреслювали, що \\(1/L\\) та \\(C\\) можна розглядати як перші наближення глобальної (\\(E_{glob}\\)) та локальної (\\(E_{loc}\\)) ефективності.\nСередню (глобальну) ефективність \\(G\\) можна визначити як\n\\[\nE_{glob} = \\sum_{i,j=1}\\left( l_{ij} \\right)^{-1} \\Big/ N(N-1).\n\\tag{14.34}\\]\nДля найбільш ефективного графа, де інформація поширюється найбільш ефективно, \\(E_{glob}\\) набуває максимального значення, а в іншому випадку — мінімального.\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          indicators[1], \n          ylabel, \n          measure_labels[1],\n          xlabel,\n          file_names[1],\n          clr=\"indigo\")\n\n\n\n\n\n\n\nРис. 14.23: Динаміка індексу BTC і глобальної ефективності мережі\n\n\n\n\n\nНа Рис. 14.23 видно, що ступінь глобальної ефективності мережі зростає в передкризові періоди, що вказує на зростання ступеня проходження інформації в мережі. З точки зору графа видимості, Біткоїн починає діяти в більш детермінований спосіб, де зв’язність його графа видимості стає близькою до топології ідеального графа, де вся інформація передається в найефективніший спосіб.\n\n\n14.2.2.7.2 Локальна ефективність\nЛокальна ефективність відіграє роль, подібну до глобального коефіцієнта кластеризації. Локальна ефективність \\(E_{loc}\\) може бути кількісно визначена як\n\\[\nE_{loc} = \\frac{1}{N}\\sum_{i \\in G_i}E_{glob}(G_i),\n\\tag{14.35}\\]\nде \\(G_i\\) — локальний підграф \\(G\\), а \\(E_{glob}(G_i)\\) характеризує ефективність цього конкретного підграфа. Подібно до глобального коефіцієнта кластеризації, \\(E_{loc}\\) визначає, наскільки відмовостійкою є досліджувана система, тобто наскільки ефективним є транспортування інформації між першими сусідами \\(i\\)-го вузла при його видаленні.\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          indicators[0], \n          ylabel, \n          measure_labels[0],\n          xlabel,\n          file_names[0],\n          clr=\"orange\")\n\n\n\n\n\n\n\nРис. 14.24: Динаміка індексу BTC і локальної ефективності мережі\n\n\n\n\n\nГлобально, \\(E_{loc}\\approx0.85\\) для ринку Біткоїна, що вказує на глобальну стійкість криптовалютної мережі до можливих атак і виключень трейдерів ринку з глобальної торгівлі. Віконна процедура показує, що \\(E_{loc}\\) спадає в передкризові періоди, що вказує на спад локальної ефективності мережі. Як уже зазначалося, оскільки увага більшості спрямовується на одного або декількох крупних гравців ринку, їх потенційне відключення з глобальної торгівлі могло б дестабілізувати весь криптовалютний ринок.\n\n\n\n14.2.2.8 Найкоротший шлях\n\nAvgPathLength = []\n\n\nfor i in tqdm(range(0,length-window,tstep)):\n    # відбираємо фрагмент\n    fragm = time_ser.iloc[i:i+window].copy()  \n\n    # виконуємо процедуру трансформації ряду \n    fragm = transformation(fragm, ret_type)\n    \n    if graph_type == 'classic':\n        g = NaturalVG(directed=None).build(fragm)\n        pos = g.node_positions()\n        nxg = g.as_networkx()\n    if graph_type == 'horizontal':\n        g = HorizontalVG(directed=None).build(fragm)\n        pos = g.node_positions()\n        nxg = g.as_networkx()\n    \n    # розрахунок середньої довжини найкоротшого шляху\n    avg_path_len = nx.average_shortest_path_length(nxg)\n   \n    AvgPathLength.append(avg_path_len)\n\n  4%|▍         | 136/3207 [00:07&lt;02:51, 17.94it/s]100%|██████████| 3207/3207 [03:10&lt;00:00, 16.80it/s]\n\n\n\nind_names = ['AvgPathLength']\n\nindicators = [AvgPathLength]\n\nmeasure_labels = [r'$ApLen$']\n\nfile_names = []\n\nfor i in range(len(ind_names)):\n    name = f\"{ind_names[i]}_symbol={symbol}_wind={window}_step={tstep}_seriestype={ret_type}_graph_type={graph_type}\"\n    np.savetxt(name + \".txt\", indicators[i])\n    file_names.append(name)\n\n\n14.2.2.8.1 Середня довжина найкоротшого шляху\nЗвертаючи увагу на довжину найкоротшого шляху між двома вершинами \\(i\\) та \\(j\\), ми можемо визначити таку міру, як середня довжина найкоротшого шляху:\n\\[\nApLen = \\sum_{i \\neq j}l_{ij} \\Big/ N(N-1).\n\\tag{14.36}\\]\n\nplot_pair(time_ser.index[window:length:tstep],\n          time_ser.values[window:length:tstep],\n          indicators[0], \n          ylabel, \n          measure_labels[0],\n          xlabel,\n          file_names[0],\n          clr=\"deeppink\")\n\n\n\n\n\n\n\nРис. 14.25: Динаміка індексу BTC і середньої довжини найкоротшого шляху\n\n\n\n\n\nНа Рис. 14.25 продемонстровано, що \\(ApLen\\) характеризується спадом у передкризові періоди та зростанням у кризові й посткризові періоди. Подібно до попередніх індикаторів, що тільки опиралися на довжини найкоротшого шляху між парами вершин, \\(ApLen\\) вказує на зростання ефективності передачі інформації між трейдерами ринку. Також можна сказати, що на побудованому криптовалютному графі видимості минулі значення “бачать” теперішні в більш ефективний спосіб, що відображається в персистентності ринку в передкризовий період.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Лабораторна робота № 14</span>"
    ]
  },
  {
    "objectID": "lab_14.html#висновок",
    "href": "lab_14.html#висновок",
    "title": "14  Лабораторна робота № 14",
    "section": "14.3 Висновок",
    "text": "14.3 Висновок\nУ даній роботі було продемонстровано можливість дослідження складних соціально-економічних систем в рамках мережної парадигми складності. Часовий ряд Біткоїна був представлений еквівалентним чином — мережею видимості, яка має широкий спектр характеристик: і спектральних, і топологічних. Приклади криптовалютних крахів показали, що більшість мережних показників можуть слугувати індикаторами-передвісниками кризових явищ і можуть бути використані для можливого раннього попередження небажаних криз на криптовалютних ринках.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Лабораторна робота № 14</span>"
    ]
  },
  {
    "objectID": "lab_14.html#завдання-для-самостійної-роботи",
    "href": "lab_14.html#завдання-для-самостійної-роботи",
    "title": "14  Лабораторна робота № 14",
    "section": "14.4 Завдання для самостійної роботи",
    "text": "14.4 Завдання для самостійної роботи\n\nДля заданих часових рядів чи їх сукупності побудувати всі види мереж, дослідити їх графодинаміку, порівняти результати і зробити висновки щодо їх прогностичних можливостей\nПроаналізувати результати як для вихідного ряду, так і для стандартизованих прибутковостей.\nЯк змінюються результати для не фінансових часових рядів. Чи спостерігається універсальність результатів?\n\n\n\n\n\n[1] A. O. Bielinskyi and V. N. Soloviev, Complex Network Precursors of Crashes and Critical Events in the Cryptocurrency Market, in Proceedings of St Student Workshop on Computer Science and Software Engineering, CS and SE@SW 2018, Kryvyi Rih, Ukraine, November 30, 2018, edited by S. S. O., S. A. M., S. V. N., and K. A. E., Vol. 2292 (CEUR-WS.org, 2028), pp. 37–45.\n\n\n[2] A. O. Bielinskyi, V. N. Soloviev, S. V. Hushko, A. E. Kiv, and A. V. Matviychuk, High-Order Network Analysis for Financial Crash Identification, in Proceedings of the Selected and Revised Papers of 10th International Conference on Monitoring, Modeling & Management of Emergent Economy (M3E2-MLPEED 2022), Virtual Event, Kryvyi Rih, Ukraine, November 17-18, 2022, edited by H. B. Danylchuk and S. O. Semerikov, Vol. 3465 (CEUR-WS.org, 2022), pp. 132–149.\n\n\n[3] A. Kiv, A. Bryukhanov, V. Soloviev, A. Bielinskyi, T. Kavetskyy, D. Dyachok, I. Donchev, and V. Lukashin, Complex Network Methods for Plastic Deformation Dynamics in Metals, Dynamics 3, 34 (2023).\n\n\n[4] R. V. Donner, M. Small, J. F. Donges, N. Marwan, Y. Zou, R. Xiang, and J. Kurths, Recurrence-Based Time Series Analysis by Means of Complex Network Methods, International Journal of Bifurcation and Chaos 21, 1019 (2011).\n\n\n[5] L. Lacasa, B. Luque, F. Ballesteros, J. Luque, and J. C. Nuño, From Time Series to Complex Networks: The Visibility Graph, Proceedings of the National Academy of Sciences 105, 4972 (2008).\n\n\n[6] A. O. Bielinskyi, O. A. Serdyuk, S. O. Semerikov, and V. N. Soloviev, Econophysics of Cryptocurrency Crashes: A Systematic Review, in Proceedings of the Selected and Revised Papers of 9th International Conference on Monitoring, Modeling & Management of Emergent Economy (M3E2-MLPEED 2021), Odessa, Ukraine, May 26-28, 2021, edited by A. E. Kiv, V. N. Soloviev, and S. O. Semerikov, Vol. 3048 (CEUR-WS.org, 2021), pp. 31–133.\n\n\n[7] X. Lan, H. Mo, S. Chen, Q. Liu, and Y. Deng, Fast transformation from time series to visibility graphs, Chaos: An Interdisciplinary Journal of Nonlinear Science 25, 083105 (2015).\n\n\n[8] B. Luque, L. Lacasa, F. Ballesteros, and J. Luque, Horizontal Visibility Graphs: Exact Results for Random Time Series, Phys. Rev. E 80, 046103 (2009).\n\n\n[9] I. V. Bezsudnov and A. A. Snarskii, From the Time Series to the Complex Networks: The Parametric Natural Visibility Graph, Physica A: Statistical Mechanics and Its Applications 414, 53 (2014).\n\n\n[10] T. T. Zhou, N. D. Jin, Z. K. Gao, and Y. B. Luo, Limited Penetrable Visibility Graph for Establishing Complex Network from Time Series, Acta Physica Sinica 61, 2012-3-030506 (2012).\n\n\n[11] Q. Xuan, J. Zhou, K. Qiu, D. Xu, S. Zheng, and X. Yang, CLPVG: Circular limited penetrable visibility graph as a new network model for time series, Chaos: An Interdisciplinary Journal of Nonlinear Science 32, 013130 (2022).\n\n\n[12] F. R. K. Chung, Spectral Graph Theory (American Mathematical Society, 1997).\n\n\n[13] N. Biggs, Spectral Graph Theory (CBMS Regional Conference Series in Mathematics 92), Bulletin of the London Mathematical Society 30, 197 (1998).\n\n\n[14] S. Butler, Interlacing for Weighted Graphs Using the Normalized Laplacian, Electronic Journal of Linear Algebra 16, 90 (2007).\n\n\n[15] G. Bounova and O. de Weck, Overview of Metrics and Their Correlation Patterns for Multiple-Metric Topology Analysis on Heterogeneous Graph Ensembles, Phys. Rev. E 85, 016117 (2012).\n\n\n[16] I. Gutman, The Energy of a Graph, (1978).\n\n\n[17] D. M. Cvetkovic, M. Doob, and H. Sachs, Spectra of Graphs. Theory and Application, (1980).\n\n\n[18] J. Wu, M. Barahona, Y.-J. Tan, and H.-Z. Deng, Spectral Measure of Structural Robustness in Complex Networks, IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 41, 1244 (2011).\n\n\n[19] W. Jun, M. Barahona, T. Yue-Jin, and D. Hong-Zhong, Natural Connectivity of Complex Networks, Chinese Physics Letters 27, 078902 (2010).\n\n\n[20] E. Estrada, Spectral Scaling and Good Expansion Properties in Complex Networks, Europhysics Letters 73, 649 (2006).\n\n\n[21] A. Berman and R. J. Plemmons, Nonnegative Matrices in the Mathematical Sciences (Society for Industrial; Applied Mathematics, 1994).\n\n\n[22] I. J. Schoenberg, Publications of Edmund Landau, in Number Theory and Analysis: A Collection of Papers in Honor of Edmund Landau (1877–1938), edited by P. Turán (Springer US, Boston, MA, 1969), pp. 335–355.\n\n\n[23] T. H. Wei, The Algebraic Foundations of Ranking Theory (University of Cambridge, 1952).\n\n\n[24] M. G. Kendall, Further Contributions to the Theory of Paired Comparisons, Biometrics 11, 43 (1955).\n\n\n[25] C. Berge, Théorie Des Graphes Et Ses Applications (Dunod, 1958).\n\n\n[26] P. Bonacich, Technique for Analyzing Overlapping Memberships, Sociological Methodology 4, 176 (1972).\n\n\n[27] Wikipedia, Arnoldi Iteration.\n\n\n[28] K. Stephenson and M. Zelen, Rethinking Centrality: Methods and Examples, Social Networks 11, 1 (1989).\n\n\n[29] V. Latora and M. Marchiori, Efficient Behavior of Small-World Networks, Phys. Rev. Lett. 87, 198701 (2001).\n\n\n[30] R. Pastor-Satorras, A. Vázquez, and A. Vespignani, Dynamical and Correlation Properties of the Internet, Phys. Rev. Lett. 87, 258701 (2001).\n\n\n[31] S. Maslov and K. Sneppen, Specificity and Stability in Topology of Protein Networks, Science 296, 910 (2002).\n\n\n[32] M. E. J. Newman, Assortative Mixing in Networks, Phys. Rev. Lett. 89, 208701 (2002).\n\n\n[33] D. J. Watts and S. H. Strogatz, Collective Dynamics of ’Small-World’ Networks, Nature 393, 440 (1998).\n\n\n[34] A. Barrat and M. Weigt, On the Properties of Small-World Network Models, The European Physical Journal B-Condensed Matter and Complex Systems 13, 547 (2000).\n\n\n[35] S. Boccaletti, V. Latora, Y. Moreno, M. Chavez, and D.-U. Hwang, Complex Networks: Structure and Dynamics, Physics Reports 424, 175 (2006).\n\n\n[36] P. Holme, C. R. Edling, and F. Liljeros, Structure and Time Evolution of an Internet Dating Community, Social Networks 26, 155 (2004).\n\n\n[37] P. Holme, F. Liljeros, C. R. Edling, and B. J. Kim, Network Bipartivity, Phys. Rev. E 68, 056107 (2003).\n\n\n[38] P. G. Lind, M. C. González, and H. J. Herrmann, Cycles and Clustering in Bipartite Networks, Phys. Rev. E 72, 056127 (2005).\n\n\n[39] P. Zhang, J. Wang, X. Li, M. Li, Z. Di, and Y. Fan, Clustering Coefficient and Community Structure of Bipartite Networks, Physica A: Statistical Mechanics and Its Applications 387, 6869 (2008).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Лабораторна робота № 14</span>"
    ]
  },
  {
    "objectID": "appa.html",
    "href": "appa.html",
    "title": "Додаток A — Інструкція зі встановлення Anaconda Navigator",
    "section": "",
    "text": "Відвідайте сторінку Anaconda за наступним посиланням. Ви маєте побачити наступне зображення:\n\n\n\n\n\n\n\nНатискаємо на кнопку Download:\n\n\n\n\n\n\n\nЯкщо ви набиратимете anaconda у пошуковому рядку, тоді потрібно перейти за наступним посиланням, що має з’явитися найпершим:\n\n\n\n\n\n\nПерейшовши по виділеному посиланню, маєте побачити наступну сторінку:\n\n\n\n\n\nНатиснемо на кнопку Download, щоб розпочати встановлення.\n\nЗ’явиться вікно наступного виду і запропонує зберегти файл там, де це потрібно:\n\n\n\n\n\n\n\nНатискаємо на клавішу Next:\n\n\n\n\n\n\n\nУважно читаємо ліцензійну угоду та натискаємо клавішу I Agree:\n\n\n\n\n\n\n\nДалі, якщо ви не перебуваєте у команді розробників, і ви єдина людина, хто буде користуватися Anaconda — Just me ваш вибір:\n\n\n\n\n\n\nНатискаємо Next.\n\nНа наступному кроці пропонується обрати папку, до якої буде встановлено дистрибутив Anaconda. Бажано, щоб шлях до папки не містив кирилиці. Можна залишити шлях за замовчуванням. Ми натиснемо на Browse. Далі, обираємо зручний для нас диск та натискаємо Создать папку. Таким чином ми створимо в нашому диску папку з назвою anaconda до якої і буде завантажено Anaconda.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nНаступним кроком буде обрання середовища змінних (the environment variables).\nЯкщо ви встановлюєте Python вперше, відмітимо Add Anaconda to my PATH environment variable. Це дасть вам можливість використовувати Anaconda в командному рядку (або Git bash, cmder, powershell і т.д.).\nЯкщо ви вже маєте Python на своєму комп’ютері, тоді прапорець не варто відмічати. Ви зможете запускати Anaconda Navigator або Anaconda Command Prompt (розташовані в меню Пуск у розділі Anaconda), якщо потрібно буде запускати Anaconda (ви завжди матимете можливість додати Anaconda до свого шляху пізніше, якщо не встановите цей прапорець).\nНа представленій локальній машині нам не треба відмічати прапорець.\n\n\n\n\n\n\n\n\n\n\nРекомендований підхід\n\n\n\n\n\n\n\nАльтернативний\n\n\n\n\n\n\nНатискаємо Install і чекаємо завершення:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nНатискаємо Next аж до вікна з подякою за встановлення Anaconda і після натискаємо Finish:\n\n\n\n\n\n\n\nНа наступному кроці потрібно перейти в меню Пуск і знайти папку під назвою Anaconda:\n\n\n\n\n\n\n\nРозгорнувши папку можна побачити цікаві для нас іконки:\n\n\n\n\n\n\nНа зазначеній вище фотографії можна бачити декілька ярликів Anaconda Navigator, Anaconda Powershell Promt, Anaconda Promt та Jupyter Notebook з різними найменуваннями anaconda та anaconda3. Це тому, що на представленій локальній машині попередньо було встановлено Anaconda, але на іншому локальному диску. Якщо ви встановлюєте Anaconda вперше чи перевстановлюєте, тоді ви матимете у 2 рази менше ярликів.\n\nНатиснувши на Anaconda Navigator, потрапляємо до середовища, що пропонує різноманітні інструменти для аналізу даних. Для подальшої роботи нам знадобиться лише Jupyter Notebook:\n\n\n\n\n\n\nНатискаємо на Launch та потрапляємо до середовища Jupyter (кореневої папки до якої було встановлено anaconda):\n\n\n\n\n\nУ верхньому лівому кутку буде знаходитись значок New. Спочатку натискаємо на нього і потім на Python 3. Має створитися відповідний Notebook у якому можна писати код:\n\n\nОкрім цього, в меню пуск можна натиснути на значок Jupyter Notebook та одразу перейти до роботи:\n\n\n\n\n\n\n\nНатиснувши на Anaconda Powershell Promt або Anaconda Promt можна перейти до командного рядка, представленого дистрибутивом Anaconda. З його допомогою можна докачати необхідні модулі або запустити необхідний інструмент, що представляє Anaconda. Запустимо Jupyter Notebook за допомогою команди jupyter notebook:",
    "crumbs": [
      "Додатки",
      "<span class='chapter-number'>A</span>  <span class='chapter-title'>Інструкція зі встановлення Anaconda Navigator</span>"
    ]
  },
  {
    "objectID": "appb.html",
    "href": "appb.html",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "",
    "text": "B.1 Коментарі коду\nКоментар — це примітка, зроблена програмістом у вихідному коді програми. Його мета — прояснити вихідний код і полегшити відстеження того, що відбувається. Все, що міститься в коментарі, зазвичай ігнорується при фактичному запуску коду, але робить коментарі корисними для включення пояснень і міркувань, а також для видалення певних рядків коду, в яких ви можете бути не впевнені. Коментарі в Python створюються за допомогою символу решітки (# вставити текст тут). Включення # у рядок коду коментує все, що слідує за ним.\n# print(\"Привіт усім\")\n# Це коментар\n# Ці рядки коду не змінять жодних значень\n# все, що слідує за першим #, не виконується як код\nВи можете побачити текст, укладений у потрійні лапки (\"\"\" вставте текст тут \"\"\"). Такий синтаксис представлятиме багаторядкове коментування, але це не зовсім точно. Це особливий тип string, що називаєтьсяdocstring і використовується для пояснення призначення функції.\n\"\"\" This is a special string \"\"\"\n\n' This is a special string '",
    "crumbs": [
      "Додатки",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Вступ до мови програмування Python</span>"
    ]
  },
  {
    "objectID": "appb.html#змінна",
    "href": "appb.html#змінна",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "B.2 Змінна",
    "text": "B.2 Змінна\nЗмінні надають імена для значень. Якщо ви хочете зберегти значення для подальшого або повторного використання, ви присвоюєте значенню ім’я, зберігаючи вміст у змінній. Змінні в програмуванні працюють аналогічно змінним в алгебрі, але в Python вони можуть приймати різні типи даних.\nОсновними типами змінних, які ми розглянемо в цьому розділі, є цілі числа, числа з плаваючою комою, логічні значення та рядки.\nЦіле число у програмуванні — це те саме, що і в математиці, число без значень після десяткової коми. Ми використовуємо вбудовану функцію print() тут для відображення значень наших змінних, а також їх типів!\n\nmy_integer = 50\nprint(my_integer, type(my_integer))\n\n50 &lt;class 'int'&gt;\n\n\nЗмінні, незалежно від типу, призначаються за допомогою знака рівності (=). Змінні чутливі до регістру, тому будь-які зміни в заголовних літерах імені змінної будуть посилатися на іншу змінну.\n\none = 1\nprint(one)\n\n1\n\n\nЧисло з плаваючою комою або float — це назва дійсного числа (знову ж таки, як у математиці). Щоб визначити float, нам потрібно або включити десяткову крапку, або вказати, що значення є float.\n\nmy_float = 1.0\nprint(my_float, type(my_float))\nmy_float = float(1)\nprint(my_float, type(my_float))\n\n1.0 &lt;class 'float'&gt;\n1.0 &lt;class 'float'&gt;\n\n\nЗмінна типу float не округлятиме число, яке ви в ній зберігаєте, тоді як змінна типу integer округлятиме. Це робить floats більш придатними для математичних обчислень, де потрібно більше, ніж просто цілі числа.\nЗверніть увагу, що оскільки ми використовували функцію float(), щоб змусити число рахуватися float, ми можемо використовувати функцію int(), щоб змусити число представлятися в типі int.\n\nmy_int = int(3.14159)\nprint(my_int, type(my_int))\n\n3 &lt;class 'int'&gt;\n\n\nФункція int() також усіче будь-які цифри, які число може містити після десяткової коми!\nРядки дозволяють включати текст як змінну для роботи. Вони визначаються з використанням або одинарних лапок (’’), або подвійних лапок (““).\n\nmy_string = 'This is a string with single quotes'\nprint(my_string)\nmy_string = \"This is a string with double quotes\"\nprint(my_string)\n\nThis is a string with single quotes\nThis is a string with double quotes\n\n\nОбидва варіанти дозволені, так що ми можемо включити апострофи або лапки в рядок, якщо ми того побажаємо.\n\nmy_string = '\"Jabberwocky\", by Lewis Carroll'\nprint(my_string)\nmy_string = \"'Twas brillig, and the slithy toves / Did gyre and gimble in the wabe;\"\nprint(my_string)\n\n\"Jabberwocky\", by Lewis Carroll\n'Twas brillig, and the slithy toves / Did gyre and gimble in the wabe;\n\n\nЛогічні значення, або bools, - це двійкові типи змінних. bool може приймати лише одне з двох значень, це True або False. У цій ідеї істинних значень є набагато більше, коли мова заходить про програмування, про що ми розповімо пізніше в розділі Логічні оператори цього зошита.\n\nmy_bool = True\nprint(my_bool, type(my_bool))\n\nTrue &lt;class 'bool'&gt;\n\n\nІснує ще багато типів даних, які ви можете призначити змінними в Python, але це основні з них!",
    "crumbs": [
      "Додатки",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Вступ до мови програмування Python</span>"
    ]
  },
  {
    "objectID": "appb.html#базова-математика",
    "href": "appb.html#базова-математика",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "B.3 Базова математика",
    "text": "B.3 Базова математика\nPython має ряд вбудованих математичних функцій. Їх можна ще більше розширити, імпортуючи пакет math або включивши будь-яку кількість інших обчислювальних пакетів.\nПідтримуються всі основні арифметичні операції: +, -, /, і *. Ви можете створювати експоненти за допомогою **, а модульна арифметика вводиться за допомогою оператора mod, %.\n\nprint('Addition: ', 2 + 2)\nprint('Subtraction: ', 7 - 4)\nprint('Multiplication: ', 2 * 5)\nprint('Division: ', 10 / 2)\nprint('Exponentiation: ', 3**2)\n\nAddition:  4\nSubtraction:  3\nMultiplication:  10\nDivision:  5.0\nExponentiation:  9\n\n\nЯкщо ви не знайомі з оператором mod, він працює як функція залишку. Якщо ми введемо \\(15 \\ \\% \\  4\\), він поверне залишок після ділення \\(15\\) на \\(4\\).\n\nprint('Частка: ', 15 % 4)\n\nЧастка:  3\n\n\nМатематичні функції також працюють зі змінними!\n\nfirst_integer = 4\nsecond_integer = 5\nprint(first_integer * second_integer)\n\n20\n\n\nЯкщо ви виконуєте математику виключно з цілими числами, ви отримуєте ціле число. Включення будь-якого значення з плаваючою точкою в обчислення зробить уже результат із плаваючою точкою.\n\nfirst_integer = 11\nsecond_integer = 3\nprint(first_integer / second_integer)\n\n3.6666666666666665\n\n\n\nfirst_number = 11.0\nsecond_number = 3.0\nprint(first_number / second_number)\n\n3.6666666666666665\n\n\nPython має кілька вбудованих математичних функцій. Найбільш помітними з них є:\n\nabs()\nround()\nmax()\nmin()\nsum()\n\nУсі ці функції діють так, як ви очікували, враховуючи їх назви. Виклик abs() для числа поверне його абсолютне значення. Функція round() округлить число до вказаної кількості десяткових знаків (значення за замовчуванням дорівнює \\(0\\)). Виклик max() або min() для набору чисел поверне, відповідно, максимальне або мінімальне значення в наборі. Виклик sum() для набору чисел призведе до їх підсумовування. Якщо ви не знайомі з тим, як працюють колекції значень у Python, не хвилюйтеся! Ми детально розглянемо набір в наступному розділі.\nДодаткові математичні функції можуть бути додані разом з пакетом math.\n\nimport math\n\nМатематична бібліотека додає довгий список нових математичних функцій до Python. Не соромтеся ознайомитися з документацією для отримання повного списку та деталей. У ній ви знайдете деякі математичні константи.\n\nprint('Pi: ', math.pi)\nprint(\"Euler's Constant: \", math.e)\n\nPi:  3.141592653589793\nEuler's Constant:  2.718281828459045\n\n\nА також деякі часто використовувані математичні функції\n\nprint('Cosine of pi: ', math.cos(math.pi))\n\nCosine of pi:  -1.0",
    "crumbs": [
      "Додатки",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Вступ до мови програмування Python</span>"
    ]
  },
  {
    "objectID": "appb.html#колекції",
    "href": "appb.html#колекції",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "B.4 Колекції",
    "text": "B.4 Колекції\n\nB.4.1 Списки (lists)\nСписок у Python - це впорядкована колекція об’єктів, яка може містити будь-який тип даних. Ми визначаємо список, використовуючи квадратні дужки ([]).\n\nmy_list = [1, 2, 3]\nprint(my_list)\n\n[1, 2, 3]\n\n\nМи також можемо отримати доступ до списку та проіндексувати його за допомогою дужок. Щоб вибрати окремий елемент, просто введіть назву списку, а потім індекс елемента, який ви шукаєте, у фігурних дужках.\n\nprint(my_list[0])\nprint(my_list[2])\n\n1\n3\n\n\nІндексація в Python починається з $ 0$. Якщо у вас є список довжиною \\(n\\), перший елемент списку знаходиться з індексом \\(0\\), другий елемент з індексом \\(1\\), і так далі, і тому подібне. Останній елемент списку матиме індекс \\(n-1\\). Будьте обережні! Спроба отримати доступ до неіснуючого індексу призведе до помилки.\n\nprint('The first, second, and third list elements: ', my_list[0], my_list[1], my_list[2])\nprint('Accessing outside the list bounds causes an error: ', my_list[3])\n\nThe first, second, and third list elements:  1 2 3\n\n\nIndexError: list index out of range\n\n\nМи можемо побачити кількість елементів у списку, викликавши функцію len().\n\nprint(len(my_list))\n\n3\n\n\nМи можемо оновлювати та змінювати список, отримуючи доступ до індексу та призначаючи нове значення.\n\nprint(my_list)\nmy_list[0] = 42\nprint(my_list)\n\n[1, 2, 3]\n[42, 2, 3]\n\n\nЦе принципово відрізняється від того, як обробляються рядки. Список є змінним, ви можете змінювати елементи списку без зміни самого списку. Деякі типи даних, такі як рядки, є незмінними. Як тільки рядок або інший незмінний тип даних був створений, він не може бути безпосередньо змінений без створення абсолютно нового об’єкта.\n\nmy_string = \"Strings never change\"\nmy_string[0] = 'Z'\n\nTypeError: 'str' object does not support item assignment\n\n\nЯк ми вже говорили раніше, список може містити будь-який тип даних. Таким чином, списки також можуть містити рядки.\n\nmy_list_2 = ['one', 'two', 'three']\nprint(my_list_2)\n\n['one', 'two', 'three']\n\n\nСписки також можуть містити кілька різних типів даних одночасно!\n\nmy_list_3 = [True, 'False', 42]\n\nЯкщо ви хочете об’єднати два списки, їх можна об’єднати символом +.\n\nmy_list_4 = my_list + my_list_2 + my_list_3\nprint(my_list_4)\n\n[42, 2, 3, 'one', 'two', 'three', True, 'False', 42]\n\n\nОкрім доступу до окремих елементів списку ми можемо отримати доступ до груп елементів за допомогою зрізу.\n\nmy_list = ['friends', 'romans', 'countrymen', 'lend', 'me', 'your', 'ears']\n\n\nB.4.1.1 Зріз (slicing)\nМи використовуємо двокрапку (:) для нарізки списків.\n\nprint(my_list[2:4])\n\n['countrymen', 'lend']\n\n\nВикористовуючи :, ми можемо вибрати групу елементів у списку, починаючи з першого вказаного елемента і закінчуючи (але не включаючи) останнім зазначеним елементом.\nМи також можемо вибрати все після певного значення\n\nprint(my_list[1:])\n\n['romans', 'countrymen', 'lend', 'me', 'your', 'ears']\n\n\nІ все перед конкретним значенням\n\nprint(my_list[:4])\n\n['friends', 'romans', 'countrymen', 'lend']\n\n\nВикористання негативних чисел буде відлічуватися з кінця індексів, а не з початку. Наприклад, індекс -1 вказує на останній елемент списку.\n\nprint(my_list[-1])\n\nears\n\n\nВи також можете додати третій компонент для нарізки. Замість того, щоб просто вказати першу та кінцеву частини вашого зрізу, ви можете вказати розмір кроку, який ви хочете зробити. Таким чином, замість того, щоб брати кожен окремий елемент, ви можете взяти будь-який інший елемент.\n\nprint(my_list[0:7:2])\n\n['friends', 'countrymen', 'me', 'ears']\n\n\nТут ми вибрали весь список (оскільки 0:7 дасть елементи від 0 до 6), і ми вибрали розмір кроку 2. Отже, це виведе елемент 0, елемент 2, елемент 4 тощо на вибраний елемент списку. Ми можемо пропустити вказаний початок і кінець нашого фрагмента, вказавши лише крок.\n\nprint(my_list[::2])\n\n['friends', 'countrymen', 'me', 'ears']\n\n\nСписки неявно вибирають початок і кінець списку, якщо не вказано інше.\n\nprint(my_list[:])\n\n['friends', 'romans', 'countrymen', 'lend', 'me', 'your', 'ears']\n\n\nПри негативному розмірі кроку ми можемо навіть перевернути список!\n\nprint(my_list[::-1])\n\n['ears', 'your', 'me', 'lend', 'countrymen', 'romans', 'friends']\n\n\nPython не має власних матриць. Інші пакети, такі як numpy, додають матриці як окремий тип даних, але в базовому Python найкращим способом створення матриці є використання списку списків.\nМи також можемо використовувати вбудовані функції для створення списків. Зокрема, ми розглянемо range() (тому що ми будемо використовувати його пізніше!). Діапазон може приймати кілька різних вхідних даних і поверне список.\n\nb = 10\nmy_list = range(b)\nprint(my_list)\n\nrange(0, 10)\n\n\nПодібно до наших попередніх методів нарізки списків, ми можемо визначити як початок, так і кінець нашого діапазону. Це поверне список, який включає початок і виключає кінець, точно так само, як зріз.\n\na = 0\nb = 10\nmy_list = range(a, b)\nprint(my_list)\n\nrange(0, 10)\n\n\nМи також можемо вказати розмір кроку. Це знову має таку ж поведінку, як і зріз.\n\na = 0\nb = 10\nstep = 2\nmy_list = range(a, b, step)\nprint(my_list)\n\nrange(0, 10, 2)\n\n\n\n\n\nB.4.2 Кортежі (Tuples)\nКортеж - це тип даних, подібний до списку в тому сенсі, що він може містити різні типи даних. Ключова відмінність тут полягає в тому, що кортеж є незмінним. Ми визначаємо кортеж, розділяючи елементи, які ми хочемо включити комами. Зазвичай кортеж укладають в круглі дужки.\n\nmy_tuple = 'I', 'have', 30, 'cats'\nprint(my_tuple)\n\n('I', 'have', 30, 'cats')\n\n\n\nmy_tuple = ('I', 'have', 30, 'cats')\nprint(my_tuple)\n\n('I', 'have', 30, 'cats')\n\n\nЯк згадувалося раніше, кортежі незмінні. Ви не можете змінити будь-яку їх частину, не визначивши новий кортеж.\n\nmy_tuple[3] = 'dogs' # Намагається змінити значення 'cats', що зберігається в кортежі, на 'dogs'\n\nTypeError: 'tuple' object does not support item assignment\n\n\nВи можете нарізати кортежі так само, як ви нарізаєте списки!\n\nprint(my_tuple[1:3])\n\n('have', 30)\n\n\nІ об’єднайте їх так, як ви б це зробили з рядками!\n\nmy_other_tuple = ('make', 'that', 50)\nprint(my_tuple + my_other_tuple)\n\n('I', 'have', 30, 'cats', 'make', 'that', 50)\n\n\nМи можемо упакувати значення разом, створивши кортеж (як зазначено вище), або ми можемо розпакувати значення з кортежу, витягуючи їх.\n\nstr_1, str_2, int_1 = my_other_tuple\nprint(str_1, str_2, int_1)\n\nmake that 50\n\n\nРозпакування присвоює кожне значення кортежу по порядку кожній змінній у лівій частині знака рівності. Деякі функції, включаючи спеціальні функції, можуть повертати кортежі, тому ми можемо використовувати це, щоб безпосередньо розпакувати їх і отримати доступ до потрібних нам значень.\n\n\nB.4.3 Множини (Sets)\nМножини - це набір невпорядкованих, унікальних елементів. Він працює майже точно так, як ви очікували б від звичайного набору математичних задач, і визначається за допомогою фігурних дужок ({}).\n\nthings_i_like = {'dogs', 7, 'the number 4', 4, 4, 4, 42, 'lizards', 'man I just LOVE the number 4'}\nprint(things_i_like, type(things_i_like))\n\n{'lizards', 'dogs', 'the number 4', 4, 'man I just LOVE the number 4', 7, 42} &lt;class 'set'&gt;\n\n\nЗверніть увагу, як будь-які додаткові екземпляри одного і того ж елемента видаляються в остаточному наборі. Ми також можемо створити множину зі списку, використовуючи функцію set().\n\nanimal_list = ['cats', 'dogs', 'dogs', 'dogs', 'lizards', 'sponges', 'cows', 'bats', 'sponges']\nanimal_set = set(animal_list)\nprint(animal_set) # видаляємо всі дублікати зі списку\n\n{'dogs', 'bats', 'lizards', 'cats', 'sponges', 'cows'}\n\n\nВиклик len() для множини повідомить вам, скільки в ньому елементів.\n\nprint(len(animal_set))\n\n6\n\n\nОскільки множина представляє невпорядковану структуру даних, ми не можемо отримати доступ до окремих елементів за допомогою індексу. Однак ми можемо легко перевірити приналежність (щоб побачити, чи міститься щось у наборі) та використовувати об’єднання та перетини множин за допомогою вбудованих функцій set.\n\n'cats' in animal_set # Тут ми перевіряємо наявність членства, використовуючи ключове слово 'in'.\n\nTrue\n\n\nТут ми перевірили, чи міститься рядок cats у нашому animal_set, і він повернув True, повідомивши нам, що він насправді знаходиться в нашому наборі.\nМи можемо з’єднати множини, використовуючи типові математичні оператори множин, а саме | для об’єднання та & для перетину. Використання | або & поверне саме те, що ви очікували б, якщо ви знайомі з множинами в математиці.\n\nprint(animal_set | things_i_like) # Ви також можете написати things_i_like / animal_set без будь-якої різниці\n\n{'dogs', 'the number 4', 4, 'bats', 'man I just LOVE the number 4', 7, 42, 'lizards', 'cats', 'sponges', 'cows'}\n\n\nСполучення двох наборів за допомогою | об’єднує множини, видаляючи будь-які повторення, щоб зробити кожен елемент набору унікальним.\n\nprint(animal_set & things_i_like) # Ви також можете написати things_i_like & animal_set без будь-якої різниці\n\n{'lizards', 'dogs'}\n\n\nСполучення двох наборів за допомогою & обчислює перетин обох наборів, повертаючи набір, який містить лише те, що вони мають спільне.\nЯкщо вам цікаво дізнатися більше про вбудовані функції для наборів, не соромтеся ознайомитися з документацією.\n\n\nB.4.4 Словники (Dictionaries)\nЩе однією важливою структурою даних у Python є словник. Словники визначаються за допомогою комбінації фігурних дужок ({}) і двокрапок (:). Фігурні дужки визначають початок і кінець словника, а двокрапки вказують пари ключ-значення. Словник - це, по суті, набір пар ключ-значення. Ключ будь-якого запису повинен бути незмінним типом даних. Це робить кандидатами як рядки, так і кортежі. Ключі можуть бути як додані, так і видалені.\nУ наступному прикладі ми маємо словник, що складається з пар ключ-значення, де ключовим є жанр художньої літератури (рядок), а значенням є список книг (list) у цьому жанрі. Оскільки колекція все ще вважається єдиною сутністю, ми можемо використовувати її для збору декількох змінних або значень в одну пару ключ-значення.\n\nmy_dict = {\"High Fantasy\": [\"Wheel of Time\", \"Lord of the Rings\"],\n           \"Sci-fi\": [\"Book of the New Sun\", \"Neuromancer\", \"Snow Crash\"],\n           \"Weird Fiction\": [\"At the Mountains of Madness\", \"The House on the Borderland\"]}\n\nПісля визначення словника ми можемо отримати доступ до будь-якого окремого значення, вказавши його ключ у дужках.\n\nprint(my_dict[\"Sci-fi\"])\n\n['Book of the New Sun', 'Neuromancer', 'Snow Crash']\n\n\nМи також можемо змінити значення, пов’язане з даним ключем\n\nmy_dict[\"Sci-fi\"] = \"I can't read\"\nprint(my_dict)\n\n{'High Fantasy': ['Wheel of Time', 'Lord of the Rings'], 'Sci-fi': \"I can't read\", 'Weird Fiction': ['At the Mountains of Madness', 'The House on the Borderland']}\n\n\nДодати нову пару ключ-значення так само просто, як і визначити її.\n\nmy_dict[\"Historical Fiction\"] = [\"Pillars of the Earth\"]\nprint(my_dict[\"Historical Fiction\"])\n\n['Pillars of the Earth']\n\n\n\nprint(my_dict)\n\n{'High Fantasy': ['Wheel of Time', 'Lord of the Rings'], 'Sci-fi': \"I can't read\", 'Weird Fiction': ['At the Mountains of Madness', 'The House on the Borderland'], 'Historical Fiction': ['Pillars of the Earth']}",
    "crumbs": [
      "Додатки",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Вступ до мови програмування Python</span>"
    ]
  },
  {
    "objectID": "appb.html#рядки-strings",
    "href": "appb.html#рядки-strings",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "B.5 Рядки (Strings)",
    "text": "B.5 Рядки (Strings)\nМи вже знаємо, що рядки зазвичай використовуються для тексту. Ми можемо використовувати вбудовані операції для легкого об’єднання, розділення та форматування рядків, залежно від наших потреб.\nСимвол + вказує на конкатенацію мовою рядків. Це об’єднає два рядки в довший рядок.\n\nfirst_string = '\"Beware the Jabberwock, my son! /The jaws that bite, the claws that catch! /'\nsecond_string = 'Beware the Jubjub bird, and shun /The frumious Bandersnatch!\"/'\nthird_string = first_string + second_string\nprint(third_string)\n\n\"Beware the Jabberwock, my son! /The jaws that bite, the claws that catch! /Beware the Jubjub bird, and shun /The frumious Bandersnatch!\"/\n\n\nРядки також індексуються приблизно так само, як і списки.\n\nmy_string = 'Supercalifragilisticexpialidocious'\nprint('The first letter is: ', my_string[0]) # Uppercase S\nprint('The last letter is: ', my_string[-1]) # lowercase s\nprint('The second to last letter is: ', my_string[-2]) # lowercase u\nprint('The first five characters are: ', my_string[0:5]) # Remember: slicing doesn't include the final element!\nprint('Reverse it!: ', my_string[::-1])\n\nThe first letter is:  S\nThe last letter is:  s\nThe second to last letter is:  u\nThe first five characters are:  Super\nReverse it!:  suoicodilaipxecitsiligarfilacrepuS\n\n\nВбудовані об’єкти та класи часто мають пов’язані з ними спеціальні функції, які називаються методами. Ми отримуємо доступ до цих методів, використовуючи точку (‘.’).\nВикористовуючи рядкові методи, ми можемо підраховувати екземпляри символу або групи символів.\n\nprint('Count of the letter i in Supercalifragilisticexpialidocious: ', my_string.count('i'))\nprint('Count of \"li\" in the same word: ', my_string.count('li'))\n\nCount of the letter i in Supercalifragilisticexpialidocious:  7\nCount of \"li\" in the same word:  3\n\n\nМи також можемо знайти перший екземпляр символу або групи символів у рядку.\n\nprint('The first time i appears is at index: ', my_string.find('i'))\n\nThe first time i appears is at index:  8\n\n\nА також замінити символи в рядку.\n\nprint(\"All i's are now a's: \", my_string.replace('i', 'a'))\n\nAll i's are now a's:  Supercalafragalastacexpaaladocaous\n\n\n\nprint(\"It's raining cats and dogs\".replace('dogs', 'more cats'))\n\nIt's raining cats and more cats\n\n\nІснують також деякі методи, які є унікальними для рядків. Функція upper() перетворює всі символи в рядку в верхній регістр, в той час як lower() перетворює всі символи в рядку в нижній регістр!\n\nmy_string = \"I can't hear you\"\nprint(my_string.upper())\nmy_string = \"I said HELLO\"\nprint(my_string.lower())\n\nI CAN'T HEAR YOU\ni said hello\n\n\n\nB.5.1 Форматування рядків\nВикористовуючи метод format(), ми можемо додавати значення змінних і форматувати наші рядки.\n\nmy_string = \"{0} {1}\".format('Marco', 'Polo')\nprint(my_string)\n\nMarco Polo\n\n\n\nmy_string = \"{1} {0}\".format('Marco', 'Polo')\nprint(my_string)\n\nPolo Marco\n\n\nМи використовуємо фігурні дужки ({}) для позначення частин рядка, які будуть заповнені пізніше, і ми використовуємо аргументи функції format() для надання значень для заміни. Цифри у фігурних дужках вказують індекс значення в аргументах format().\nДивіться format() документація для отримання додаткових прикладів.\nЯкщо вам потрібне швидке та брудне форматування, ви можете замість цього використовувати символ %, який називається оператором форматування рядка.\n\nprint('insert %s here' % 'value')\n\ninsert value here\n\n\nСимвол % в основному вказує Python на створення заповнювача. Будь-який символ, що слідує за % (у рядку), вказує, який тип матиме значення, введене в заповнювач. Цей символ називається типом перетворення. Після закриття рядка нам знадобиться ще один %, за яким слідують значення для вставки. У випадку одного значення ви можете просто помістити його туди. Якщо ви вставляєте більше одного значення, вони повинні бути укладені в кортеж.\n\nprint('There are %s cats in my %s' % (13, 'apartment'))\n\nThere are 13 cats in my apartment\n\n\nУ цих прикладах %s вказує, що Python повинен перетворити значення в рядки. Існує кілька типів перетворення, які ви можете використовувати, щоб уточнити форматування. Дивіться форматування рядка для отримання додаткових прикладів та більш повної інформації про використання.",
    "crumbs": [
      "Додатки",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Вступ до мови програмування Python</span>"
    ]
  },
  {
    "objectID": "appb.html#логічні-оператори",
    "href": "appb.html#логічні-оператори",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "B.6 Логічні оператори",
    "text": "B.6 Логічні оператори\n\nB.6.1 Базова логіка\nЛогічні оператори мають справу з булевими значеннями, як ми коротко розглянули раніше. Якщо ви пам’ятаєте, bool приймає одне з двох значень: True або False (або \\(1\\) або \\(0\\)). Основні логічні твердження, які ми можемо зробити, визначаються за допомогою вбудованих компараторів. Це == (дорівнює), != (не дорівнює), &lt; (Менше), &gt; (Більше), &lt;= (менше або дорівнює) і &gt;= (більше або дорівнює).\n\nprint(5 == 5)\n\nTrue\n\n\n\nprint(5 &gt; 5)\n\nFalse\n\n\nЦі компаратори також працюють у поєднанні зі змінними.\n\nm = 2\nn = 23\nprint(m &lt; n)\n\nTrue\n\n\nМи можемо зв’язати ці компаратори разом, щоб створити більш складні логічні оператори, використовуючи логічні оператори or, and і not.\n\nstatement_1 = 10 &gt; 2\nstatement_2 = 4 &lt;= 6\nprint(\"Statement 1 truth value: {0}\".format(statement_1))\nprint(\"Statement 2 truth value: {0}\".format(statement_2))\nprint(\"Statement 1 and Statement 2: {0}\".format(statement_1 and statement_2))\n\nStatement 1 truth value: True\nStatement 2 truth value: True\nStatement 1 and Statement 2: True\n\n\nОператор or виконує логічне обчислення або. Будь-який компонент, об’єднаний за допомогою або, що є True, представлятиме все твердження як True. Оператор and виводить True, лише якщо всі компоненти разом є True. В іншому випадку він видасть False. Твердження not просто інвертує значення істинності будь-якого наступного за ним твердження. Таким чином, твердження True буде оцінено як False, коли перед ним буде поставлено not. Аналогічно, False твердження стане True, коли перед ним буде стояти not.\nПрипустимо, у нас є два логічні твердження, \\(P\\) і \\(Q\\). Таблиця істинності для основних логічних операторів виглядає наступним чином:\n\n\n\nP\nQ\nnot P\nP and Q\nP or Q\n\n\n\n\nTrue\nTrue\nFalse\nTrue\nTrue\n\n\nFalse\nTrue\nTrue\nFalse\nTrue\n\n\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\nFalse\nFalse\nTrue\nFalse\nFalse\n\n\n\nМи можемо зв’язати кілька логічних операторів разом, використовуючи логічні оператори.\n\nprint(((2 &lt; 3) and (3 &gt; 0)) or ((5 &gt; 6) and not (4 &lt; 2)))\n\nTrue\n\n\nЛогічні твердження можуть бути настільки простими або складними, наскільки нам подобається, залежно від того, що нам потрібно висловити. Оцінюючи наведене вище логічне твердження крок за кроком, ми бачимо, що ми оцінюємо (True and True) or (False and not False). Дана конструкція набуваж вигляду True or (False and True). Згодом стає True or False, і в кінцевому рахунку оцінюється як True.\n\nB.6.1.1 Істинність\nТипи даних у Python мають цікаву характеристику, яка називається істинністю. Це означає, що більшість вбудованих типів будуть оцінюватися як True або False, коли потрібне логічне значення (наприклад, за допомогою оператора if). Як правило, контейнери, такі як рядки, кортежі, словники, списки та множини, повертають True, якщо вони взагалі що-небудь містять, і False, якщо вони нічого не містять.\n\n# Cхоже до того, як працюють float() та int(), book () змушує значення вважатися логічним!\nprint(bool(''))\n\nFalse\n\n\n\nprint(bool('I have character!'))\n\nTrue\n\n\n\nprint(bool([]))\n\nFalse\n\n\n\nprint(bool([1, 2, 3]))\n\nTrue\n\n\nІ так далі, для інших колекцій та контейнерів. None також оцінюється як False. Число 1 еквівалентно True, а число 0 також еквівалентно False в логічному контексті.\n\n\n\nB.6.2 If-оператори\nМи можемо створювати сегменти коду, які виконуються тільки при виконанні набору умов. Ми використовуємо оператори if у поєднанні з логічними операторами для створення розгалужень у нашому коді.\nБлок if вводиться, коли умова вважається True. Якщо умова оцінюється як False, блок if буде просто пропущений, якщо тільки до нього не додається блок else. Умови створюються за допомогою логічних операторів або за допомогою істинності значень у Python. Оператор if визначається двокрапкою і блоком тексту з відступом.\n\nif \"Condition\":\n    print(True)\nelse:\n    print(False)\n\nTrue\n\n\n\ni = 4\nif i == 5:\n    print('The variable i has a value of 5')\n\nОскільки в цьому прикладі i = 4 і оператор if шукає лише те, чи i = 5, оператор print ніколи не буде виконаний. Ми можемо додати оператор else, щоб створити блок коду на випадок надзвичайних ситуацій на випадок, якщо умова в операторі if не буде оцінена як True.\n\ni = 5\nif i == 5:\n    print(\"Усі рядки в цьому блоці з відступом є частиною цього блоку\")\n    print('Змінна i має значення 5')\nelse:\n    print(\"Усі рядки в цьому блоці з відступом є частиною цього блоку\")\n    print('Змінна i не дорівнює 5')\n\nУсі рядки в цьому блоці з відступом є частиною цього блоку\nЗмінна i має значення 5\n\n\nМи можемо реалізувати інші гілки від того самого оператора if, використовуючи elif, скорочення від else if. Ми можемо включати стільки elif-сів, скільки захочемо, поки не вичерпаємо всі логічні гілки умови.\n\ni = 1\nif i == 1:\n    print('Змінна i має значення 1')\nelif i == 2:\n    print('Змінна і має значення 2')\nelif i == 3:\n    print('Змінна і має значення 3')\nelse:\n    print(\"Мене не хвилює змінна і\")\n\nМене не хвилює змінна і\n\n\nВи також можете вкласти оператори if в інші оператори if, щоб перевірити наявність додаткових умов.\n\ni = 10\nif i % 2 == 0:\n    if i % 3 == 0:\n        print('і ділиться як на 2, так і на 3!')\n    elif i % 5 == 0:\n        print('і ділиться як на 2, так і на 5!')\n    else:\n        print('i ділиться на 2, але не на 3 або 5!')\nelse:\n    print('Я припускаю, що i - непарне число.')\n\nі ділиться як на 2, так і на 5!\n\n\nПам’ятайте, що ми можемо згрупувати кілька умов разом, використовуючи логічні оператори!\n\ni = 11\nj = 12\nif i &lt; 10 and j &gt; 11:\n    print('{0} менше 10 і {1} більше 11!'.format(i, j))\n\nВи можете використовувати логічні компаратори для порівняння рядків!\n\nmy_string = \"Farthago delenda est\"\nif my_string == \"Carthago delenda est\":\n    print('And so it was! For the glory of Rome!')\nelse:\n    print('War elephants are TERRIFYING. I am staying home.')\n\nWar elephants are TERRIFYING. I am staying home.\n\n\nЯк і у випадку з іншими типами даних, == перевірить, чи дві речі з обох сторін мають однакове значення.\nДеякі вбудовані функції повертають логічне значення, тому їх можна використовувати як умови в операторі if. Користувацькі функції також можуть бути сконструйовані таким чином, щоб вони повертали логічне значення.\nКлючове слово in зазвичай використовується для перевірки приналежності значення до іншого значення. Ми можемо перевірити приналежність у контексті оператора if і використовувати його для виведення значення істини.\n\nif 'a' in my_string or 'e' in my_string:\n    print('Those are my favorite vowels!')\n\nThose are my favorite vowels!\n\n\nТут ми використовуємо in, щоб перевірити, чи містить змінна my_string містить якісь конкретні літери.",
    "crumbs": [
      "Додатки",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Вступ до мови програмування Python</span>"
    ]
  },
  {
    "objectID": "appb.html#циклічні-структури",
    "href": "appb.html#циклічні-структури",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "B.7 Циклічні структури",
    "text": "B.7 Циклічні структури\nЦиклічні структури є однією з найважливіших частин програмування. Цикл for і цикл while надають спосіб багаторазового запуску блоку коду повторно. Цикл while буде повторюватися до виконання певної умови. Якщо в будь-який момент після ітерації ця умова більше не виконується, цикл завершується. Цикл for буде виконувати ітерацію за послідовністю значень і завершиться, коли послідовність закінчиться. Можливо включити умови в цикл for, щоб вирішити, чи, можливо, просто дозволити йому піти своїм шляхом.\n\ni = 10\nwhile i &gt; 0:\n    i -= 1\n    print('Я зациклений! {0} значень до завершення!'.format(i))\n\nЯ зациклений! 9 значень до завершення!\nЯ зациклений! 8 значень до завершення!\nЯ зациклений! 7 значень до завершення!\nЯ зациклений! 6 значень до завершення!\nЯ зациклений! 5 значень до завершення!\nЯ зациклений! 4 значень до завершення!\nЯ зациклений! 3 значень до завершення!\nЯ зациклений! 2 значень до завершення!\nЯ зациклений! 1 значень до завершення!\nЯ зациклений! 0 значень до завершення!\n\n\nЗа допомогою циклів while нам потрібно переконатися, що щось насправді змінюється від ітерації до ітерації, щоб цикл фактично закінчувався. У цьому випадку ми використовуємо скорочення i -= 1 (скорочення від i = i - 1), так що значення i стає меншим з кожною ітерацією. Врешті-решт i буде зменшено до 0, що призведе до виконання умови False та виходу з циклу.\nЦикл for повторюється задану кількість разів, що визначається, наприклад, при вказівці кільності ітерацій в ітераторі range. У цьому випадку ми проходимось по списку, що був повернутий з range(). Цикл for вибирає значення зі списку по порядку і тимчасово присвоює йому значення i, щоб із цим значенням можна було виконувати операції.\n\nfor i in range(5):\n    print('Я зациклений! Я вже на {0}-ій ітерації!'.format(i + 1))\n\nЯ зациклений! Я вже на 1-ій ітерації!\nЯ зациклений! Я вже на 2-ій ітерації!\nЯ зациклений! Я вже на 3-ій ітерації!\nЯ зациклений! Я вже на 4-ій ітерації!\nЯ зациклений! Я вже на 5-ій ітерації!\n\n\nЗверніть увагу, що в цьому циклі for ми використовуємо ключове слово in. Використання ключового слова in не обмежується перевіркою приналежності, як у прикладі if-конструкцій. Ви можете оброблювати будь-яку колекцію за допомогою циклу for, використовуючи ключове слово in.\nУ наступному прикладі ми переглянемо множину, оскільки хочемо перевірити наявність вмісту та додати до нового набору.\n\nmy_list = {'cats', 'dogs', 'lizards', 'cows', 'bats', 'sponges', 'humans'}\nmammal_list = {'cats', 'dogs', 'cows', 'bats', 'humans'} # перераховані всі ссавці в у світі\nmy_new_list = set()\nfor animal in my_list:\n    if animal in mammal_list:\n        # додаємо будь-яку тварину, що знаходиться і в my_list, і в mammal_list\n        my_new_list.add(animal)\n\nprint(my_new_list)\n\n{'dogs', 'bats', 'cats', 'humans', 'cows'}\n\n\nЄ два твердження, які дуже корисні при роботі як з циклами for, так і з циклами while. Це break і continue. Якщо break трапляється в будь-який момент під час виконання циклу, цикл негайно завершується.\n\ni = 10\nwhile True:\n    if i == 14:\n        break\n    i += 1\n    print(i)\n\n11\n12\n13\n14\n\n\n\nfor i in range(5):\n    if i == 2:\n        break\n    print(i)\n\n0\n1\n\n\nОператор continue вкаже циклу негайно завершити цю ітерацію і перейти до наступної ітерації циклу.\n\ni = 0\nwhile i &lt; 5:\n    i += 1\n    if i == 3:\n        continue\n    print(i)\n\n1\n2\n4\n5\n\n\nЦей цикл пропускає друк числа \\(3\\) через інструкцію continue, яка виконується, коли ми вводимо оператор if. Код ніколи не бачить команди для друку числа \\(3\\), оскільки він уже перейшов до наступної ітерації.\nЗмінна, яку ми використовуємо для ітерації циклу, збереже своє значення при завершенні циклу. Аналогічно, будь-які змінні, визначені в контексті циклу, продовжуватимуть існувати поза ним.\n\nfor i in range(5):\n    loop_string = 'Я виходжу за межі циклу!'\n    print('Я вічний! Я {0} і я існую скрізь!'.format(i))\n\nprint('Моє значення {0}'.format(i))\nprint(loop_string)\n\nЯ вічний! Я 0 і я існую скрізь!\nЯ вічний! Я 1 і я існую скрізь!\nЯ вічний! Я 2 і я існую скрізь!\nЯ вічний! Я 3 і я існую скрізь!\nЯ вічний! Я 4 і я існую скрізь!\nМоє значення 4\nЯ виходжу за межі циклу!\n\n\nМи також можемо виконувати ітерації по словнику!\n\nmy_dict = {'firstname' : 'Inigo', 'lastname' : 'Montoya', 'nemesis' : 'Rugen'}\n\n\nfor key in my_dict:\n    print(key)\n\nfirstname\nlastname\nnemesis\n\n\nЯкщо ми просто перебираємо словник, не роблячи нічого іншого, ми отримуємо лише ключі. Ми можемо або використовувати ключі для отримання значень, як у прикладі:\n\nfor key in my_dict:\n    print(my_dict[key])\n\nInigo\nMontoya\nRugen\n\n\nАбо ми можемо використовувати функцію items(), щоб отримати і ключ, і значення одночасно\n\nfor key, value in my_dict.items():\n    print(key, ':', value)\n\nfirstname : Inigo\nlastname : Montoya\nnemesis : Rugen\n\n\nФункція items створює кортеж з кожної пари ключ-значення, а цикл for розпаковує цей кортеж в ключ, значення при кожному окремому виконанні циклу!",
    "crumbs": [
      "Додатки",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Вступ до мови програмування Python</span>"
    ]
  },
  {
    "objectID": "appb.html#функції",
    "href": "appb.html#функції",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "B.8 Функції",
    "text": "B.8 Функції\nФункція-це багаторазовий блок коду, який ви можете викликати повторно для виконання обчислень, виведення даних або дійсно робити все, що завгодно. Це один з ключових аспектів використання мови програмування. Щоб додати до вбудованих функцій у Python, ви можете визначити свої власні!\n\ndef hello_world():\n    \"\"\" Виводить Hello, world! \"\"\"\n    print('Hello, world!')\n\nhello_world()\n\nHello, world!\n\n\n\nfor i in range(5):\n    hello_world()\n\nHello, world!\nHello, world!\nHello, world!\nHello, world!\nHello, world!\n\n\nФункції визначаються за допомогою def, імені функції, списку параметрів та двокрапки. Все, що вказано з відступом нижче двокрапки, буде включено у визначення функції.\nМи можемо змусити наші функції робити все, що ви можете зробити зі звичайним блоком коду. Наприклад, наша функція hello_world() виводить рядок при кожному його виклику. Якщо ми хочемо зберегти значення, обчислене функцією, ми можемо визначити функцію так, щоб вона повертала (return) потрібне нам значення. Це дуже важлива особливість функцій, оскільки будь-яка змінна, визначена виключно всередині функції, не буде існувати поза нею.\n\ndef see_the_scope():\n    return \"Я тут застряг!\"\n\nprint(see_the_scope())\n\nЯ тут застряг!\n\n\n\na = see_the_scope()\nprint(a)\n\nЯ тут застряг!\n\n\nОбласть змінної — це частина блоку коду, де ця змінна прив’язана до певного значення. Функції в Python мають закриту область дії, що робить можливим прямий доступ до змінних лише всередині цих областей. Якщо ми передамо ці значення оператору return, ми можемо отримати їх із функції.\n\ndef free_the_scope():\n    in_function_string = \"Anything you can do I can do better!\"\n    return in_function_string\nmy_string = free_the_scope()\nprint(my_string)\n\nAnything you can do I can do better!\n\n\nТак само, як ми можемо отримувати значення з функції, ми також можемо розміщувати значення у функції. Ми робимо це, визначаючи нашу функцію з параметрами.\n\ndef multiply_by_five(x):\n    \"\"\" Множимо вхідне значення на 5 \"\"\"\n    return x * 5\n\nn = 4\nprint(n)\nprint(multiply_by_five(n))\n\n4\n20\n\n\nУ цьому прикладі у нас був лише один параметр для нашої функції, x. Ми можемо легко ввести додаткові параметри, розділивши їх комами.\n\ndef calculate_area(length, width):\n    \"\"\" Визначаємо площу прямокутника \"\"\"\n    return length * width\n\n\nl = 5\nw = 10\nprint('Area: ', calculate_area(l, w))\nprint('Length: ', l)\nprint('Width: ', w)\n\nArea:  50\nLength:  5\nWidth:  10\n\n\n\ndef calculate_volume(length, width, depth):\n    \"\"\" Визначаємо об'єм прямокутної призми \"\"\"\n    return length * width * depth\n\nМи можемо визначити функцію так, щоб вона приймала довільну кількість параметрів. Повідомляємо Python, що хочемо цього, використовуючи зірочку (*).\n\ndef sum_values(*args):\n    sum_val = 0\n    for i in args:\n        sum_val += i\n    return sum_val\n\n\nprint(sum_values(1, 2, 3))\nprint(sum_values(10, 20, 30, 40, 50))\nprint(sum_values(4, 2, 5, 1, 10, 249, 25, 24, 13, 6, 4))\n\n6\n150\n343\n\n\nВикористовуйте *args як параметр для вашої функції, коли ви не знаєте, скільки значень можна передати в неї, як у випадку з нашою функцією sum. Зірочка в даному випадку — це синтаксис, який повідомляє Python, що ви збираєтеся передати довільну кількість параметрів у свою функцію. Ці параметри зберігаються у вигляді кортежу.\n\ndef test_args(*args):\n    print(type(args))\n\ntest_args(1, 2, 3, 4, 5, 6)\n\n&lt;class 'tuple'&gt;\n\n\nНаші функції можуть повертати будь-який тип даних. Це дозволяє нам легко створювати функції, що перевіряють умови, які ми можемо захотіти відстежувати.\nТут ми визначаємо функцію, яка повертає логічне значення. Ми можемо легко використовувати це в поєднанні з операторами if та іншими ситуаціями, що потребують логічного значення.\n\ndef has_a_vowel(word):\n    \"\"\"\n    Перевіряємо, чи містить слово голосну\n\n    \"\"\"\n    vowel_list = ['a', 'e', 'i', 'o', 'u']\n\n    for vowel in vowel_list:\n        if vowel in word:\n            return True\n\n    return False\n\n\nmy_word = 'catnapping'\nif has_a_vowel(my_word):\n    print('Містить.')\nelse:\n    print('Не містить.')\n\nМістить.\n\n\n\ndef point_maker(x, y):\n    \"\"\" Групує значення x і y в точку, технічно кортеж \"\"\"\n    return x, y\n\nЦя наведена вище функція повертає впорядковану пару вхідних параметрів, збережених як кортеж.\n\na = point_maker(0, 10)\nb = point_maker(5, 3)\ndef calculate_slope(point_a, point_b):\n    \"\"\" Обчислює лінійний нахил між двома точками\"\"\"\n    return (point_b[1] - point_a[1])/(point_b[0] - point_a[0])\nprint(\"Кут нахилу між a і b {0}\".format(calculate_slope(a, b)))\n\nКут нахилу між a і b -1.4",
    "crumbs": [
      "Додатки",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Вступ до мови програмування Python</span>"
    ]
  },
  {
    "objectID": "appb.html#подальші-кроки",
    "href": "appb.html#подальші-кроки",
    "title": "Додаток B — Вступ до мови програмування Python",
    "section": "B.9 Подальші кроки",
    "text": "B.9 Подальші кроки\nЯкщо ви хочете заглибитися в матеріал, зверніться до документації по Python.",
    "crumbs": [
      "Додатки",
      "<span class='chapter-number'>B</span>  <span class='chapter-title'>Вступ до мови програмування Python</span>"
    ]
  },
  {
    "objectID": "appc.html",
    "href": "appc.html",
    "title": "Додаток C — Основи Jupyter Notebook",
    "section": "",
    "text": "C.1 Комірка Markdown\nЯк можна здогадатися з назви в Markdown осередках можна створювати текст в markdown форматі. Підтримуються різні способи форматування, які можна подивитися за посиланням. Текст, який ви зараз читаєте, також знаходиться в markdown клітинці.\nКрім форматування тексту також підтримується можливість створення математичних формул за допомогою LaTex. Формулу можна вбудувати в текст (наприклад, \\(e^{i\\pi}=-1\\)) або створити в окремому рядку:\n\\[e^x=\\sum_{k=0}^\\infty \\frac{x^k}{k!}\\]\nДля редагування тексту в markdown осередку необхідно два рази клікнути по ній.",
    "crumbs": [
      "Додатки",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Основи Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "appc.html#комірка-code",
    "href": "appc.html#комірка-code",
    "title": "Додаток C — Основи Jupyter Notebook",
    "section": "C.2 Комірка Code",
    "text": "C.2 Комірка Code\nНаступна комірка є Сode осередком і в ній можна писати код і виконувати його. Для виконання коду необхідно натиснути Ctrl + Enter(виконати і залишитися в поточній комірці) або Shift + Enter (виконати і перейти в наступну комірку)\n\nimport numpy as np # імпортуємо бібліотеку\n\nЯкщо останній рядок коду повертає яке-небудь значення, то воно відображається відразу після комірки\n\nnp.random.rand(10) # генеруємо випадкові значення\n\narray([0.66066316, 0.72739371, 0.52981988, 0.49703974, 0.48385109,\n       0.12894506, 0.13740195, 0.81722274, 0.05088196, 0.9682992 ])",
    "crumbs": [
      "Додатки",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Основи Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "appc.html#автодоповнення-та-робота-з-документацією",
    "href": "appc.html#автодоповнення-та-робота-з-документацією",
    "title": "Додаток C — Основи Jupyter Notebook",
    "section": "C.3 Автодоповнення та робота з документацією",
    "text": "C.3 Автодоповнення та робота з документацією\nДля автодоповнення можна використовувати клавішу &lt;TAB &gt; після точки або всередині дужки при виклику функції. При цьому вийде список доступних варіантів, які можна вибрати, щоб автоматично доповнити код. Можете спробувати автодоповнення поставивши курсор після np.random.&lt;TAB&gt;.\nВ Jupyter є кілька способів викликати документацію. Перший спосіб це використовувати поєднання клавіш Shift + Tab. Другий спосіб поставити знак ? після необхідного модуля\n\nnp?\n\nType:        module\nString form: &lt;module 'numpy' from 'c:\\\\Users\\\\Andrii\\\\anaconda3\\\\Lib\\\\site-packages\\\\numpy\\\\__init__.py'&gt;\nFile:        c:\\users\\andrii\\anaconda3\\lib\\site-packages\\numpy\\__init__.py\nDocstring:  \nNumPy\n=====\n\nProvides\n  1. An array object of arbitrary homogeneous items\n  2. Fast mathematical operations over arrays\n  3. Linear Algebra, Fourier Transforms, Random Number Generation\n\nHow to use the documentation\n----------------------------\nDocumentation is available in two forms: docstrings provided\nwith the code, and a loose standing reference guide, available from\n`the NumPy homepage &lt;https://numpy.org&gt;`_.\n\nWe recommend exploring the docstrings using\n`IPython &lt;https://ipython.org&gt;`_, an advanced Python shell with\nTAB-completion and introspection capabilities.  See below for further\ninstructions.\n\nThe docstring examples assume that `numpy` has been imported as ``np``::\n\n  &gt;&gt;&gt; import numpy as np\n\nCode snippets are indicated by three greater-than signs::\n\n  &gt;&gt;&gt; x = 42\n  &gt;&gt;&gt; x = x + 1\n\nUse the built-in ``help`` function to view a function's docstring::\n\n  &gt;&gt;&gt; help(np.sort)\n  ... # doctest: +SKIP\n\nFor some objects, ``np.info(obj)`` may provide additional help.  This is\nparticularly true if you see the line \"Help on ufunc object:\" at the top\nof the help() page.  Ufuncs are implemented in C, not Python, for speed.\nThe native Python help() does not know how to view their help, but our\nnp.info() function does.\n\nTo search for documents containing a keyword, do::\n\n  &gt;&gt;&gt; np.lookfor('keyword')\n  ... # doctest: +SKIP\n\nGeneral-purpose documents like a glossary and help on the basic concepts\nof numpy are available under the ``doc`` sub-module::\n\n  &gt;&gt;&gt; from numpy import doc\n  &gt;&gt;&gt; help(doc)\n  ... # doctest: +SKIP\n\nAvailable subpackages\n---------------------\nlib\n    Basic functions used by several sub-packages.\nrandom\n    Core Random Tools\nlinalg\n    Core Linear Algebra Tools\nfft\n    Core FFT routines\npolynomial\n    Polynomial tools\ntesting\n    NumPy testing tools\ndistutils\n    Enhancements to distutils with support for\n    Fortran compilers support and more  (for Python &lt;= 3.11).\n\nUtilities\n---------\ntest\n    Run numpy unittests\nshow_config\n    Show numpy build configuration\nmatlib\n    Make everything matrices.\n__version__\n    NumPy version string\n\nViewing documentation using IPython\n-----------------------------------\n\nStart IPython and import `numpy` usually under the alias ``np``: `import\nnumpy as np`.  Then, directly past or use the ``%cpaste`` magic to paste\nexamples into the shell.  To see which functions are available in `numpy`,\ntype ``np.&lt;TAB&gt;`` (where ``&lt;TAB&gt;`` refers to the TAB key), or use\n``np.*cos*?&lt;ENTER&gt;`` (where ``&lt;ENTER&gt;`` refers to the ENTER key) to narrow\ndown the list.  To view the docstring for a function, use\n``np.cos?&lt;ENTER&gt;`` (to view the docstring) and ``np.cos??&lt;ENTER&gt;`` (to view\nthe source code).\n\nCopies vs. in-place operation\n-----------------------------\nMost of the functions in `numpy` return a copy of the array argument\n(e.g., `np.sort`).  In-place versions of these functions are often\navailable as array methods, i.e. ``x = np.array([1,2,3]); x.sort()``.\nExceptions to this rule are documented.",
    "crumbs": [
      "Додатки",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Основи Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "appc.html#magic-команди",
    "href": "appc.html#magic-команди",
    "title": "Додаток C — Основи Jupyter Notebook",
    "section": "C.4 Magic команди",
    "text": "C.4 Magic команди\nJupyter підтримує набір так званих “чарівних” (magic) команд. Це різні корисні команди, які не є частиною Python. Всі ці команди починаються з %.\nМожна безпосередньо завантажити вміст зовнішнього файлу в комірку за допомогою команди %load\n\n# %load code/magic_example.py\ndef square(x): # ініціалізуємо функцію знаходження квадрату вхідного значення\n    \"\"\"\n    Squares given number\n    \"\"\"\n    return x ** 2 # повертаємо значення\n\n\nprint(square(42)) # виводимо результат\n\n1764\n\n\nЗ корисних команд також можна відзначити команду %timeit, яка виконує код багато разів і виводить середній час виконання коду\n\n%timeit L = [n ** 2 for n in range(1000)]\n\n377 µs ± 21.4 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n\n\nСписок усіх magic команд можна подивитися окремою командою %lsmagic.",
    "crumbs": [
      "Додатки",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Основи Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "appc.html#робота-з-графікою",
    "href": "appc.html#робота-з-графікою",
    "title": "Додаток C — Основи Jupyter Notebook",
    "section": "C.5 Робота з графікою",
    "text": "C.5 Робота з графікою\nУ Python є багато бібліотек для візуалізації даних. Більшість з них інтегруються з Jupyter і відображають графіки.\n\nimport matplotlib.pyplot as plt # імпортуємо бібліотеку\n\n# вбудовуємо виведені рисунки в юпітеровський ноутбук\n%matplotlib inline\n\nplt.plot([1, 4], [1, 4]); # виводимо лінію за двома точками\n\n\n\n\n\n\n\n\nАбо будуємо декілька графіків:\n\nall_data = [np.random.normal(0, std, size=100) for std in range(1, 4)] # генеруємо список значень із нормального розподілу\nlabels = ['x1', 'x2', 'x3'] # ініціалізуємо список міток\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9, 4)) # ініціалізуємо об'єкт матриці рисунків\n\n# прямокутна коробчаста діаграма\nbplot1 = axes[0].boxplot(all_data,\n                         vert=True,  # вертикальне вирівнювання\n                         patch_artist=True,  # заповнити кольором\n                         labels=labels)  # використовується для позначення підписів на вісі x\naxes[0].set_title('Прямокутна діаграма') # встановлюємо титулку для першого рисунку\n\n# побудова коробчастої діаграми з виїмкою\nbplot2 = axes[1].boxplot(all_data,\n                         notch=True,\n                         vert=True,  # вертикальне вирівнювання\n                         patch_artist=True,  # заповнити кольором\n                         labels=labels)  # використовується для позначення підписів на вісі x\naxes[1].set_title('Прямокутна діаграма з виїмкою') # встановлюємо титулку для другого рисунку\n\n# заповнити кольорами\ncolors = ['pink', 'lightblue', 'lightgreen']\nfor bplot in (bplot1, bplot2):\n    for patch, color in zip(bplot['boxes'], colors):\n        patch.set_facecolor(color)\n\n# додати сітку з горизонтальних ліній\nfor ax in axes:\n    ax.yaxis.grid(True)\n    ax.set_xlabel('Три відокремлений зразки')\n    ax.set_ylabel('Спостережувані значення')\n\n\n\n\n\n\n\n\nАбо тривимірну графіку:\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom matplotlib import cm\nfrom matplotlib.ticker import LinearLocator, FormatStrFormatter\nimport numpy as np\n\nfig = plt.figure(figsize=(10, 10)) # ініціалізуємо об'єкт рисунок\nax = fig.add_subplot(projection='3d') # додаємо до об'єкту тривимірне представлення\n\n# ініціалізуємо дані\nX = np.arange(-5, 5, 0.25)\nY = np.arange(-5, 5, 0.25)\nX, Y = np.meshgrid(X, Y) # заповнюємо поверхню значеннями за двома вісями\nR = np.sqrt(X**2 + Y**2)\nZ = np.sin(R) # ініціалізуємо значення по вісі Oz\n\n# будуємо поверхню\nsurf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm) # будуємо тривимірну поверхню по заданим значенням\n\nax.set_zlim(-1.01, 1.01) # встановлюємо границі по вісі Oz\nax.zaxis.set_major_locator(LinearLocator(10)) # встановлюємо 10 граничних ліній по вісі Oz\nax.zaxis.set_major_formatter(FormatStrFormatter('%.02f')) # визначаємо формат виведення значень",
    "crumbs": [
      "Додатки",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Основи Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "appc.html#інші-можливості",
    "href": "appc.html#інші-можливості",
    "title": "Додаток C — Основи Jupyter Notebook",
    "section": "C.6 Інші можливості",
    "text": "C.6 Інші можливості\nДля Jupyter Notebook було створено велику кількість плагінів. Наприклад, можна вбудовувати відео з youtube:\n\nfrom IPython.display import YouTubeVideo\nYouTubeVideo('kjBOesZCoqc')\n\n\n        \n        \n\n\nАбо інтерактивні карти (дана комірка відобразиться тільки якщо у вас встановлений folium. Якщо у вас нічого не відображається, то можете пропустити даний приклад, він далі не знадобиться)\nВстановити необхідну бібліотеку можна через команду pip install назва бібліотеки, яку варто прописати в консолі, як представлено в прикладі нижче:\n\nАбо, як варіант, можна прописати команду прямо в комірці середовища Jupyter Notebook, як представлено в прикладі нижче:\n\n!pip install folium\n\n\nimport folium\nm = folium.Map(zoom_start=12, location=[47.89829743895897, 33.36626740165739])\nm\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\nАбо вбудувати будь-який інший шматок HTML за допомогою магічної команди %%html. Нижче наведено приклад вбудовування посту з Твіттеру:\n\n%%html\n&lt;blockquote class=\"twitter-tweet\" data-lang=\"en\"&gt;&lt;p lang=\"en\" dir=\"ltr\"&gt;Replace &quot;AI&quot; with &quot;matrix multiplication &amp; gradient descent&quot; in the calls for &quot;government regulation of AI&quot; to see just how absurd they are&lt;/p&gt;&mdash; Ben Hamner (@benhamner) &lt;a href=\"https://twitter.com/benhamner/status/892136662171504640?ref_src=twsrc%5Etfw\"&gt;July 31, 2017&lt;/a&gt;&lt;/blockquote&gt;\n&lt;script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"&gt;&lt;/script&gt;\n\nReplace \"AI\" with \"matrix multiplication & gradient descent\" in the calls for \"government regulation of AI\" to see just how absurd they are— Ben Hamner (@benhamner) July 31, 2017",
    "crumbs": [
      "Додатки",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Основи Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "appc.html#гарячі-клавіші",
    "href": "appc.html#гарячі-клавіші",
    "title": "Додаток C — Основи Jupyter Notebook",
    "section": "C.7 Гарячі клавіші",
    "text": "C.7 Гарячі клавіші\nБагато дій можна виконати за допомогою так званих гарячих клавіш. Список гарячих клавіш можна знайти в меню Help - Keyboard shortcuts. Нижче наведено список найбільш корисних поєднань:\n\n\n\n\n\n\n\nКлюч\nОпис\n\n\n\n\nEsc\nвийти з режиму редагування та виділити поточну комірку\n\n\nEnter\nперейти в режим редагування поточної комірки\n\n\nCtrl+S, S\nзберегти файл\n\n\nCtrl+Enter\nвиконати код і залишитися в поточній комірці\n\n\nShift + Enter\nвиконати код і перейти в наступну клітинку\n\n\nShift + Tab\nвиводить спливаюче вікно з документацією\n\n\na\nдодати комірку згори (above)\n\n\nb\nдодати комірку знизу (below)\n\n\nc\nскопіювати комірку\n\n\nv\nвставити скопійовану клітинку\n\n\ndd\nвидалити комірку\n\n\nz\nскасування останньої дії",
    "crumbs": [
      "Додатки",
      "<span class='chapter-number'>C</span>  <span class='chapter-title'>Основи Jupyter Notebook</span>"
    ]
  },
  {
    "objectID": "appd.html",
    "href": "appd.html",
    "title": "Додаток D — Вступ до Google Colab",
    "section": "",
    "text": "D.1 Перші кроки роботи в Google Colab\nGoogle Colab (Collaboratory) — це потужна хмарна платформа, яка дозволяє писати і виконувати код на Python через веб-браузер. Вона надає можливість працювати у середовищі Jupyter Notebook без жодних налаштувань чи інсталяції на вашому локальному комп’ютері. Colab особливо корисний для задач машинного навчання, аналізу даних та наукових обчислень, оскільки пропонує безкоштовний доступ до графічних процесорів та попередньо встановлених бібліотек Python, таких як TensorFlow, Keras, PyTorch, NumPy, Pandas, Matplotlib та Scikit-learn.\nЯк програміст, ви можете виконувати наступні дії за допомогою Google Colab:\nВідкрийте у своєму браузері URL-адресу: https://colab.research.google.com. У вашому браузері відобразиться наступна сторінка (за умови, що ви увійшли до свого Google акаунта):\nУ нижньому лівому куті натисніть на кнопку + New notebook, з’явиться робочий блокнот:\nЯк ви могли помітити, інтерфейс Colab дуже схожий на інтерфейс Jupyter. Тут є вікно коду, в якому ви можете вводити свій Python код.\nЗа замовчуванням Colab блокнот використовує іменування UntitledXX.ipynb. Щоб перейменувати блокнот, клацніть на цій назві і введіть бажану назву у вікні редагування, як показано нижче:\nЦей блокнот буде названо Перший ноутбук. Отже, введіть цю назву у вікні редагування і натисніть клавішу ENTER. Блокнот встановить введене ім’я.\nТепер введемо тривіальний код на Python у вікні коду:\nЩоб виконати код, натисніть на стрілку в лівій частині комірки коду:\nРезультатом виконання є Tue Mar  5 13:52:49 2024. Можна будь-коли очистити вихідні дані, натиснувши на іконку зліва від дисплея вихідних даних:\nЩоб додати більше комірок для написання коду, виберіть наступні пункти меню:\nУ якості альтернативи можна навести вказівник миші на нижню центральну частину клітинки коду. Коли з’являться кнопки CODE і TEXT, натисність на CODE, щоб додати нову комірку:\nНова комірка коду буде додана під поточною коміркою. Додайте наступні два рядки коду в новостворену комірку:\nТепер, якщо ви запустите цю комірку, ви побачите наступний результат:\nДля запуску всіх комірок коду без переривань і покрокового натискання на кожну комірку вручну, виконайте пункт Runtime \\ Run all або Runtime \\ Restart session and run all:\nЯкщо ваш блокнот містить велику кількість комірок із кодами, ви можете зіткнутися із ситуаціями, коли ви хочете змінити порядок виконання цих клітинок. Ви можете зробити це, виділивши комірку, яку ви хочете перемістити, і натиснувши на стрілки Move cell up або Move cell down, як показано на наступному скріншоті:\nВи можете натискати на стрілки кілька разів, щоб перемістити комірку більше, ніж на одну позицію.\nПід час роботи з вашим проєктом ви могли б створити кілька непотрібних комірок у блокноті. Ви можете легко видалити такі комірки з проєкту одним натиском миші. Натисніть на іконку із символом смітника у верхньому правому куті комірки з кодом для її видалення. Нижче показано приклад:\nДля збереження вашого проєкту у вас є декілька опцій:\nДля використання переваг усіх чотирьох опцій у меню даного середовище існує вкладка File, де можна натиснути на слідуючі пункти:\nДані опції проілюстровано на скріншоті нижче:\nУ подальшому може виникунути необхідність, наприклад, працювати не з власними файлами “з абсолютного нуля”, а, наприклад, уже готовими Jupyter блокнотами. Для цього треба перейти до File \\ Open notebook або File \\ Upload notebook. Якщо ви обираєте пункт File \\ Open notebook, тоді це означатиме, що ви бажаєте відкрити робочий блокнот із хмарного середовище або вашого Google диску. Якщо ви натискаєте File \\ Upload notebook, ви хочете завантажити готовий блокнот із вашого персонального комп’ютера. Нижче представлено скріншот даних опцій:\nЗавантажимо готовий блокнот однієї з лабораторних із моделювання в Python. Для цього натиснемо File \\ Upload notebook. Далі нам треба буде натиснути на кнопку Browse та обрати потрібний нам файл, натиснувши на клавішу Open:\nВ якості прикладу ми обрали першу лабораторну роботу. Після завантаження цього файлу до Google Colab має з’явитись наступне вікно:\nColab, як і Jupyter Notebook, має два типи комірок: текстові та кодові. Текстові клітинки форматуються за допомогою простої мови розмітки Markdown.\nЩоб побачити джерело розмітки, двічі клацніть текстову комірку. При цьому буде показано як джерело розмітки, так і відображену версію. Над джерелом розмітки знаходиться панель інструментів для полегшення редагування.",
    "crumbs": [
      "Додатки",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Вступ до Google Colab</span>"
    ]
  },
  {
    "objectID": "appd.html#перші-кроки-роботи-в-google-colab",
    "href": "appd.html#перші-кроки-роботи-в-google-colab",
    "title": "Додаток D — Вступ до Google Colab",
    "section": "",
    "text": "писати та виконувати код на Python;\nдокументувати свій код;\ncтворювати/завантажувати/поширювати Jupyter блокноти;\nімпортувати/зберігати блокноти з/на Google Диск;\nімпортувати/публікувати блокноти з GitHub;\nімпортувати зовнішні набори даних, наприклад, з Kaggle;\nінтегрувати PyTorch, TensorFlow, Keras, OpenCV;\nмати безкоштовний хмарний сервіс з графічним процесором.\n\n\n\n\n\n\n\nПримітка\n\n\n\nОскільки Colab неявно використовує Google диск для зберігання ваших блокнотів, перш ніж продовжувати роботу, переконайтеся, що ви увійти до свого облікового запису Google\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nimport time\nprint(time.ctime())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntime.sleep(5)\nprint (time.ctime())\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nзберегти його на сторінці GitHub;\nзберегти його у вашому Google диску;\nзавантажити його на ваше локальне середовище у форматі .ipynb;\nзберегти його у хмарному середовищі Google Colab.\n\n\n\nSave a copy in Drive для збереження на Google диску;\nSave a copy in GitHub для збереження в репозитарії GitHub;\nSave для збереження копії у хмарному середовищі;\nDownload для збереження у форматі .ipynb або класичному Python форматі .py.",
    "crumbs": [
      "Додатки",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Вступ до Google Colab</span>"
    ]
  },
  {
    "objectID": "appd.html#синтаксис-markdown",
    "href": "appd.html#синтаксис-markdown",
    "title": "Додаток D — Вступ до Google Colab",
    "section": "D.2 Синтаксис Markdown",
    "text": "D.2 Синтаксис Markdown\n\n\n\nMarkdown\nВид\n\n\n\n\n**жирний текст**\nжирний текст\n\n\n*курсивний текст* або _курсивний текст_\nкурсивний текст\n\n\n`Виділення`\nВиділення\n\n\n~~прочерк~~\nпрочерк\n\n\n[Посилання](https://www.google.com)\nПосилання\n\n\n![Рисунок](https://www.google.com/images/rss.png)\n\n\n\n\n\nЗаголовки генеруються у вигляді назв секцій.\n# Секція 1\n# Секція 2\n## Під-секція для Секції 2\n### Під-секція під під-секцією під Секцією 2\n# Секція 3\nЗміст, доступний у лівій частині Colab, заповнюється з використанням не більше однієї назви розділу з кожної текстової комірки.\n\n&gt;Один рівень відступу\n\nОдин рівень відступу\n\n&gt;&gt;Два рівні відступу\n\n\nДва рівні відступу\n\n\n\nБлоки коду:\n```{python}\nprint(\"a\")\n```\nprint(\"a\")\n\nУпорядковані списки:\n1. Один\n1. Два\n1. Три\n\nОдин\nДва\nТри\n\n\nНевпорядковані списки:\n* Один\n* Два\n* Три\n\nОдин\nДва\nТри\n\n\nРівняння:\n$y=x^2$\n\n$e^{i\\pi} + 1 = 0$\n\n$e^x=\\sum_{i=0}^\\infty \\frac{1}{i!}x^i$\n\n$\\frac{n!}{k!(n-k)!} = {n \\choose k}$\n\\(y=x^2\\)\n\\(e^{i\\pi} + 1 = 0\\)\n\\(e^x=\\sum_{i=0}^\\infty \\frac{1}{i!}x^i\\)\n\\(\\frac{n!}{k!(n-k)!} = {n \\choose k}\\)\n\nТаблиці:\nНазва першої колонки  | Назва другої колонки \n----------------------|------------------\nРяд 1, Колонка 1      | Ряд 1, Колонка 2 \nРяд 2, Колонка 1      | Ряд 2, Колонка 2 \n\n\n\n\nНазва першої колонки\nНазва другої колонки\n\n\n\n\nРяд 1, Колонка 1\nРяд 1, Колонка 2\n\n\nРяд 2, Колонка 1\nРяд 2, Колонка 2\n\n\n\n\nГоризонтальне розділювання:\n---",
    "crumbs": [
      "Додатки",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Вступ до Google Colab</span>"
    ]
  },
  {
    "objectID": "appd.html#різниця-між-colab-markdown-та-іншими-діалектами-mardown",
    "href": "appd.html#різниця-між-colab-markdown-та-іншими-діалектами-mardown",
    "title": "Додаток D — Вступ до Google Colab",
    "section": "D.3 Різниця між Colab Markdown та іншими діалектами Mardown",
    "text": "D.3 Різниця між Colab Markdown та іншими діалектами Mardown\nColab використовує marked.js і тому схожий, але не зовсім ідентичний Markdown, що використовується Jupyter і Github.\nColab підтримує (MathJax) \\(\\text{LaTeX}\\) рівняння, як і Jupyter, але не дозволяє HTML-теги в Markdown. Colab не підтримує деякі доповнення GitHub, такі як смайлики і прапорці.\nЯкщо у блокноті Colab необхідно включити HTML, зверніться до %%html magic.",
    "crumbs": [
      "Додатки",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Вступ до Google Colab</span>"
    ]
  },
  {
    "objectID": "appd.html#встановлення-відсутніх-у-google-colab-бібліотек",
    "href": "appd.html#встановлення-відсутніх-у-google-colab-бібліотек",
    "title": "Додаток D — Вступ до Google Colab",
    "section": "D.4 Встановлення відсутніх у Google Colab бібліотек",
    "text": "D.4 Встановлення відсутніх у Google Colab бібліотек\nЩоб імпортувати бібліотеку, якої за замовчуванням немає у Collaboratory, ви можете скористатися командами !pip install або !apt-get install.\n\n\n\n\n\nНижче представлено приклад встановлення одразу чотирьох бібліотек: matplotlib, pandas, numpy і neurokit2:\n\n\n\n\n\n\n\n\n\n\n\nПримітка по встановленню бібліотек\n\n\n\nКожного разу, коли ви відкриваєте Colab блокнот, ви розпочинаєте нову робочу сесію з чистого аркуша. Тобто, всі файли та бібліотеки, які ви завантажували під час попередньої сесії, будуть очищені. Тому при роботі з кожним новим файлом вам треба буде завантажувати всі необхідні бібліотеки із самого початку. У цьому й полягає головний недолік Google Colab у порівнянні з Jupyter Notebook",
    "crumbs": [
      "Додатки",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Вступ до Google Colab</span>"
    ]
  },
  {
    "objectID": "appd.html#імпорт-власних-файлів-до-google-colab",
    "href": "appd.html#імпорт-власних-файлів-до-google-colab",
    "title": "Додаток D — Вступ до Google Colab",
    "section": "D.5 Імпорт власних файлів до Google Colab",
    "text": "D.5 Імпорт власних файлів до Google Colab\nБуває так, що виникає необхідність у роботі з локальними файлами у форматі .txt або .csv, які можуть містити результати ваших власних вимірювань. Тоді потребується імпортувати їх до хмарного середовища Colab. Для цього переходимо до панелі меню з лівої сторони вашого середовища й натискаємо на пункт Files:\n\n\n\n\n\nБачимо, що за замовчуванням Google Colab відображає вміст вашого хмарного середовища та перелік системних файлів. Для імпорту власного файлу треба натиснути на перший значок із чотирьох, що зв’явилися в розгорнотому вікні. Потрібний для натиску значок виділений на скріншоті нижче:\n\n\n\n\n\nПісля вибору потрібного для роботи файлу тиснемо на Open. У списку файлів має з’явитися обраний нами файл:\n\n\n\n\n\n\n\n\n\n\n\n\n\nЯкщо, наприклад, потребується зчитати даний файл із використанням методу pd.read_csv() бібліотеки Pandas, нам треба скопіювати шлях до зчитаного файлу. Для цього виділяємо файл курсором миші й натискаємо на вспливаючу трикрапку. Там має з’явитися пункт Copy path. Натиснувши на нього, можна підставляти шлях до методу змінної path, яка далі підставляється до методу pd.read_csv():",
    "crumbs": [
      "Додатки",
      "<span class='chapter-number'>D</span>  <span class='chapter-title'>Вступ до Google Colab</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Додаток E — Список рекомендованої літератури",
    "section": "",
    "text": "[1] R.\nV. Donner, M. Small, J. F. Donges, N. Marwan, Y. Zou, R. Xiang, and J.\nKurths, Recurrence-Based Time\nSeries Analysis by Means of Complex Network Methods,\nInternational Journal of Bifurcation and Chaos 21, 1019\n(2011).\n\n\n[2] L.\nLacasa, B. Luque, F. Ballesteros, J. Luque, and J. C. Nuño, From Time Series to\nComplex Networks: The Visibility Graph, Proceedings of the\nNational Academy of Sciences 105, 4972 (2008).\n\n\n[3] A.\nO. Bielinskyi, O. A. Serdyuk, S. O. Semerikov, and V. N. Soloviev,\nEconophysics of\nCryptocurrency Crashes: A Systematic Review, in Proceedings\nof the Selected and Revised Papers of 9th International Conference on\nMonitoring, Modeling & Management of Emergent Economy\n(M3E2-MLPEED 2021), Odessa, Ukraine, May 26-28, 2021,\nedited by A. E. Kiv, V. N. Soloviev, and S. O. Semerikov, Vol. 3048\n(CEUR-WS.org, 2021), pp. 31–133.\n\n\n[4] B.\nLuque, L. Lacasa, F. Ballesteros, and J. Luque, Horizontal Visibility\nGraphs: Exact Results for Random Time Series, Phys. Rev. E\n80, 046103 (2009).\n\n\n[5] X.\nLan, H. Mo, S. Chen, Q. Liu, and Y. Deng, Fast\ntransformation from time series to visibility graphs,\nChaos: An Interdisciplinary Journal of Nonlinear Science\n25, 083105 (2015).\n\n\n[6] I.\nV. Bezsudnov and A. A. Snarskii, From the Time Series\nto the Complex Networks: The Parametric Natural Visibility\nGraph, Physica A: Statistical Mechanics and Its Applications\n414, 53 (2014).\n\n\n[7] T.\nT. Zhou, N. D. Jin, Z. K. Gao, and Y. B. Luo, Limited Penetrable\nVisibility Graph for Establishing Complex Network from Time\nSeries, Acta Physica Sinica 61, 2012-3-030506\n(2012).\n\n\n[8] Q.\nXuan, J. Zhou, K. Qiu, D. Xu, S. Zheng, and X. Yang, CLPVG:\nCircular limited penetrable visibility graph as a new network model for\ntime series, Chaos: An Interdisciplinary Journal of\nNonlinear Science 32, 013130 (2022).\n\n\n[9] F.\nR. K. Chung, Spectral\nGraph Theory (American Mathematical Society, 1997).\n\n\n[10] N.\nBiggs, Spectral\nGraph Theory (CBMS Regional Conference Series in Mathematics\n92), Bulletin of the London Mathematical Society\n30, 197 (1998).\n\n\n[11] S.\nButler, Interlacing for Weighted\nGraphs Using the Normalized Laplacian, Electronic Journal of\nLinear Algebra 16, 90 (2007).\n\n\n[12] G.\nBounova and O. de Weck, Overview of Metrics\nand Their Correlation Patterns for Multiple-Metric Topology Analysis on\nHeterogeneous Graph Ensembles, Phys. Rev. E\n85, 016117 (2012).\n\n\n[13] I.\nGutman, The Energy of a Graph, (1978).\n\n\n[14] W.\nJun, M. Barahona, T. Yue-Jin, and D. Hong-Zhong, Natural\nConnectivity of Complex Networks, Chinese Physics Letters\n27, 078902 (2010).\n\n\n[15] E.\nEstrada, Spectral Scaling and\nGood Expansion Properties in Complex Networks, Europhysics\nLetters 73, 649 (2006).\n\n\n[16] J.\nWu, M. Barahona, Y.-J. Tan, and H.-Z. Deng, Spectral Measure of\nStructural Robustness in Complex Networks, IEEE Transactions on\nSystems, Man, and Cybernetics - Part A: Systems and Humans\n41, 1244 (2011).\n\n\n[17] D.\nM. Cvetkovic, M. Doob, and H. Sachs, Spectra of Graphs. Theory and\nApplication, (1980).\n\n\n[18] A.\nBerman and R. J. Plemmons, Nonnegative Matrices in\nthe Mathematical Sciences (Society for Industrial; Applied\nMathematics, 1994).\n\n\n[19] I.\nJ. Schoenberg, Publications of\nEdmund Landau, in Number Theory and Analysis: A Collection\nof Papers in Honor of Edmund Landau (1877–1938), edited by P. Turán\n(Springer US, Boston, MA, 1969), pp. 335–355.\n\n\n[20] T.\nH. Wei, The Algebraic Foundations of Ranking Theory (University\nof Cambridge, 1952).\n\n\n[21] M.\nG. Kendall, Further\nContributions to the Theory of Paired Comparisons, Biometrics\n11, 43 (1955).\n\n\n[22] C.\nBerge, Théorie Des Graphes Et Ses Applications (Dunod,\n1958).\n\n\n[23] P.\nBonacich, Technique for\nAnalyzing Overlapping Memberships, Sociological Methodology\n4, 176 (1972).\n\n\n[24] Wikipedia, Arnoldi\nIteration.\n\n\n[25] K.\nStephenson and M. Zelen, Rethinking\nCentrality: Methods and Examples, Social Networks\n11, 1 (1989).\n\n\n[26] V.\nLatora and M. Marchiori, Efficient Behavior\nof Small-World Networks, Phys. Rev. Lett. 87,\n198701 (2001).\n\n\n[27] R.\nPastor-Satorras, A. Vázquez, and A. Vespignani, Dynamical and\nCorrelation Properties of the Internet, Phys. Rev. Lett.\n87, 258701 (2001).\n\n\n[28] S.\nMaslov and K. Sneppen, Specificity and Stability\nin Topology of Protein Networks, Science 296,\n910 (2002).\n\n\n[29] M.\nE. J. Newman, Assortative Mixing\nin Networks, Phys. Rev. Lett. 89, 208701\n(2002).\n\n\n[30] D.\nJ. Watts and S. H. Strogatz, Collective Dynamics of\n’Small-World’ Networks, Nature 393, 440\n(1998).\n\n\n[31] A.\nBarrat and M. Weigt, On the Properties of\nSmall-World Network Models, The European Physical Journal\nB-Condensed Matter and Complex Systems 13, 547\n(2000).\n\n\n[32] S.\nBoccaletti, V. Latora, Y. Moreno, M. Chavez, and D.-U. Hwang, Complex Networks:\nStructure and Dynamics, Physics Reports 424,\n175 (2006).\n\n\n[33] P.\nHolme, C. R. Edling, and F. Liljeros, Structure and Time\nEvolution of an Internet Dating Community, Social Networks\n26, 155 (2004).\n\n\n[34] P.\nHolme, F. Liljeros, C. R. Edling, and B. J. Kim, Network\nBipartivity, Phys. Rev. E 68, 056107\n(2003).\n\n\n[35] P.\nG. Lind, M. C. González, and H. J. Herrmann, Cycles and Clustering\nin Bipartite Networks, Phys. Rev. E 72, 056127\n(2005).\n\n\n[36] P.\nZhang, J. Wang, X. Li, M. Li, Z. Di, and Y. Fan, Clustering\nCoefficient and Community Structure of Bipartite Networks,\nPhysica A: Statistical Mechanics and Its Applications\n387, 6869 (2008).\n\n\n[37] J.\nS. Richman and J. R. Moorman, Physiological\nTime-Series Analysis Using Approximate Entropy and Sample\nEntropy, American Journal of Physiology-Heart and Circulatory\nPhysiology 278, H2039 (2000).\n\n\n[38] C.\nBandt and B. Pompe, Permutation\nEntropy: A Natural Complexity Measure for Time Series, Phys.\nRev. Lett. 88, 174102 (2002).\n\n\n[39] H.\nKantz and T. Schreiber, Nonlinear Time Series Analysis\n(Cambridge University Press, 2004).\n\n\n[40] S.\nJ. Roberts, W. Penny, and I. Rezek, Temporal and Spatial\nComplexity Measures for Electroencephalogram Based Brain-Computer\nInterfacing, Medical & Biological Engineering &\nComputing 37, 93 (1999).\n\n\n[41] M.\nRostaghi and H. Azami, Dispersion Entropy: A\nMeasure for Time-Series Analysis, IEEE Signal Processing\nLetters 23, 610 (2016).\n\n\n[42] J. C. Crepeau and L.\nK. Isaacson, Journal of Non-Equilibrium Thermodynamics\n16, 137 (1991).\n\n\n[43] S.\nM. Pincus, I. M. Gladstone, and R. A. Ehrenkranz, A Regularity Statistic for\nMedical Data Analysis, Journal of Clinical Monitoring\n7, 335 (1991).\n\n\n[44] S.\nM. Pincus, Approximate Entropy as a\nMeasure of System Complexity, Proceedings of the National\nAcademy of Sciences 88, 2297 (1991).\n\n\n[45] W.\nChen, Z. Wang, H. Xie, and W. Yu, Characterization of\nSurface EMG Signal Based on Fuzzy Entropy, IEEE Transactions on\nNeural Systems and Rehabilitation Engineering 15, 266\n(2007).\n\n\n[46] H.-B. Xie, W.-X. He, and H. Liu, Measuring Time\nSeries Regularity Using Nonlinear Similarity-Based Sample\nEntropy, Physics Letters A 372, 7140\n(2008).\n\n\n[47] V.\nPlerou, P. Gopikrishnan, B. Rosenow, L. A. Nunes Amaral, and H. E.\nStanley, Universal and\nNonuniversal Properties of Cross Correlations in Financial Time\nSeries, Phys. Rev. Lett. 83, 1471\n(1999).\n\n\n[48] T.\nGuhr, A. Müller–Groeling, and H. A. Weidenmüller, Random-Matrix\nTheories in Quantum Physics: Common Concepts, Physics Reports\n299, 189 (1998).\n\n\n[49] Y.\nV. Fyodorov and A. D. Mirlin, Analytical Derivation\nof the Scaling Law for the Inverse Participation Ratio in\nQuasi-One-Dimensional Disordered Systems, Phys. Rev. Lett.\n69, 1093 (1992).\n\n\n[50] E.\nP. Wigner, On\nthe Statistical Distribution of the Widths and Spacings of Nuclear\nResonance Levels, Mathematical Proceedings of the Cambridge\nPhilosophical Society 47, 790 (1951).\n\n\n[51] E.\nP. Wigner, On a Class\nof Analytic Functions from the Quantum Theory of Collisions,\nAnnals of Mathematics 53, 36 (1951).\n\n\n[52] F.\nJ. Dyson, Statistical Theory of the Energy Levels of Complex\nSystems. I, Journal of Mathematical Physics\n3, 140 (2004).\n\n\n[53] F.\nJ. Dyson and M. L. Mehta, Statistical Theory of the Energy Levels of Complex\nSystems. IV, Journal of Mathematical Physics\n4, 701 (2004).\n\n\n[54] M.\nL. Mehta and F. J. Dyson, Statistical Theory of the Energy Levels of Complex\nSystems. V, Journal of Mathematical Physics\n4, 713 (2004).\n\n\n[55] M.\nL. Mehta, Random Matrices (Academic Press, 1991).\n\n\n[56] T.\nA. Brody, J. Flores, J. B. French, P. A. Mello, A. Pandey, and S. S. M.\nWong, Random-Matrix Physics:\nSpectrum and Strength Fluctuations, Rev. Mod. Phys.\n53, 385 (1981).\n\n\n[57] L.\nLaloux, P. Cizeau, J.-P. Bouchaud, and M. Potters, Noise Dressing of\nFinancial Correlation Matrices, Phys. Rev. Lett.\n83, 1467 (1999).\n\n\n[58] Aspects of Multivariate\nStatistical Theory (Wiley, New York, 1982).\n\n\n[59] F.\nJ. Dyson, Distribution of Eigenvalues for a Class of Real Symmetric\nMatrices, Revista Mexicana de Fisica 20, 231\n(1971).\n\n\n[60] A.\nM. Sengupta and P. P. Mitra, Distributions of\nSingular Values for Some Random Matrices, Phys. Rev. E\n60, 3389 (1999).\n\n\n[61] C.\nE. Shannon, A Mathematical\nTheory of Communication, Bell System Technical Journal\n27, 379 (1948).\n\n\n[62] R.\nA. Fisher and E. J. Russell, On the Mathematical\nFoundations of Theoretical Statistics, Philosophical\nTransactions of the Royal Society of London. Series A, Containing Papers\nof a Mathematical or Physical Character 222, 309\n(1922).\n\n\n[63] B.\nHjorth, EEG\nAnalysis Based on Time Domain Properties,\nElectroencephalography and Clinical Neurophysiology 29,\n306 (1970).\n\n\n[64] F.\nMormann, T. Kreuz, C. Rieke, R. G. Andrzejak, A. Kraskov, P. David, C.\nE. Elger, and K. Lehnertz, On the\nPredictability of Epileptic Seizures, Clinical Neurophysiology\n116, 569 (2005).\n\n\n[65] V.\nMarmelat, K. Torre, and D. Delignieres, Relative Roughness: An\nIndex for Testing the Suitability of the Monofractal Model,\nFrontiers in Physiology 3, (2012).\n\n\n[66] T.\nM. Cover, Elements of Information Theory (John Wiley &\nSons, 1999).\n\n\n[67] A.\nN. Kolmogorov, Three Approaches to the\nQuantitative Definition of Information, International Journal\nof Computer Mathematics 2, 157 (1968).\n\n\n[68] M.\nS. Kanwal, J. A. Grochow, and N. Ay, Comparing Information-Theoretic\nMeasures of Complexity in Boltzmann Machines, Entropy\n19, (2017).\n\n\n[69] M.\nLi and P. Vitányi, Preliminaries,\nin An Introduction to Kolmogorov Complexity and Its\nApplications (Springer New York, New York, NY, 2008), pp.\n1–99.\n\n\n[70] D.\nG. Bonchev, Information\nTheoretic Complexity Measures, in Encyclopedia of\nComplexity and Systems Science, edited by R. A. Meyers (Springer\nNew York, New York, NY, 2009), pp. 4820–4839.\n\n\n[71] L.\nT. Lui, G. Terrazas, H. Zenil, C. Alexander, and N. Krasnogor, Complexity Measurement Based on Information Theory and\nKolmogorov Complexity, Artificial Life\n21, 205 (2015).\n\n\n[72] J.-L. Blanc, L. Pezard, and A. Lesne, Delay Independence of\nMutual-Information Rate of Two Symbolic Sequences, Phys. Rev. E\n84, 036214 (2011).\n\n\n[73] S.\nZozor, P. Ravier, and O. Buttelli, On Lempel–Ziv\nComplexity for Multidimensional Data Analysis, Physica A:\nStatistical Mechanics and Its Applications 345, 285\n(2005).\n\n\n[74] E.\nEstevez-Rams, R. Lora Serrano, B. Aragón Fernández, and I. Brito Reyes,\nOn\nthe non-randomness of maximum Lempel Ziv complexity sequences of finite\nsize, Chaos: An Interdisciplinary Journal of Nonlinear\nScience 23, 023118 (2013).\n\n\n[75] A.\nLempel and J. Ziv, On the Complexity of\nFinite Sequences, IEEE Transactions on Information Theory\n22, 75 (1976).\n\n\n[76] R.\nGiglio, R. Matsushita, A. Figueiredo, I. Gleria, and S. D. Silva, Algorithmic Complexity\nTheory and the Relative Efficiency of Financial Markets,\nEurophysics Letters 84, 48005 (2008).\n\n\n[77] C.\nTaufemback, R. Giglio, and S. D. Silva, Algorithmic complexity theory detects decreases in the\nrelative efficiency of stock markets in the aftermath of the 2008\nfinancial crisis, Economics Bulletin\n31, 1631 (2011).\n\n\n[78] R.\nGiglio and S. Da Silva, Ranking the\nStocks Listed on Bovespa According to Their Relative Efficiency,\nMPRA Paper, University Library of Munich, Germany, 2009.\n\n\n[79] Y.\nBai, Z. Liang, and X. Li, A Permutation\nLempel-Ziv Complexity Measure for EEG Analysis, Biomedical\nSignal Processing and Control 19, 102 (2015).\n\n\n[80] M.\nBorowska, Multiscale\nPermutation Lempel–Ziv Complexity Measure for Biomedical Signal\nAnalysis: Interpretation and Application to Focal EEG Signals,\nEntropy 23, (2021).\n\n\n[81] B.\nK. Hillen, G. T. Yamaguchi, J. J. Abbas, and R. Jung, Joint-Specific Changes in\nLocomotor Complexity in the Absence of Muscle Atrophy Following\nIncomplete Spinal Cord Injury, Journal of NeuroEngineering and\nRehabilitation 10, 1 (2013).\n\n\n[82] M.\nD. Costa, C.-K. Peng, and A. L. Goldberger, Multiscale Analysis of\nHeart Rate Dynamics: Entropy and Time Irreversibility Measures,\nCardiovascular Engineering 8, 88 (2008).\n\n\n[83] R.\nClausius, T. A. Hirst, and J. Tyndall, The Mechanical Theory of\nHeat: With Its Applications to the Steam-Engine and to the Physical\nProperties of Bodies (J. Van Voorst, 1867).\n\n\n[84] L.\nBoltzmann, Weitere Studien über Das wärmegleichgewicht Unter\nGasmolekülen, in Kinetische Theorie II:\nIrreversible Prozesse Einführung Und Originaltexte\n(Vieweg+Teubner Verlag, Wiesbaden, 1970), pp. 115–225.\n\n\n[85] M.\nJ. Katz, Fractals and the\nAnalysis of Waveforms, Computers in Biology and Medicine\n18, 145 (1988).\n\n\n[86] C.\nSevcik, A Procedure to\nEstimate the Fractal Dimension of Waveforms, (2010).\n\n\n[87] A.\nKalauzi, T. Bojić, and L. Rakić, Extracting Complexity\nWaveforms from One-Dimensional Signals, Nonlinear Biomedical\nPhysics 3, 1 (2009).\n\n\n[88] F.\nHasselman, When\nthe Blind Curve Is Finite: Dimension Estimation and Model Inference\nBased on Empirical Waveforms, Frontiers in Physiology\n4, (2013).\n\n\n[89] R.\nF. Voss, Fractals in Nature:\nFrom Characterization to Simulation, in The Science of\nFractal Images, edited by H.-O. Peitgen and D. Saupe (Springer New\nYork, New York, NY, 1988), pp. 21–70.\n\n\n[90] P.\nGrassberger and I. Procaccia, Measuring the\nStrangeness of Strange Attractors, Physica D: Nonlinear\nPhenomena 9, 189 (1983).\n\n\n[91] P.\nGrassberger and I. Procaccia, Characterization of\nStrange Attractors, Phys. Rev. Lett. 50, 346\n(1983).\n\n\n[92] P.\nGrassberger, Generalized\nDimensions of Strange Attractors, Physics Letters A\n97, 227 (1983).\n\n\n[93] T.\nHiguchi, Approach to an\nIrregular Time Series on the Basis of the Fractal Theory,\nPhysica D: Nonlinear Phenomena 31, 277 (1988).\n\n\n[94] A.\nPetrosian, Kolmogorov Complexity of\nFinite Sequences and Recognition of Different Preictal EEG\nPatterns, in Proceedings Eighth IEEE Symposium on\nComputer-Based Medical Systems (1995), pp. 212–217.\n\n\n[95] R.\nEsteller, G. Vachtsevanos, J. Echauz, and B. Litt, A Comparison of Waveform\nFractal Dimension Algorithms, IEEE Transactions on Circuits and\nSystems I: Fundamental Theory and Applications 48, 177\n(2001).\n\n\n[96] C.\nGoh, B. Hamadicharef, G. T. Henderson, and E. C. Ifeachor, Comparison of Fractal Dimension Algorithms for the\nComputation of EEG Biomarkers for Dementia, in 2nd International Conference on Computational\nIntelligence in Medicine and Healthcare (CIMED2005)\n(Professor José Manuel Fonseca, UNINOVA,\nPortugal, Lisbon, Portugal, 2005).\n\n\n[97] C.\nF. Vega and J. Noel, Parameters Analyzed of\nHiguchi’s Fractal Dimension for EEG Brain Signals, in 2015\nSignal Processing Symposium (SPSympo) (2015), pp. 1–5.\n\n\n[98] B.\nB. Mandelbrot and J. A. Wheeler, The Fractal\nGeometry of Nature, American Journal of Physics\n51, 286 (1983).\n\n\n[99] H.\nE. Hurst, Long-Term\nStorage Capacity of Reservoirs, Transactions of the American\nSociety of Civil Engineers 116, 770 (1951).\n\n\n[100] H. E. Hurst, A Suggested Statistical Model of\nSome Time Series Which Occur in Nature, Nature\n180, 494 (1957).\n\n\n[101] C.-K. Peng, S. V. Buldyrev, S. Havlin, M.\nSimons, H. E. Stanley, and A. L. Goldberger, Mosaic Organization of\nDNA Nucleotides, Phys. Rev. E 49, 1685\n(1994).\n\n\n[102] Z.-Q. Jiang, W.-J. Xie, and W.-X. Zhou, Testing the Weak-Form\nEfficiency of the WTI Crude Oil Futures Market, Physica A:\nStatistical Mechanics and Its Applications 405, 235\n(2014).\n\n\n[103] S. V. Bozhokin and D. A. Parshin, Fractals\nand Multifractals: Textbook (Scientific; Publishing Center\n\"Regular; Chaotic Dynamics\", 2001).\n\n\n[104] B. B. Mandelbrot, C. J. G. Evertsz, and Y.\nHayakawa, Exactly\nSelf-Similar Left-Sided Multifractal Measures, Phys. Rev. A\n42, 4528 (1990).\n\n\n[105] H. F. Jelinek, N. Elston, and B. Zietsch,\nFractal Analysis: Pitfalls and Revelations in Neuroscience, in\nFractals in Biology and Medicine, edited by G. A. Losa, D.\nMerlini, T. F. Nonnenmacher, and E. R. Weibel (Birkhäuser\nBasel, Basel, 2005), pp. 85–94.\n\n\n[106] B. B. Mandelbrot and B. B. Mandelbrot, The\nFractal Geometry of Nature, Vol. 1 (WH freeman New York,\n1982).\n\n\n[107] H. Steinhaus, Length, Shape and Area,\nin Colloquium Mathematicum, Vol. 3 (Polska Akademia Nauk.\nInstytut Matematyczny PAN, 1954), pp. 1–13.\n\n\n[108] A. Vulpiani, Lewis Fry Richardson:\nScientist, Visionary and Pacifist, Lettera Matematica\n2, 121 (2014).\n\n\n[109] B. Hayes, Computing Science:\nStatistics of Deadly Quarrels, American Scientist\n90, 10 (2002).\n\n\n[110] B. Mandelbrot, How Long Is the\nCoast of Britain? Statistical Self-Similarity and Fractional\nDimension, Science 156, 636 (1967).\n\n\n[111] C. Tsallis, Possible Generalization of\nBoltzmann-Gibbs Statistics, Journal of Statistical Physics\n52, 479 (1988).\n\n\n[112] C. Tsallis, Dynamical Scenario\nfor Nonextensive Statistical Mechanics, Physica A: Statistical\nMechanics and Its Applications 340, 1 (2004).\n\n\n[113] C. Tsallis, M. Gell-Mann, and Y. Sato, Asymptotically\nScale-Invariant Occupancy of Phase Space Makes the Entropy\n&lt;i&gt;s&lt;sub&gt;q&lt;/Sub&gt;&lt;/i&gt; Extensive,\nProceedings of the National Academy of Sciences 102,\n15377 (2005).\n\n\n[114] C. Tsallis, Economics and Finance:\nQ-Statistical Stylized Features Galore, Entropy\n19, (2017).\n\n\n[115] G. Nicolis, I. Prigogine, W. H. Freeman, and\nCompany, Exploring Complexity: An Introduction (W.H. Freeman,\n1989).\n\n\n[116] C. Tsallis, Beyond Boltzmann–Gibbs–Shannon\nin Physics and Elsewhere, Entropy 21,\n(2019).\n\n\n[117] E. G. Pavlos, O. E. Malandraki, O. V.\nKhabarova, L. P. Karakatsanis, G. P. Pavlos, and G. Livadiotis, Non-Extensive Statistical\nAnalysis of Energetic Particle Flux Enhancements Caused by the\nInterplanetary Coronal Mass Ejection-Heliospheric Current Sheet\nInteraction, Entropy 21, (2019).\n\n\n[118] R. de Oliveira, S. Brito, L. da Silva, and C.\nTsallis, Connecting Complex Networks to Nonadditive Entropies,\nScientific Reports 11, 1130 (2021).\n\n\n[119] G. Pavlos, A. Iliopoulos, L. Karakatsanis, M.\nXenakis, and E. Pavlos, Complexity\nof Economical Systems., Journal of Engineering Science &\nTechnology Review 8, (2015).\n\n\n[120] A. Bielinskyi, S. Semerikov, O. Serdyuk, V.\nSolovieva, V. N. Soloviev, and L. Pichl, Econophysics of\nSustainability Indices, in Proceedings of the Selected\nPapers of the Special Edition of International Conference on Monitoring,\nModeling & Management of Emergent Economy\n(M3E2-MLPEED 2020), Odessa, Ukraine, July 13-18, 2020,\nedited by A. Kiv, Vol. 2713 (CEUR-WS.org, 2020), pp. 372–392.\n\n\n[121] G. L. Ferri, M. F. Reynoso Savio, and A.\nPlastino, Tsallis’ q-Triplet\nand the Ozone Layer, Physica A: Statistical Mechanics and Its\nApplications 389, 1829 (2010).\n\n\n[122] C. Anteneodo and C. Tsallis, Breakdown of\nExponential Sensitivity to Initial Conditions: Role of the Range of\nInteractions, Phys. Rev. Lett. 80, 5313\n(1998).\n\n\n[123] C. TSALLIS, Some Open Problems in\nNonextensive Statistical Mechanics, International Journal of\nBifurcation and Chaos 22, 1230030 (2012).\n\n\n[124] S. Umarov, C. Tsallis, and S. Steinberg, On Aq-Central Limit\nTheorem Consistent with Nonextensive Statistical Mechanics,\nMilan Journal of Mathematics 76, 307 (2008).\n\n\n[125] D. Stosic, D. Stosic, T. B. Ludermir, and T.\nStosic, Nonextensive Triplets\nin Cryptocurrency Exchanges, Physica A: Statistical Mechanics\nand Its Applications 505, 1069 (2018).\n\n\n[126] A. O. Bielinskyi, A. V. Matviychuk, O. A.\nSerdyuk, S. O. Semerikov, V. V. Solovieva, and V. N. Soloviev, Correlational\nand Non-Extensive Nature of Carbon Dioxide Pricing Market, in\nICTERI 2021 Workshops, edited by O. Ignatenko, V. Kharchenko,\nV. Kobets, H. Kravtsov, Y. Tarasich, V. Ermolayev, D. Esteban, V.\nYakovyna, and A. Spivakovsky, Vol. 1635 (Springer International\nPublishing, Cham, 2022), pp. 183–199.\n\n\n[127] S. G. Stavrinides et al., On the Chaotic Nature\nof Random Telegraph Noise in Unipolar RRAM Memristor Devices,\nChaos, Solitons & Fractals 160, 112224\n(2022).\n\n\n[128] A. M. Fraser and H. L. Swinney, Independent Coordinates\nfor Strange Attractors from Mutual Information, Phys. Rev. A\n33, 1134 (1986).\n\n\n[129] J. Theiler, Statistical Precision of\nDimension Estimators, Phys. Rev. A 41, 3038\n(1990).\n\n\n[130] M. Casdagli, S. Eubank, J. D. Farmer, and J.\nGibson, State\nSpace Reconstruction in the Presence of Noise, Physica D:\nNonlinear Phenomena 51, 52 (1991).\n\n\n[131] M. T. Rosenstein, J. J. Collins, and C. J. De\nLuca, A\nPractical Method for Calculating Largest Lyapunov Exponents from Small\nData Sets, Physica D: Nonlinear Phenomena 65,\n117 (1993).\n\n\n[132] M. T. Rosenstein, J. J. Collins, and C. J. De\nLuca, Reconstruction\nExpansion as a Geometry-Based Framework for Choosing Proper Delay\nTimes, Physica D: Nonlinear Phenomena 73, 82\n(1994).\n\n\n[133] H. S. Kim, R. Eykholt, and J. D. Salas, Nonlinear Dynamics,\nDelay Times, and Embedding Windows, Physica D: Nonlinear\nPhenomena 127, 48 (1999).\n\n\n[134] J. V. Lyle, M. Nandi, and P. J. Aston, Symmetric Projection\nAttractor Reconstruction: Sex Differences in the ECG, Frontiers\nin Cardiovascular Medicine 8, (2021).\n\n\n[135] T. Gautama, D. Mandic, and M. Van Hulle, A\nDifferential Entropy Based Method for Determining the Optimal Embedding\nParameters of a Signal, Proceedings 6, 29\n(2003).\n\n\n[136] M. B. Kennel, R. Brown, and H. D. I. Abarbanel,\nDetermining\nEmbedding Dimension for Phase-Space Reconstruction Using a Geometrical\nConstruction, Phys. Rev. A 45, 3403\n(1992).\n\n\n[137] L. Cao, Practical Method\nfor Determining the Minimum Embedding Dimension of a Scalar Time\nSeries, Physica D: Nonlinear Phenomena 110, 43\n(1997).\n\n\n[138] A. Krakovská, K. Mezeiová, and H. Budáčová,\nUse of False Nearest Neighbours for Selecting Variables and\nEmbedding Parameters for State Space Reconstruction, Journal of\nComplex Systems 2015, (2015).\n\n\n[139] C. Rhodes and M. Morari, The False Nearest\nNeighbors Algorithm: An Overview, Computers & Chemical\nEngineering 21, S1149 (1997).\n\n\n[140] T. Rawald, Scalable and Efficient Analysis\nof Large High-Dimensional Data Sets in the Context of Recurrence\nAnalysis, PhD thesis, Humboldt-Universität zu Berlin,\nMathematisch-Naturwissenschaftliche Fakultät, 2018.\n\n\n[141] F. Takens, Detecting Strange Attractors in\nTurbulence, in Dynamical Systems and Turbulence, Warwick\n1980, edited by D. Rand and L.-S. Young (Springer Berlin\nHeidelberg, Berlin, Heidelberg, 1981), pp. 366–381.\n\n\n[142] N. H. Packard, J. P. Crutchfield, J. D. Farmer,\nand R. S. Shaw, Geometry from a Time\nSeries, Phys. Rev. Lett. 45, 712 (1980).\n\n\n[143] K. Shockley and M. Riley, In Recurrence\nQuantification Analysis: Theory and Best Practices, 1st ed.\n(Springer, New York, 2015).\n\n\n[144] J.-P. Eckmann, S. O. Kamphorst, and D. Ruelle,\nRecurrence Plots\nof Dynamical Systems, Europhysics Letters 4,\n973 (1987).\n\n\n[145] N. Marwan, N. Wessel, U. Meyerfeldt, A.\nSchirdewan, and J. Kurths, Recurrence-Plot-Based\nMeasures of Complexity and Their Application to Heart-Rate-Variability\nData, Phys. Rev. E 66, 026702 (2002).\n\n\n[146] C. L. Webber and J. P. Zbilut, Dynamical Assessment\nof Physiological Systems and States Using Recurrence Plot\nStrategies, Journal of Applied Physiology 76,\n965 (1994).\n\n\n[147] J. P. Zbilut and C. L. Webber, Embeddings and\nDelays as Derived from Quantification of Recurrence Plots,\nPhysics Letters A 171, 199 (1992).\n\n\n[148] A. Tomashin, G. Leonardi, and S. Wallot, Four Methods to Distinguish\nBetween Fractal Dimensions in Time Series Through Recurrence\nQuantification Analysis, Entropy 24,\n(2022).\n\n\n[149] T. Gneiting, H. Ševčíková, and D. B. Percival,\nEstimators of Fractal Dimension: Assessing the Roughness\nof Time Series and Spatial Data, Statistical Science\n27, 247 (2012).\n\n\n[150] K. Falconer, Fractal Geometry: Mathematical\nFoundations and Applications (John Wiley & Sons,\n2003).\n\n\n[151] A. Kalauzi, T. Bojić, and L. Rakić, Extracting Complexity\nWaveforms from One-Dimensional Signals, Nonlinear Biomedical\nPhysics 3, (2009).\n\n\n[152] A. A. Anis and E. H. Lloyd, The Expected Value of the\nAdjusted Rescaled Hurst Range of Independent Normal Summands,\nBiometrika 63, 111 (1976).\n\n\n[153] A. Hacine-Gharbi and P. Ravier, A Binning Formula of\nBi-Histogram for Joint Entropy Estimation Using Mean Square Error\nMinimization, Pattern Recognition Letters 101,\n21 (2018).\n\n\n[154] A. Orozco-Duque, D. Novak, V. Kremen, and J.\nBustamante, Multifractal\nAnalysis for Grading Complex Fractionated Electrograms in Atrial\nFibrillation, Physiological Measurement 36,\n2269 (2015).\n\n\n[155] A. Faini, G. Parati, and P. Castiglioni, Multiscale Assessment of\nthe Degree of Multifractality for Physiological Time Series,\nPhilosophical Transactions of the Royal Society A: Mathematical,\nPhysical and Engineering Sciences 379, 20200254\n(2021).\n\n\n[156] M. Costa, A. L. Goldberger, and C.-K. Peng,\nMultiscale\nEntropy Analysis of Biological Signals, Phys. Rev. E\n71, 021906 (2005).\n\n\n[157] I. Prigogine and E. N. Hiebert, From Being\nto Becoming: Time and Complexity in the Physical\nSciences, Physics Today 35, 69\n(1982).\n\n\n[158] J. F.Donges, R. V. Donner, and J. Kurths,\nTesting Time\nSeries Irreversibility Using Complex Network Methods,\nEurophysics Letters 102, 10004 (2013).\n\n\n[159] M. Zanin, A. Rodríguez-González, E. Menasalvas\nRuiz, and D. Papo, Assessing Time Series\nReversibility Through Permutation Patterns, Entropy\n20, (2018).\n\n\n[160] R. Flanagan and L. Lacasa, Irreversibility of\nFinancial Time Series: A Graph-Theoretical Approach, Physics\nLetters A 380, 1689 (2016).\n\n\n[161] A. Puglisi and D. Villamaina, Irreversible Effects\nof Memory, Europhysics Letters 88, 30004\n(2009).\n\n\n[162] C. Diks, J. C. van Houwelingen, F. Takens, and\nJ. DeGoede, Reversibility as a\nCriterion for Discriminating Time Series, Physics Letters A\n201, 221 (1995).\n\n\n[163] C. S. Daw, C. E. A. Finney, and M. B. Kennel,\nSymbolic Approach\nfor Measuring Temporal “Irreversibility”, Phys.\nRev. E 62, 1912 (2000).\n\n\n[164] P. Guzik, J. Piskorski, T. Krauze, A.\nWykretowicz, and H. Wysocki, Heart Rate Asymmetry by\nPoincaré Plots of RR Intervals, Biomedical Engineering /\nBiomedizinische Technik 51, 272 (2006).\n\n\n[165] A. Porta, S. Guzzetti, N. Montano, T.\nGnecchi-Ruscone, R. Furlan, and A. Malliani, Time\nReversibility in Short-Term Heart Period Variability, in\n2006 Computers in Cardiology (2006), pp. 77–80.\n\n\n[166] L. Lacasa, A. Nuñez, É. Roldán, J. M. R.\nParrondo, and B. Luque, Time Series\nIrreversibility: A Visibility Graph Approach, The European\nPhysical Journal B 85, (2012).\n\n\n[167] M. Costa, A. L. Goldberger, and C.-K. Peng,\nBroken\nAsymmetry of the Human Heartbeat: Loss of Time Irreversibility in Aging\nand Disease, Phys. Rev. Lett. 95, 198102\n(2005).\n\n\n[168] C. L. Ehlers, J. Havstad, D. Prichard, and J.\nTheiler, Low Doses of\nEthanol Reduce Evidence for Nonlinear Structure in Brain\nActivity, The Journal of Neuroscience 18, 7474\n(1998).\n\n\n[169] C. Yan, P. Li, L. Ji, L. Yao, C. Karmakar, and\nC. Liu, Area\nAsymmetry of Heart Rate Variability Signal, BioMedical\nEngineering OnLine 16, (2017).\n\n\n[170] C. K. Karmakar, A. Khandoker, and M.\nPalaniswami, Phase Asymmetry of\nHeart Rate Variability Signal, Physiological Measurement\n36, 303 (2015).\n\n\n[171] I. Grosse, P. Bernaola-Galván, P. Carpena, R.\nRomán-Roldán, J. Oliver, and H. E. Stanley, Analysis of Symbolic\nSequences Using the Jensen-Shannon Divergence, Physical Review\nE 65, (2002).\n\n\n[172] L. Lacasa and R. Flanagan, Time Reversibility\nfrom Visibility Graphs of Nonstationary Processes, Phys. Rev. E\n92, 022817 (2015).\n\n\n[173] E. P. White, B. J. Enquist, and J. L. Green,\nOn Estimating the\nExponent of Power-Law Frequency Distributions, Ecology\n89, 905 (2008).\n\n\n[174] C. J. Gavilán-Moreno and G. Espinosa-Paredes,\nUsing Largest\nLyapunov Exponent to Confirm the Intrinsic Stability of Boiling Water\nReactors, Nuclear Engineering and Technology\n48, 434 (2016).\n\n\n[175] A. Prieto-Guerrero and G. Espinosa-Paredes,\nDynamics of\nBWRs and Mathematical Models, in Linear and Non-Linear\nStability Analysis in Boiling Water Reactors, edited by A.\nPrieto-Guerrero and G. Espinosa-Paredes (Woodhead Publishing, 2019), pp.\n193–268.\n\n\n[176] D. Nychka, S. Ellner, A. R. Gallant, and D.\nMcCaffrey, Finding\nChaos in Noisy Systems, Journal of the Royal Statistical\nSociety. Series B (Methodological) 54, 399\n(1992).\n\n\n[177] A. Wolf, J. B. Swift, H. L. Swinney, and J. A.\nVastano, Determining Lyapunov\nExponents from a Time Series, Physica D: Nonlinear Phenomena\n16, 285 (1985).\n\n\n[178] M. Sano and Y. Sawada, Measurement of the\nLyapunov Spectrum from a Chaotic Time Series, Phys. Rev. Lett.\n55, 1082 (1985).\n\n\n[179] J.-P. Eckmann, S. O. Kamphorst, D. Ruelle, and\nS. Ciliberto, Liapunov Exponents from\nTime Series, Phys. Rev. A 34, 4971\n(1986).\n\n\n[180] U. Parlitz, Identification of True\nand Spurious Lyapunov Exponents from Time Series, International\nJournal of Bifurcation and Chaos 02, 155 (1992).\n\n\n[181] M. Balcerzak, D. Pikunov, and A. Dabrowski,\nThe Fastest,\nSimplified Method of Lyapunov Exponents Spectrum Estimation for\nContinuous-Time Dynamical Systems, Nonlinear Dynamics\n94, 3053 (2018).\n\n\n[182] J. W. Kantelhardt, E. Koscielny-Bunde, H. H. A.\nRego, S. Havlin, and A. Bunde, Detecting\nLong-Range Correlations with Detrended Fluctuation Analysis,\nPhysica A: Statistical Mechanics and Its Applications\n295, 441 (2001).\n\n\n[183] J. W. Kantelhardt, Fractal and\nMultifractal Time Series, in Mathematics of Complexity\nand Dynamical Systems, edited by R. A. Meyers (Springer New York,\nNew York, NY, 2011), pp. 463–487.\n\n\n[184] J. W. Kantelhardt, S. A. Zschiegner, E.\nKoscielny-Bunde, S. Havlin, A. Bunde, and H. E. Stanley, Multifractal\nDetrended Fluctuation Analysis of Nonstationary Time Series,\nPhysica A: Statistical Mechanics and Its Applications\n316, 87 (2002).\n\n\n[185] C.-K. Peng, S. Havlin, H. E. Stanley, and A. L.\nGoldberger, Quantification of Scaling\nExponents and Crossover Phenomena in Nonstationary Heartbeat Time\nSeries, Chaos: An Interdisciplinary Journal of Nonlinear\nScience 5, 82 (1995).\n\n\n[186] S. Dutta, Multifractal\nProperties of ECG Patterns of Patients Suffering from Congestive Heart\nFailure, Journal of Statistical Mechanics: Theory and\nExperiment 2010, P12021 (2010).\n\n\n[187] E. Maiorino, L. Livi, A. Giuliani, A.\nSadeghian, and A. Rizzi, Multifractal\nCharacterization of Protein Contact Networks, Physica A:\nStatistical Mechanics and Its Applications 428, 302\n(2015).\n\n\n[188] P. H. Figueirêdo, E. Nogueira, M. A. Moret, and\nS. Coutinho, Multifractal Analysis\nof Polyalanines Time Series, Physica A: Statistical Mechanics\nand Its Applications 389, 2090 (2010).\n\n\n[189] G. R. Jafari, P. Pedram, and L. Hedayatifar,\nErratum:\nLong-Range Correlation and Multifractality in Bach’s Inventions\nPitches, Journal of Statistical Mechanics: Theory and\nExperiment 2012, E03001 (2012).\n\n\n[190] Z.-Q. Jiang, W.-J. Xie, W.-X. Zhou, and D.\nSornette, Multifractal Analysis of\nFinancial Markets: A Review, Reports on Progress in Physics\n82, 125901 (2019).\n\n\n[191] L. Telesca, V. Lapenna, and M. Macchiato,\nMultifractal\nFluctuations in Earthquake-Related Geoelectrical Signals, New\nJournal of Physics 7, 214 (2005).\n\n\n[192] E. G. Yee Leung and Z. Yu, Temporal Scaling\nBehavior of Avian Influenza a (H5N1): The Multifractal Detrended\nFluctuation Analysis, Annals of the Association of American\nGeographers 101, 1221 (2011).\n\n\n[193] F. Liao and Y.-K. Jan, Using Multifractal\nDetrended Fluctuation Analysis to Assess Sacral Skin Blood Flow\nOscillations in People with Spinal Cord Injury, The Journal of\nRehabilitation Research and Development 48, 787\n(2011).\n\n\n[194] L. Telesca, V. Lapenna, and M. Macchiato,\nMultifractal\nFluctuations in Seismic Interspike Series, Physica A:\nStatistical Mechanics and Its Applications 354, 629\n(2005).\n\n\n[195] M. S. Movahed, F. Ghasemi, S. Rahvar, and M. R.\nR. Tabar, Long-Range Correlation\nin Cosmic Microwave Background Radiation, Phys. Rev. E\n84, 021103 (2011).\n\n\n[196] P. Mali, S. Sarkar, S. Ghosh, A. Mukhopadhyay,\nand G. Singh, Multifractal\nDetrended Fluctuation Analysis of Particle Density Fluctuations in\nHigh-Energy Nuclear Collisions, Physica A: Statistical\nMechanics and Its Applications 424, 25 (2015).\n\n\n[197] I. T. Pedron, Correlation and\nMultifractality in Climatological Time Series, Journal of\nPhysics: Conference Series 246, 012034 (2010).\n\n\n[198] R. Rak, S. Drożdż, J. Kwapień, and P.\nOświȩcimka, Detrended\nCross-Correlations Between Returns, Volatility, Trading Activity, and\nVolume Traded for the Stock Market Companies, Europhysics\nLetters 112, 48001 (2015).\n\n\n[199] M. Wątorek, S. Drożdż, J. Kwapień, L. Minati,\nP. Oświęcimka, and M. Stanuszek, Multiscale\nCharacteristics of the Emerging Global Cryptocurrency Market,\nPhysics Reports 901, 1 (2021).\n\n\n[200] T. C. Halsey, M. H. Jensen, L. P. Kadanoff, I.\nProcaccia, and B. I. Shraiman, Fractal Measures and\nTheir Singularities: The Characterization of Strange Sets,\nNuclear Physics B - Proceedings Supplements 2, 501\n(1987).\n\n\n[201] T. C. Halsey, M. H. Jensen, L. P. Kadanoff, I.\nProcaccia, and B. I. Shraiman, Fractal Measures and\nTheir Singularities: The Characterization of Strange Sets,\nPhys. Rev. A 33, 1141 (1986).\n\n\n[202] U. Frisch and G. Parisi, Turbulence and\nPredictability of Geophysical Flows and Climate Dynamics, in\nProceedings of the International School of Physics“enrico Fermi,\"\nCourse LXXXVIII, Varenna, 1983 (North-Holland, New York,\n1985).\n\n\n[203] E. A. Ihlen, Introduction to\nMultifractal Detrended Fluctuation Analysis in Matlab,\nFrontiers in Physiology 3, (2012).\n\n\n[204] P. Oświȩcimka, L. Livi, and S. Drożdż, Right-Side-Stretched\nMultifractal Spectra Indicate Small-Worldness in Networks,\nCommunications in Nonlinear Science and Numerical Simulation\n57, 231 (2018).\n\n\n[205] S. Drożdż and P. Oświȩcimka, Detecting and\nInterpreting Distortions in Hierarchical Organization of Complex Time\nSeries, Phys. Rev. E 91, 030902 (2015).\n\n\n[206] S. Drożdż, R. Kowalski, P. Oświȩcimka, R. Rak,\nand R. Gȩbarowski, Dynamical Variety of Shapes\nin Financial Multifractality, Complexity 2018,\n1 (2018).\n\n\n[207] M. Dai, C. Zhang, and D. Zhang, Multifractal and\nSingularity Analysis of Highway Volume Data, Physica A:\nStatistical Mechanics and Its Applications 407, 332\n(2014).\n\n\n[208] M. Dai, J. Hou, and D. Ye, Multifractal\nDetrended Fluctuation Analysis Based on Fractal Fitting: The Long-Range\nCorrelation Detection Method for Highway Volume Data, Physica\nA: Statistical Mechanics and Its Applications 444, 722\n(2016).\n\n\n[209] X. Sun, H. Chen, Z. Wu, and Y. Yuan, Multifractal\nAnalysis of Hang Seng Index in Hong Kong Stock Market, Physica\nA: Statistical Mechanics and Its Applications 291, 553\n(2001).\n\n\n[210] E. Canessa, Multifractality in\nTime Series, Journal of Physics A: Mathematical and General\n33, 3637 (2000).\n\n\n[211] A. Kasprzak, R. Kutner, J. Perelló, and J.\nMasoliver, Higher-Order\nPhase Transitions on Financial Markets, The European Physical\nJournal B: Condensed Matter and Complex Systems 76, 513\n(2010).\n\n\n[212] H. D. I. Abarbanel, R. Brown, J. J. Sidorowich,\nand L. Sh. Tsimring, The Analysis of\nObserved Chaotic Data in Physical Systems, Rev. Mod. Phys.\n65, 1331 (1993).\n\n\n[213] J.-P. Eckmann and D. Ruelle, Ergodic Theory of Chaos\nand Strange Attractors, Rev. Mod. Phys. 57,\n617 (1985).\n\n\n[214] E. L. Platt, Network\nScience with Python and NetworkX Quick Start Guide: Explore and\nVisualize Network Data Effectively (Packt Publishing,\n2019).\n\n\n[215] T. H. Cormen, C. E. Leiserson, R. L. Rivest,\nand C. Stein, Introduction\nto Algorithms, Fourth Edition (MIT Press, 2022).\n\n\n[216] P. Lévy, Calcul Des\nProbabilités, Par Paul lévy, ...\n(Gauthier-Villars, 1925).\n\n\n[217] S. Mittnik, S. T. rachev, T. Doganoglu, and D.\nChenyao, Maximum Likelihood\nEstimation of Stable Paretian Models, Mathematical and Computer\nModelling 29, 275 (1999).\n\n\n[218] E. F. Fama and R. Roll, Parameter\nEstimates for Symmetric Stable Distributions, Journal of the\nAmerican Statistical Association 66, 331 (1971).\n\n\n[219] J. H. McCulloch, Simple Consistent\nEstimators of Stable Distribution Parameters, Communications in\nStatistics - Simulation and Computation 15, 1109\n(1986).\n\n\n[220] J. H. McCulloch, 13 Financial\nApplications of Stable Distributions, in Statistical\nMethods in Finance, Vol. 14 (Elsevier, 1996), pp. 393–425.\n\n\n[221] J. P. Nolan, Maximum Likelihood\nEstimation and Diagnostics for Stable Distributions, in\nLévy Processes: Theory and Applications, edited by\nO. E. Barndorff-Nielsen, S. I. Resnick, and T. Mikosch\n(Birkhäuser Boston, Boston, MA, 2001), pp. 379–400.\n\n\n[222] A. Alvarez and P. Olivares, Méthodes\nd’estimation Pour Des Lois Stables Avec Des Applications En\nFinance, Journal de La Société Française de Statistique\n146, 23 (2005).\n\n\n[223] J. P. Nolan, An Algorithm for\nEvaluating Stable Densities in Zolotarev’s (m)\nParameterization, Mathematical and Computer Modelling\n29, 229 (1999).\n\n\n[224] V. M. Zolotarev, One-Dimensional Stable\nDistributions (American Mathematical Society, 1986).\n\n\n[225] D. Salas-Gonzalez, J. M. Górriz, J. Ramírez, M.\nSchloegl, E. W. Lang, and A. Ortiz, Parameterization\nof the Distribution of White and Grey Matter in MRI Using the α-Stable\nDistribution, Computers in Biology and Medicine\n43, 559 (2013).\n\n\n[226] P. Lévy, Theorie de l’addition Des\nVariables Aleatoires (Gauthier-Villars, 1954).\n\n\n[227] T. J. Kozubowski, M. M. Meerschaert, A. K.\nPanorska, and H.-P. Scheffler, Operator Geometric\nStable Laws, Journal of Multivariate Analysis\n92, 298 (2005).\n\n\n[228] B. V. Gnedenko and A. N. Kolmogorov, Limit\nDistributions for Sums of Independent Random Variables\n(Addison-Wesley, 1968).\n\n\n[229] W. H. Dumouchel, Stable\nDistributions in Statistical Inference: 1. Symmetric Stable\nDistributions Compared to Other Symmetric Long-Tailed\nDistributions, Journal of the American Statistical Association\n68, 469 (1973).\n\n\n[230] V. N. Soloviev and A. Belinskyi, Methods of Nonlinear\nDynamics and the Construction of Cryptocurrency Crisis Phenomena\nPrecursors, in Proceedings of the 14th International\nConference on ICT in Education, Research and Industrial\nApplications. Integration, Harmonization and Knowledge Transfer. Volume\nII: Workshops, Kyiv, Ukraine, May 14-17, 2018, edited\nby V. Ermolayev, M. C. Suárez-Figueroa, V. Yakovyna, V. S. Kharchenko,\nV. Kobets, H. Kravtsov, V. S. Peschanenko, Y. Prytula, M. S.\nNikitchenko, and A. Spivakovsky, Vol. 2104 (CEUR-WS.org, 2018), pp.\n116–127.\n\n\n[231] A. Bielinskyi, V. N. Soloviev, S. Semerikov,\nand V. Solovieva, Detecting Stock Crashes\nUsing Levy Distribution, in Proceedings of the Selected\nPapers of the 8th International Conference on Monitoring, Modeling\n& Management of Emergent Economy,\nM3E2-EEMLPEED 2019, Odessa, Ukraine, May 22-24, 2019,\nedited by A. Kiv, S. Semerikov, V. N. Soloviev, L. Kibalnyk, H.\nDanylchuk, and A. Matviychuk, Vol. 2422 (CEUR-WS.org, 2019), pp.\n420–433.\n\n\n[232] V. N. Soloviev, A. Bielinskyi, and V.\nSolovieva, Entropy Analysis of\nCrisis Phenomena for DJIA Index, in\nProceedings of the 15th International Conference on ICT\nin Education, Research and Industrial Applications. Integration,\nHarmonization and Knowledge Transfer. Volume II: Workshops,\nKherson, Ukraine, June 12-15, 2019, edited by V. Ermolayev, F.\nMallet, V. Yakovyna, V. S. Kharchenko, V. Kobets, A. Kornilowicz, H.\nKravtsov, M. S. Nikitchenko, S. Semerikov, and A. Spivakovsky, Vol. 2393\n(CEUR-WS.org, 2019), pp. 434–449.\n\n\n[233] V. N. Soloviev, A. Bielinskyi, O. Serdyuk, V.\nSolovieva, and S. Semerikov, Lyapunov Exponents as\nIndicators of the Stock Market Crashes, in Proceedings of\nthe 16th International Conference on ICT in Education,\nResearch and Industrial Applications. Integration, Harmonization and\nKnowledge Transfer. Volume II: Workshops, Kharkiv, Ukraine,\nOctober 06-10, 2020, edited by O. Sokolov, G. Zholtkevych, V.\nYakovyna, Y. Tarasich, V. Kharchenko, V. Kobets, O. Burov, S. Semerikov,\nand H. Kravtsov, Vol. 2732 (CEUR-WS.org, 2020), pp. 455–470.\n\n\n[234] V. N. Soloviev and A. Belinskiy, Complex Systems\nTheory and Crashes of Cryptocurrency Market, in Information\nand Communication Technologies in Education, Research, and Industrial\nApplications, edited by V. Ermolayev, M. C. Suárez-Figueroa, V.\nYakovyna, H. C. Mayr, M. Nikitchenko, and A. Spivakovsky (Springer\nInternational Publishing, Cham, 2019), pp. 276–297.\n\n\n[235] A. O. Bielinskyi, S. V. Hushko, A. V.\nMatviychuk, O. A. Serdyuk, S. O. Semerikov, and V. N. Soloviev, Irreversibility of\nFinancial Time Series: A Case of Crisis, in Proceedings of\nthe Selected and Revised Papers of 9th International Conference on\nMonitoring, Modeling & Management of Emergent Economy\n(M3E2-MLPEED 2021), Odessa, Ukraine, May 26-28, 2021,\nedited by A. E. Kiv, V. N. Soloviev, and S. O. Semerikov, Vol. 3048\n(CEUR-WS.org, 2021), pp. 134–150.\n\n\n[236] V. N. Soloviev, A. O. Bielinskyi, and N. A.\nKharadzjan, Coverage of the\nCoronavirus Pandemic Through Entropy Measures, in 3rd\nWorkshop for Young Scientists in Computer Science and Software\nEngineering (CS and SE and SW 2020), Kryvyi Rih,\nUkraine, November 27, 2020, edited by K. A. E., S. S. O., S.\nV. N., and S. A. M., Vol. 2832 (CEUR-WS.org, 2021), pp. 24–42.\n\n\n[237] A. O. Bielinskyi, V. N. Soloviev, S. O.\nSemerikov, and V. V. Solovieva, IDENTIFYING STOCK MARKET\nCRASHES BY FUZZY MEASURES OF COMPLEXITY, Neuro-Fuzzy Modeling\nTechniques in Economics 10, 3 (2021).\n\n\n[238] A. O. Bielinskyi, A. E. Kiv, Y. O. Prikhozha,\nM. A. Slusarenko, and V. N. Soloviev, Complex Systems and Physics\nEducation, in Proceedings of the 9th Workshop on Cloud\nTechnologies in Education, CTE 2021, Kryvyi Rih, Ukraine,\nDecember 17, 2021, edited by A. E. Kiv, S. O. Semerikov, and M. P.\nShyshkina, Vol. 3085 (CEUR-WS.org, 2021), pp. 56–80.\n\n\n[239] A. O. Bielinskyi and V. N. Soloviev, Complex Network\nPrecursors of Crashes and Critical Events in the Cryptocurrency\nMarket, in Proceedings of St Student Workshop on Computer\nScience and Software Engineering, CS and SE@SW\n2018, Kryvyi Rih, Ukraine, November 30, 2018, edited by S. S. O.,\nS. A. M., S. V. N., and K. A. E., Vol. 2292 (CEUR-WS.org, 2028), pp.\n37–45.\n\n\n[240] A. Kiv, A. Bryukhanov, A. Bielinskyi, V.\nSoloviev, T. Kavetskyy, D. Dyachok, I. Donchev, and V. Lukashin, Irreversibility of\nPlastic Deformation Processes in Metals, in Information\nTechnology for Education, Science, and Technics, edited by E.\nFaure, O. Danchenko, M. Bondarenko, Y. Tryus, C. Bazilo, and G. Zaspa\n(Springer Nature Switzerland, Cham, 2023), pp. 425–445.\n\n\n[241] A. Bielinskyi, V. Soloviev, V. Solovieva, A.\nMatviychuk, and S. Semerikov, The Analysis of\nMultifractal Cross-Correlation Connectedness Between Bitcoin and the\nStock Market, in Information Technology for Education,\nScience, and Technics, edited by E. Faure, O. Danchenko, M.\nBondarenko, Y. Tryus, C. Bazilo, and G. Zaspa (Springer Nature\nSwitzerland, Cham, 2023), pp. 323–345.\n\n\n[242] A. O. Bielinskyi, V. N. Soloviev, V. Solovieva,\nS. O. Semerikov, and M. A. Radin, Recurrence\nQuantification Analysis of Energy Market Crises: A Nonlinear Approach to\nRisk Management, in Proceedings of the Selected and Revised\nPapers of 10th International Conference on Monitoring, Modeling\n& Management of Emergent Economy\n(M3E2-MLPEED 2022), Virtual Event, Kryvyi Rih, Ukraine,\nNovember 17-18, 2022, edited by H. B. Danylchuk and S. O.\nSemerikov, Vol. 3465 (CEUR-WS.org, 2022), pp. 110–131.\n\n\n[243] A. O. Bielinskyi, V. N. Soloviev, S. V. Hushko,\nA. E. Kiv, and A. V. Matviychuk, High-Order Network\nAnalysis for Financial Crash Identification, in Proceedings\nof the Selected and Revised Papers of 10th International Conference on\nMonitoring, Modeling & Management of Emergent Economy\n(M3E2-MLPEED 2022), Virtual Event, Kryvyi Rih, Ukraine,\nNovember 17-18, 2022, edited by H. B. Danylchuk and S. O.\nSemerikov, Vol. 3465 (CEUR-WS.org, 2022), pp. 132–149.\n\n\n[244] A. Kiv, A. Bryukhanov, V. Soloviev, A.\nBielinskyi, T. Kavetskyy, D. Dyachok, I. Donchev, and V. Lukashin,\nComplex Network\nMethods for Plastic Deformation Dynamics in Metals, Dynamics\n3, 34 (2023).",
    "crumbs": [
      "Додатки",
      "<span class='chapter-number'>E</span>  <span class='chapter-title'>Список рекомендованої літератури</span>"
    ]
  }
]